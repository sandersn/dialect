* Scrumph : Internal Scrum
** 2009/12/2
   - I re-arranged hypotheses. I marked up the methods intro with
     section numbers.
  - I will rewrite the headings of the methods section. I will
    re-arrange methods intro so the section numbers look less
    stupid. I will mail Sandra to set up a time on Friday to talk
    about the new methods sections.
  - Hypotheses section still sucks pretty bad. There is a lot of noise
    text left to be excised before giving a draft to Sandra.
** 2009/12/3
   - I rewrote the headings of the method sections. I re-arranged the
     methods intro so that at least the section 3 references are in
     order (though nested within section 5 references). I mailed
     Sandra and set up a time around 1:30. Maybe earlier; I will
     probably go to the reading group.
   - I will write stubs for unfinished method sections. I will rewrite
     the hypotheses section to remove the noise. I will clean up the
     whole thing for noise and send a copy to Sandra.
** 2009/12/4
   - I wrote stubs for the unfinished method sections. I rewrote the
     hypotheses sections to remove noise, along with the whole thing,
     and sent a copy to Sandra.
   - I will meet with Sandra to ask her about the appropriateness of
     the new subsections. I will expand the ones that I keep as much
     as I can, then do additional research to find out what to put in
     the others.
   - I have a ton of errands to run. Probably I should turn in my
     Swedish book soon and try to find one that  actually has
     linguists in mind.
** 2009/12/7
   - I met with Sandra, she gave me some advice. I did research on
     kinds of backoff and wrote up a couple of the sections.
   - I will finish research for alternate distance measures section,
     write up all sections, and maybe start making them sound good.
   - 
** 2009/12/8
   - I finished research alternate distance measures section and wrote
     up all sections. None of them sound particularly good.
   - I will make all sections sound good.
   - Some of the section sstill need a little research and some
     citation (textbooks, mainly, though)
** 2009/12/9
   - I made all the sections sound good, except for the last sentence
     of each one. ugh. I added citations from the appropriate papers
     where they were missing.
   - I will double-check the last stupid-sounding sentences and
     re-read the whole methods section, then send to Sandra. The rest
     of the day I will work on converting to git (installing on
     peregrin if needed), resolving unicode problems in Consts.hs and
     investigate the lexicalisation of the Berkeley parser.
** 2009/12/10
   - I added more than I thought I'd have to to the proposal, then
     sent it off to Sandra. I switched to the more reliable way of
     storing non-standard diacritic sequences in Haskell in Consts.hs.
   - I will start testing the build process with tagPos, because it
     calls Swedia.extractTnt, which I'm working on. I will verify that
     both the Python and Haskell versions reproduce all the relevant
     words from the interviews.
   - Lexicalisation of the Berkeley parser (trainCfg) is delayed until
     testing tagPos and tagDep are tested. Also I need to figure out
     a programmatic way to verify the extractTnt output.
** 2009/12/11
   - I finished the switch to git with a massive cleanup and addition
     of some overlooked files to source control. I fixed a couple of
     bugs with Swedia.hs and then found a couple more that I missed.
   - I need to write some tests for Swedia. But I also need to move on
     to the other parts of the experiment.
   - I should probably (re?)download HUnit and QuickCheck
** 2009/12/14
   - I wrote some tests for Swedia. They aren't very good and I
     haven't integrated them into any kind of build I updated HUnit
     and QuickCheck to newest versions.
   - I will make sure my entire experiment still builds. I will figure
     out what test data to work with, either fake files or fake
     literals or a real file/directory.
   - Pretty soon I need to work on ConvertTalbankenToPTB so I can run
     Berkeley results. Also I need to make sure banks can
     build my entire experiment; it's a lot faster than jones and all
     of my corpora are free so I might as well copy them over.
** 2009/12/16
   - I checked that tagPos builds and got testing of swedia to run. I
     did not figure out what test data to use.
   - I will figure out what test data to work with. I will re-read
     Convert..PTB code and some Talbanken files to remember how to get
     lexical items from the XML.
   - Need to set up connection to jones so I can test the build.
** 2009/12/17
   - I copied an example *.cha for testing. I wrote a skeleton for
     testing Convert* code, downloaded an exmple *.tiger.xml, and
     looked at the types to see which functions were pure vs impure.
   - I will write some tests for ConvertTalbankenToPTB, and keep an
     eye out for how to modify to include lexical items. I should test
     the entire build today too, in the background.
   - 
** 2009/12/28
   - I wrote some tests, modified ConvertTagsToTxt to use lexical
     items, and re-ran the Berkeley parser on this. Haven't got the
     results yet.
   - I guess I will test my lexical subset code for talbanken. Except
     I don't have talbanken so I guess I'll just use some txt file.
   - I need to check the parser results and start running R on them.
   - Note: I fixed a critical (?) bug in ConvertTagsToTxt. Before
     lexicalising, sentenceEnd was wrong, meaning that (groupBy
     sentenceEnd) was wrong, meaning that everything was one long
     sentence??
** 2009/12/29
   - I wrote a bunch of tests for ConvertTagsToTxt. Maybe others? I
     can't remember.
   - I will write more tests, mostly for ConvertTagsTo[Conll|Txt]
   - I wish I knew how to lift all from Bool to QuickCheck Property.
** 2009/12/30
   - I wrote a bunch of tests for TestConvertTags. I got the skeleton
     for TestPath up.
   - I will write tests for DepPath. I might start planning my
     dissertation chapter layout.
   - I am blocked on testing Path and testing modifications to the
     entire build; I need network access for that.
** 2010/01/04
   - I wrote tests for DepPath and planned my dissertation writing
     effort. It is below under *Plan.
   - I will fix DepPath and research Latex chapters. I will rerun
     everything from the top once I find out how long Yuyin's lopar
     will run. (I should e-mail her). I might need to prioritise the
     switch to banks.
   - It appears that I am getting significant results and that I
     didn't understand the output. '.' contributes to significance;
     '*' does not. HOWEVER, I was getting significant results on
     erroneous Dep formatted stuff, so who knows. This is like the 3rd
     time this has happened ok.
** 2010/01/05
   - I mostly fixed DepPath and downloaded the IU dissertation style
     sheet (as of 1999, in the Math department). I was misreading the
     output, and I am getting significant results! I am not sure what
     the distances are, though. It's only printing '1' for all, which
     I do not think is an average. I need to check the code again.
   - Today I will only test only running the experiment on banks if I
     have time. If I have still more time, I'll start figuring out how
     to parellelise it using multi-run.
   - Still need to make DepPath print the region name at the top of
     the file.
** 2010/01/06
   - I got multi-run working and it is super fast. I also fixed
     DepPath for real.
   - Today I will (1) reorganise icectrl.cpp so that only the
     currently used code is in the main file, and everthing else is in
     an include file. I will also chop up my proposal into
     dissertation chapters and paste the pieces in.
   - Still need to re-run DepPath.
** 2010/01/07
   - I reorganise icectrl into 3 files, changed the name to icesig.cpp,
     and added icedist.cpp. I dumped the distances and loaded them
     into Excel. I pasted a bunch of proposal text into my
     dissertation, plus some qual paper.
   - I will dump R tables and generate histograms. I will download a
     map from bing of Sweden and annotate it with pixel locations so I
     can figure out the distances in order to have geographic distnace
     matrix also.
   - Later I also need to write C++ to dump the features comparing an
     entire region to an entire other region. Then I can read them
     with something decent like Python or Haskell and do some
     analysis.
     I should also paste some of the detailed methods from my qual
     paper into the methods chapter. There is some detailed discussion
     of R, normalisation and leaf-ancestor paths.
** 2010/01/08
   - I annotated a list of locations with their relative geographical
     location. I dumped all data to R-format text tables. I generated
     PDF cluster diagrams (Ward's method worked best).
   - I will run the correlations and establish which are
     significant. I will write them up. I will make a nice diagram for
     visualising the clusters on the map of Sweden. I will write code
     to dump the diffs between the features of each set.
   - Tomorrow I should start putting all the diagrams into the
     dissertation.
** 2010/01/11
   - I ran correlations, found the significant ones and put them in a
     Latex table. I downloaded OmniGraffle and marked up the SVG/PDF
     map of Sweden from Wikipedia (available under CC GPL-Like
     licence). I wrote the code to dump per-feature diffs in C++. Then
     I wrote some Haskell code that reads in each file and (currently)
     prints the header plus the top and bottom 5 features. I ran it
     but there are way too many features to look at.
   - I will create a dict in consts of the Agree or Dep clusters,
     write some code in Norte to concat files sans the header (which I
     now think is a bad idea), then run the feature extraction on
     them. This will make only 10 comparisons to look at instead of
     528.
   - Need To pay for Omnigraffle--find out how to get academic
     discount
** 2010/01/12
   - I created a consts.py dict to map agree-clusters to sites, then
     wrote some code in norte.py to produce cluster feature
     analyses. Since I don't delete tmp files, RankFeatures extracts
     every comparison: 528 + 10. I cut the last 10 out and pasted them
     in results.tex. I also reformatted the Swedia cluster map to use
     larger fonts and coloured dots.
   - I will figure out a way to visualise and analyse the top feature
     differences between clusters (maybe Excel?). I will format them
     nicely in the results section. I will re-format the clusters so
     that they fit the page. I will work on pasting some more methods
     in and then look at the intro/hypotheses--I'm still not sure how
     that should go.
   - 
** 2010/01/13
   - I put the top 5 top/bottom features from each cluter in Excel and
     wrote some short analysis on the strength of each cluster. I
     reformatted clusters. I pasted in some more method detail from my
     qual paper and wrote some more on the intro.
   - I will download RuG-L04's newest version and get it to generate
     nice diagrams. I will re-read my intro and figure out what goes
     in between the introductory material and the hypotheses. Or after
     them. I should look at Heeringa's thesis again.
   - 
** 2010/01/14
   - I downloaded and figured out L04's basics. I didn't reread my
     intro. I don't have borders to my map, but I did include it
     already in the dissertation.
   - I will spend a couple of hours trying to add simple borders to
     the L04 maps and then re-read my intro.
   - I should upload a copy of dissertation.tex so that I can show
     Sandra. Also I need to print an invoice and check that AFP still
     works on jones.
     Also I should ask Sandra when /how much I should start sending on
     to Henrik Rosenkvist.
** 2010/01/15
   - I got simple borders added to the MDS diagrams from L04. I
     started an overview section at the end of the background
     chapter. I tweaked the wording earlier in the chapter, but didn't
     touch the hypotheses from the proposal.
   - I will meet with Sandra to find out what I still need to do on
     the proposal. I will also ask her about variations on the
     results, general layout of the dissertation, organisation of the
     first chapter (maybe) and when/how much results to send to
     Henrik Rosenkvist.
   - Still not sure what to do about hypotheses in the background
     chapter.
     Later today: create hypotheses separate chapter, generate
     figures for POS run and see how good they are. Revise proposal
     with Sandra's revisions.
* Plan
** January
*** Get results
**** TODO Try lexicalised vs POS
     Even if I can't get POS parsing out of the Berkeley parser, I
     should post-process the output to cut off the terminals. ALSO, I
     should just use the raw output of TnT.
     As is the features are just the raw trigrams of the interview,
     not POS.
**** TODO Try lexical cleaning
**** TODO Try different parameters to parsers
**** DONE Move experiment to Banks
**** TODO Try different distance measures
**** DONE Try different regions
     the current ones are smallest and don't need the additional
     precision
*** Introduction
**** Figure out how to structure latex chapters
**** Introduction
**** Hypotheses
** February
*** Revise Hypotheses
*** Methods
** March
*** Results (I HOPE)
** April
*** Discussion
** May
*** Cleanup and Defence
** June
*** Slop/revisions
* TODO Maybe should clean lexically more aggresively, esp punctuation (non .)

  Do this by
  liftM2 Set.difference
    (getDirectoryContents "." >>= wds ".t")
    (getDirectoryContents "." >>= wds "talbanken*.???")
    where wds ext = filter (isSuffixOf ext)
                    & mapM (readFile & liftM words)
                    & liftM (concat & Set.fromList)
  swediaSet = set()
  talbankenSet = set()
  for f in os.dir("."):
    if f.endswith(".t"):
      swediaSet += wds(f)
    elif f.endswith(".talbanken"):
      talbankenSet += wds(f)
  print talbankenSet - swediaSet
  def wds(f):
    return open(f).read().split()
  and see what common noise shows up
* TODO Find out how to do assertThrows in HUnit
* TODO Remember to upgrade blog software
* TODO Per-comparison variance is apparently borked.
ask Rosenkvist
    (1) to look at results (start with cluster map+dep
clusters I think)
    (2) to recommend a starting point for dialects in Swedish, plus
    current syntax papers in Swedish dialectology. 'His' page on Lunsd
    Universitet is sadly missing links to his papers.

ask local Swedish speaker to look over automatic annotation quality
esp Berkeley parser and its POS tagging quality

(maybe feed Joakim's parser POSs from the Berkely parses in order to
make the comparison between dep and path fair.) (but this will
decrease quality of)

send around revised draft, ask to set up a meeting. Then prepare a
10-15 minute overview of what I am planning to do and what I have done
so far. They will read the draft and at the meeting decide whether I
have taken the right amount of work. I will field questions if
necessary.

intro:
Add examples of how easy it is chop syntax trees into features:
    sheesh
    dependency path/pos
    dependency path/labels
    supertags
    trigrams/lexical
    trigrams/pos
    leaf-ancestor paths/lexical items
    leaf-ancestor paths/no lexical
    whole-tree checksum
    any number of syntax-esque features used in CL like word count,
    tree height, node count, internal parent-child pairs
    all types of crazy crap and that's from 5 minutes of thinking.
Provide evidence for why large corpora allow you to identify the best
feature types

Write self-citation in first person.

section 3.1: 

repeat that non-word-aligned corpora for syntax means that different
methods are needed

syntactically annotated corpora are also POS annotated (and usually
morphosyntax)

add justification for why it is better to represent the upper
structure of the tree--statistical information from the upper part of
the tree might not be useful? But I say it is, so here's why.

move "If R is bad measure" up as hypothesis 2.

explain why automatic annotation is required "preferably
automatically". Manual annotatino is BETTER, but I can't do it and
nobody else has done.


try paths of dependency labels instead of POSs
