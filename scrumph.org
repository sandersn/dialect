* Scrumph : Internal Scrum
** 2009/12/2
   - I re-arranged hypotheses. I marked up the methods intro with
     section numbers.
  - I will rewrite the headings of the methods section. I will
    re-arrange methods intro so the section numbers look less
    stupid. I will mail Sandra to set up a time on Friday to talk
    about the new methods sections.
  - Hypotheses section still sucks pretty bad. There is a lot of noise
    text left to be excised before giving a draft to Sandra.
** 2009/12/3
   - I rewrote the headings of the method sections. I re-arranged the
     methods intro so that at least the section 3 references are in
     order (though nested within section 5 references). I mailed
     Sandra and set up a time around 1:30. Maybe earlier; I will
     probably go to the reading group.
   - I will write stubs for unfinished method sections. I will rewrite
     the hypotheses section to remove the noise. I will clean up the
     whole thing for noise and send a copy to Sandra.
** 2009/12/4
   - I wrote stubs for the unfinished method sections. I rewrote the
     hypotheses sections to remove noise, along with the whole thing,
     and sent a copy to Sandra.
   - I will meet with Sandra to ask her about the appropriateness of
     the new subsections. I will expand the ones that I keep as much
     as I can, then do additional research to find out what to put in
     the others.
   - I have a ton of errands to run. Probably I should turn in my
     Swedish book soon and try to find one that  actually has
     linguists in mind.
** 2009/12/7
   - I met with Sandra, she gave me some advice. I did research on
     kinds of backoff and wrote up a couple of the sections.
   - I will finish research for alternate distance measures section,
     write up all sections, and maybe start making them sound good.
   - 
** 2009/12/8
   - I finished research alternate distance measures section and wrote
     up all sections. None of them sound particularly good.
   - I will make all sections sound good.
   - Some of the section sstill need a little research and some
     citation (textbooks, mainly, though)
** 2009/12/9
   - I made all the sections sound good, except for the last sentence
     of each one. ugh. I added citations from the appropriate papers
     where they were missing.
   - I will double-check the last stupid-sounding sentences and
     re-read the whole methods section, then send to Sandra. The rest
     of the day I will work on converting to git (installing on
     peregrin if needed), resolving unicode problems in Consts.hs and
     investigate the lexicalisation of the Berkeley parser.
** 2009/12/10
   - I added more than I thought I'd have to to the proposal, then
     sent it off to Sandra. I switched to the more reliable way of
     storing non-standard diacritic sequences in Haskell in Consts.hs.
   - I will start testing the build process with tagPos, because it
     calls Swedia.extractTnt, which I'm working on. I will verify that
     both the Python and Haskell versions reproduce all the relevant
     words from the interviews.
   - Lexicalisation of the Berkeley parser (trainCfg) is delayed until
     testing tagPos and tagDep are tested. Also I need to figure out
     a programmatic way to verify the extractTnt output.
** 2009/12/11
   - I finished the switch to git with a massive cleanup and addition
     of some overlooked files to source control. I fixed a couple of
     bugs with Swedia.hs and then found a couple more that I missed.
   - I need to write some tests for Swedia. But I also need to move on
     to the other parts of the experiment.
   - I should probably (re?)download HUnit and QuickCheck
** 2009/12/14
   - I wrote some tests for Swedia. They aren't very good and I
     haven't integrated them into any kind of build I updated HUnit
     and QuickCheck to newest versions.
   - I will make sure my entire experiment still builds. I will figure
     out what test data to work with, either fake files or fake
     literals or a real file/directory.
   - Pretty soon I need to work on ConvertTalbankenToPTB so I can run
     Berkeley results. Also I need to make sure banks can
     build my entire experiment; it's a lot faster than jones and all
     of my corpora are free so I might as well copy them over.
** 2009/12/16
   - I checked that tagPos builds and got testing of swedia to run. I
     did not figure out what test data to use.
   - I will figure out what test data to work with. I will re-read
     Convert..PTB code and some Talbanken files to remember how to get
     lexical items from the XML.
   - Need to set up connection to jones so I can test the build.
** 2009/12/17
   - I copied an example *.cha for testing. I wrote a skeleton for
     testing Convert* code, downloaded an exmple *.tiger.xml, and
     looked at the types to see which functions were pure vs impure.
   - I will write some tests for ConvertTalbankenToPTB, and keep an
     eye out for how to modify to include lexical items. I should test
     the entire build today too, in the background.
   - 
** 2009/12/28
   - I wrote some tests, modified ConvertTagsToTxt to use lexical
     items, and re-ran the Berkeley parser on this. Haven't got the
     results yet.
   - I guess I will test my lexical subset code for talbanken. Except
     I don't have talbanken so I guess I'll just use some txt file.
   - I need to check the parser results and start running R on them.
   - Note: I fixed a critical (?) bug in ConvertTagsToTxt. Before
     lexicalising, sentenceEnd was wrong, meaning that (groupBy
     sentenceEnd) was wrong, meaning that everything was one long
     sentence??
** 2009/12/29
   - I wrote a bunch of tests for ConvertTagsToTxt. Maybe others? I
     can't remember.
   - I will write more tests, mostly for ConvertTagsTo[Conll|Txt]
   - I wish I knew how to lift all from Bool to QuickCheck Property.
** 2009/12/30
   - I wrote a bunch of tests for TestConvertTags. I got the skeleton
     for TestPath up.
   - I will write tests for DepPath. I might start planning my
     dissertation chapter layout.
   - I am blocked on testing Path and testing modifications to the
     entire build; I need network access for that.
** 2010/01/04
   - I wrote tests for DepPath and planned my dissertation writing
     effort. It is below under *Plan.
   - I will fix DepPath and research Latex chapters. I will rerun
     everything from the top once I find out how long Yuyin's lopar
     will run. (I should e-mail her). I might need to prioritise the
     switch to banks.
   - It appears that I am getting significant results and that I
     didn't understand the output. '.' contributes to significance;
     '*' does not. HOWEVER, I was getting significant results on
     erroneous Dep formatted stuff, so who knows. This is like the 3rd
     time this has happened ok.
** 2010/01/05
   - I mostly fixed DepPath and downloaded the IU dissertation style
     sheet (as of 1999, in the Math department). I was misreading the
     output, and I am getting significant results! I am not sure what
     the distances are, though. It's only printing '1' for all, which
     I do not think is an average. I need to check the code again.
   - Today I will only test only running the experiment on banks if I
     have time. If I have still more time, I'll start figuring out how
     to parellelise it using multi-run.
   - Still need to make DepPath print the region name at the top of
     the file.
** 2010/01/06
   - I got multi-run working and it is super fast. I also fixed
     DepPath for real.
   - Today I will (1) reorganise icectrl.cpp so that only the
     currently used code is in the main file, and everthing else is in
     an include file. I will also chop up my proposal into
     dissertation chapters and paste the pieces in.
   - Still need to re-run DepPath.
* Plan
** January
*** Get results
**** TODO Try lexicalised vs POS
**** TODO Try lexical cleaning
**** TODO Try different regions
**** TODO Try different parameters to parsers
**** TODO Try different distance measures
**** TODO Move experiment to Banks
*** Introduction
**** Figure out how to structure latex chapters
**** Introduction
**** Hypotheses
** February
*** Revise Hypotheses
*** Methods
** March
*** Results (I HOPE)
** April
*** Discussion
** May
*** Cleanup and Defence
** June
*** Slop/revisions
* TODO Maybe should clean lexically more aggresively, esp punctuation (non .)

  Do this by
  liftM2 Set.difference
    (getDirectoryContents "." >>= wds ".t")
    (getDirectoryContents "." >>= wds "talbanken*.???")
    where wds ext = filter (isSuffixOf ext)
                    & mapM (readFile & liftM words)
                    & liftM (concat & Set.fromList)
  swediaSet = set()
  talbankenSet = set()
  for f in os.dir("."):
    if f.endswith(".t"):
      swediaSet += wds(f)
    elif f.endswith(".talbanken"):
      talbankenSet += wds(f)
  print talbankenSet - swediaSet
  def wds(f):
    return open(f).read().split()
  and see what common noise shows up
* TODO Find out how to do assertThrows in HUnit
* TODO Remember to upgrade blog software
* TODO Remember to *backup* blog posts
* TODO Remember to port viper-mode changes to modern emacs (and upgrade peregrin's viper)
* TODO Write some usable feature extraction code.
