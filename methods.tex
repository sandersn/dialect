\chapter{Methods}

There is a lot of background information, like other distance
measures and the underlying math of the distance measure I did use.

Here is what Josh misunderstood: he thought that I was taking the top
3 features first from each set of features (which have the set of
features normalised across sites, which I don't do, because to do this
properly would obviate the need for MDS since I'd be working with a
distance instead of a dissimilarity). But the top 3 would be different
for each site pair. THEN he thought I was doing MDS.

TODO: This is badly organised; half of it is historical background
(ie things I build on but don't actually use) and half of it is stuff
I actually use (or extend). But the dividing line
is in the wrong place; R and leaf-ancestor paths are presented as
history but are actually used in my code.

Probably I should break the first half into a background chapter and move it
earlier in the dissertation. It might even be appropriate in the first
chapter, except that should be a short introduction I think.

\section{Previous Work}

\subsection{S\'eguy}

Measurement of linguistic similarity has always been a part of
linguistics. However, until \namecite{seguy73} dubbed a new set of
approaches `dialectometry', these methods lagged behind the rest of
linguistics in formality. S\'eguy's quantitative analysis
of Gascogne French, while not aided by computer, was the predecessor
of more powerful statistical methods that essentially required the use
of computer as well as establishing the field's general dependence on
well-crafted dialect surveys that divide incoming data along
traditional linguistic boundaries: phonology, morphology, syntax, etc.
This makes both collection and analysis easier, although it requires
more work to combine separate analyses to produce a complete picture of dialect
variation.

The project to build the Atlas Linguistique et Ethnographique de la
Gascogne, which S\'eguy directed, collected data in a dialect survey
of Gascogne which asked speakers questions informed by different areas
of linguistics. For example, the pronunciation of `dog' ({\it chien})
was collected to measure phonological variation. It had two common
variants and many other rare ones: [k\~an], [k\~a], as well as [ka],
[ko], [kano], among others. These variants were, for the most part,
% or hat "chapeau": SapEu, kapEt, kapEu (SapE, SapEl, kapEl
known by linguists ahead of time, but their exact geographical
distribution was not.

The atlases, as eventually published, contained not only annotated
maps, but some analyses as well. These analyses were what S\'eguy named
dialectometry. Dialectometry differs from previous attempts to find
dialect boundaries in the way it combines information from the
dialect survey. Previously, dialectologists found isogloss
boundaries for individual items. A dialect boundary was generated when
enough individual isogloss boundaries coincided. However, for any real
corpus, there is so
much individual variation that only major dialect boundaries can
be captured this way.

S\'eguy reversed the process. He first combined survey data to get
a numeric score between each site. Then he posited dialect boundaries
where large distances resulted between sites. The difference is
important, because a single numeric score is easier to
analyze than hundreds of individual boundaries.
Much more subtle dialect boundaries are visible this way; where before
one saw only a jumble of conflicting boundary lines, now one sees
smaller, but consistent, numerical differences separating regions. {Dialectometry
  enables classification of gradient dialect boundaries, since now one
can distinguish weak and strong boundaries. Previously, weak
boundaries were too uncertain.}

However, S\'eguy's method of combination is simple both
linguistically and mathematically. When comparing two sites, any
difference in a response is counted as 1. Only identical
responses count as a distance of 0. Words are not analyzed
phonologically, nor are responses weighted by their relative amount
of variation. Finally, only geographically adjacent sites are
compared. This is a reasonable restriction, but later studies were
able to lift it because of the availability of greater computational
power. Work following S\'eguy's improves on both aspects. In
particular, Hans Goebl developed dialectometry models that are
more mathematically sophisticated.

\subsection{Goebl}

Hans Goebl emerged as a leader in the field of dialectometry,
formalizing the aims and methods of dialectometry. His primary
contribution was development of various methods to combine individual
distances into global distances and global distances into global clusters. These
methods were more sophisticated mathematically than previous
dialectometry and operated on any features extracted from the data. His
analyses have used primarily the Atlas Linguistique de Fran\c{c}ais.

\namecite{goebl06} provides a summary of his work. Most relevant for
this paper are the measures Relative Identity Value and Weighted
Identity Value. They are general methods that are the basis for nearly
all subsequent fine-grained dialectometrical analyses. They have three
important properties. First, they are independent of the source
data. They can operate over any linguistic data for which they are
given a feature set, such as the one proposed by \namecite{gersic71} for
phonology. Second, they can compare data even for items that do not
have identical feature sets, such as Ger\v{s}i\'c's $d$,
which cannot compare consonants and vowels. Third, they can compare
data sets that are missing some entries. This improves on S\'eguy's
analysis by providing a principled way to handle missing survey
responses.

Relative Identity Value, when comparing any two items, counts the
number of features which share the same value and then discounts
(lowers) the importance of the result by the number of unshared
features. The result is a single percentage that indicates
relative similarity. Calculating this distance between all pairs
of items in two regions produces a matrix which can be used for
clustering or other purposes. Note that the presentation below splits
Goebl's original equations into more manageable pieces; the high-level
equation for Relative Identity Value is:

\begin{equation}
  \frac{\textrm{identical}_{jk}} {\textrm{identical}_{jk} - \textrm{unidentical}_{jk}}
\label{riv}
\end{equation}
For some items being compared $j$ and $k$. In this case
\textit{identical} is
\begin{equation}
  \textrm{identical}_{jk} = |f \in \textrm{\~N}_{jk} : f_j = f_k|
\end{equation}
where $\textrm{\~N}_{jk}$ is the set of features shared by  $j$ and
$k$ and $f_j$ and $f_k$ are the value of some feature $f$ for $j$ and
$k$ respectively. \textit{unidentical} is defined similarly, except
that it counts all features N, not just the shared features
$\textrm{\~N}_{jk}$.

\begin{equation}
  \textrm{unidentical}_{jk} = |f \in \textrm{N} : f_j \neq f_k|
\end{equation}

Weighted Identity Value is a refinement of Relative Identity
Value. This measure defines some differences as more
important than others. In particular, feature values that only occur
in a few items give more information than feature values that appear
in a large number of items. This
idea shows up later in the normalization of syntax distance given by
\namecite{nerbonne06}.

The mathematical reasoning behind this idea is fairly simple. Goebl
is interested in feature values that occur in only a few items. If a
feature has some value that is shared by all of the items, then all
items belong to the same group. This feature value provides {\it no}
useful information for distinguishing the items.  The situation
improves if all but one item share the same value for a feature; at
least there are now two groups, although the larger group is still not
very informative.  The most information is available if each item
being studied has a different value for a feature; the items fall
trivially into singleton groups, one per item.

Equation \ref{wiv-ident} implements this idea by discounting
the \textit{identical} count from equation \ref{riv} by
the amount of information that feature value conveys. The
amount of information, as discussed above, is based on the number of
items that share a particular value for a feature. If all items share
the same value for some feature, then \textit{identical} will be discounted all the
way to zero--the feature conveys no useful information.
Weighted Identical Value's equation for \textit{identical} is
therefore
\begin{equation}
  \textrm{identical} = \sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_{j}}{(Ni)w} & \textrm{if} f_j = f_k
  \end{array} \right.
\label{wiv-ident}
\end{equation}

\noindent{}The complete definition of Weighted Identity Value is
\begin{equation} \sum_i \frac{\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
\end{array} \right.}
  {\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
    \end{array} \right. - |f \in \textrm{N} : f_j \neq f_k|}
  \label{wiv-full}
  \end{equation}

  \noindent{}where $\textrm{agree}f_{j}$ is the number of items that agree
  with item $j$ on feature $f$ and $Ni$ is the total number of
  items ($w$ is the weight, discussed below). Because of the
  piecewise definition of \textit{identical}, this number is always at
  least $1$ because $f_k$ agrees already with $f_j$.
  This equation takes the count of shared features and weights
  them by the size of the sharing group. The features that are shared
  with a large number of other items get a larger fraction of the normal
  count subtracted.

  For example, let $j$ and $k$ be sets of productions for the
  underlying English segment /s/. The allophones of /s/ vary mostly on the feature
  \textit{voice}. Seeing an unvoiced [s] for /s/ is less ``surprising'' than
  seeing a voiced [z], so the discounting process should
  reflect this. For example, assume that an English corpus contains 2000
  underlying /s/ segments. If 500 of them are realized as [z], the
  discounting for \textit{voice} will be as follows:

  \begin{equation}
    \begin{array}{c}
      identical_{/s/\to[z]} = 1 - 500/2000 = 1 - 0.25 = 0.75 \\
      identical_{/s/\to[s]} = 1 - 1500/2000 = 1 - 0.75 = 0.25
    \end{array}
    \label{wiv-voice}
  \end{equation}

  Each time /s/ surfaces as [s], it only receives 1/4 of a point
  toward the agreement score when it matches another [s]. When /s/
  surfaces as [z], it receives three times as much for matching
  another [z]: 3/4 points towards the agreement score. If the
  alternation is even more weighted toward faithfulness, the ratio
  changes even more; if /s/ surfaces as [z] only 1/10 of the time,
  then [z] receives 9 times more value for matching than [s] does.

  The final value, $w$, which is what gives the name ``weighted
  identity value'' to this measure, provides a way to control how much
  is discounted. A high $w$ will subtract more from uninteresting
  groups, so that \textit{voice} might be worth less than
  \textit{place} for /t/ because /t/'s allophones vary more over
  \textit{place}. In equation \ref{wiv-voice}, $w$ is left at 1 to
  facilitate the presentation.

\section{Dialectometry}

It is at this point that the two types of analysis, phonological and
syntactic, diverge. Although Goebl's techniques are general enough to
operate over any set of features that can be extracted, better results
can be obtained by specializing the general measures above to take
advantage of properties of the input.  Specifically, the application
of computational linguistics to dialectometry beginning in the 1990s
introduced methods from other fields. These methods, while generally
giving more accurate results quickly, are tied to the type of data on
which they operate.

Currently, the dominant phonological distance measure is Levenshtein
distance. This distance is essentially the count of differing
segments, although various refinements have been tried, such as
inclusion of distinctive features or phonetic
correlates. \namecite{heeringa04} gives an excellent analysis of the
applications and variations of Levenshtein distance. While Levenshtein
distance provides much information as a classifier, it is limited
because it must have a word aligned corpus for comparison. A number of
statistical methods have been proposed that remove this requirement
such as \namecite{hinrichs07} and \namecite{sanders09}, but none have
been as successful on existing dialect resources, which are small and
are already word-aligned. New resources are not easy to develop
because the statistical methods still rely on a phonetic transcription
process.

\subsection{Syntactic Distance}

Recently, computational dialectometry has expanded to analysis of
syntax as well. The first work in this area was \quotecite{nerbonne06}
analysis of Finnish L2 learners of English, followed by
\quotecite{sanders07} analysis of British dialect areas. Syntax
distance must be approached quite differently than phonological
distance. Syntactic data is extractable from raw text, so it is much
easier to build a syntactic corpus. But this implies an associated
drop in manual linguistic processing of the data. As a result, the
principal difference between present phonological and syntactic
corpora is that phonology data is word-aligned, while syntax data is
not sentence-aligned. Automatically constructed syntactic corpora
lead naturally to statistical measures over large amounts of data
rather than more sensitive measures that operate on small corpora.

\namecite{nerbonne06} were the first to use the syntactic distance
measure described below. They analyzed two corpora, both of Finnish
L2 speakers of English. The first corpus was gathered from speakers
who learned English after childhood and the second was gathered from
speakers who learned English as children. Nerbonne \& Wiersma found a
significant difference between the two corpora. The trigrams that
contributed most to the difference were those in the older corpus that
are unexpected in English. For example, the trigram COP-ADJ-N/COM is
not common in English because a noun phrase following a copula
typically begins with a determiner. Other trigrams indicate
hypercorrection on the part of the older speakers; they appear in the
younger corpus but not as often. Nerbonne \& Wiersma analyzed this as
interference from Finnish; the younger learners of English learned it
more completely with less interference from Finnish.

My subsequent work in \cite{sanders07} and \cite{sanders08b}
expanded on the Finnish experiment in two ways. First, it introduced
leaf-ancestor paths as an alternative feature type. Second, it tested
the distance method on a larger set of corpora: Government Office
Regions of England, as well as Scotland and Wales, for a total of
11 corpora. Each was smaller than the Finnish L2 corpora, so the
permutation test parameters had to be adjusted for some feature
combinations.

The distances between regions were clustered using hierarchical
agglomerative clustering, as described in section
\ref{cluster-analysis}. The resulting tree showed a North/South
distinction with some unexpected differences from previously
hypothesized dialect boundaries; for example, the Northwest region
clustered with the Southwest region. This contrasted with the
clustered phonological distances also produced in
\namecite{sanders08b}. In that experiment, there was no significant
correlation between the inter-region phonological distances and
syntactic distances.

There are several possible reasons for this lack of correlation. The
two distance measures may find different dialect boundaries based on
differences between syntax and phonology. Dialect boundaries may have
shifted during the 40 years between the collection of the SED and the
collection of the ICE-GB. One or both methods may be measuring the
wrong thing. In this dissertation, although the focus will remain on results
of computational syntax distance as compared to traditional syntactic
dialectology, the discussion compares recent phonological
dialectometry results on Swedish to the results obtained here.

\subsubsection{Nerbonne and Wiersma}
\label{nerbonne06}

Due to the lack of alignment between the larger corpora available for
syntactic analysis, a statistical comparison of differences is more
appropriate than the simple symbolic approach possible with the
word-aligned corpora used in phonology. This statistical approach
means that a syntactic distance measure will have to use counting as
its basis.

\namecite{nerbonne06}'s method models syntax by part-of-speech (POS)
trigrams and uses differences between trigram type counts in a
permutation test of significance. The heart of the measure is simple:
the difference in type counts between the combined features of two
corpora. \namecite{kessler01} originally proposed this measure, the
{\sc Recurrence} metric ($R$):

\begin{equation}
R = \sum_i |c_{ai} - c_{bi}|
\label{rmeasure}
\end{equation}

\noindent{}Given two corpora $a$ and $b$, $c_a$ and $c_b$ are the
feature counts. $i$ ranges over all features, so $c_{ai}$ and $c_{bi}$ are the
counts of corpora $a$ and $b$ for feature $i$. $R$ is designed to
represent the amount of variation exhibited by the two corpora while
the contribution of individual features remains transparent to aid later
analysis. Unfortunately, it doesn't indicate whether its results are significant; a
permutation test is needed for that, described in section
\ref{permutationtest}.

\subsubsection{Dialectometry in British English}

The methods used in this dissertation are an evolution of those in my
previous work on British English: \cite{sanders07} and
\cite{sanders08b}. There, I compared phonological and syntactic
dialectometry as described above. The process is similar to Wiersma's
work in \cite{nerbonne06} and \cite{wiersma09}. Here, it is slightly
simpler and with a few variations.

The input are 30 corpora, one for each interview site (described in
section \ref{syntactically-annotated-corpus}). The sentences in each
corpus have their features generated (the features are described in
\ref{syntactic-features}). Optionally, only 1000 sentences are sampled
with replacement, but the corpus sizes, unlike the British interviews
in my previous work, are fairly similar in size so this is only
necessary for comparison to previous work. Then the features are
counted, producing a mapping of feature types to token counts.

At this point, two corpora are compared based on these feature
counts. The feature counts are first normalised to account for
variation in corpus size and sentence length (described in
the next section). Optionally, they are converted to ratios, meaning
that the counts are scaled relative to the other corpus. For example,
counts of 10 and 30 would produce the ration 1 to 3, as would the
counts 100 and 300. Finally, the distance (described above in
\ref{nerbonne06}) is calculated 10 times and the result is averaged.

The corpus is sampled by sentence rather than by feature because the
intent is to capture syntax, where the composite unit is the
sentence. Similarly, phonology's composite unit is the word---most
processes operate within the word on individual segments; some
processes operate between words but they are fewer. Therefore, the
assumption that words are independent will lose some information but
not the majority. In the same way, the basic unit of syntax is the
sentence; processes operate on the words in the sentence, but
inter-sentence processes are fewer. Because of this, the corpora are
sampled by sentence, combining the sentences of all speakers from an
interview site into a single corpus.

This dissertation skips the per-speaker sampling of
\quotecite{wiersma09} work on Finnish L2 speakers. I assume that,
since discovery of dialect features is the goal of this research, the
sentences of speakers from the same village are independent of the
speaker, at least with respect to dialect features. Although the
motivation is partly theoretical, there is also a difference between
the Swediasyn dialect corpus, with 2-4 speakers for each of 30 sites,
and Wiersma's L2 corpus, with dozens of speakers but only two
groups. Sampling per-speaker would not be feasible for the Swediasyn
because there aren't enough speakers per village.

\subsubsection{Normalization}

The two corpora being compared can differ in size, even if they are
samples with the same number of sentences; if one corpus contains many
long sentences with the other contains many short ones, raw counts
will favour the features extracted from the long sentences simply
because each sentence yields more features. Addtionally, the counts can
optionally be converted to ratios to ignore the effect of
frequency---in effect, this ranks features only by how much they differ
between the two corpora, ignoring the question of how often they occur
relative to the other features extracted from the corpora. That is,
a high ratio for a rare feature that happens only ten times in both
corpora is just as important as a high ratio for a common feature that
happens thousands of times.

The first normalization normalizes the counts for each feature within the
pair of vectors $a$ and $b$. The purpose is to normalise the
difference in sentence length, where longer sentences with more words
cause features to be relatively more frequent than corpora with many
short sentences.  Each count $c_i$ in the vector $c$ is converted to a
frequency $f_i$ \[f_i=\frac{c_i}{N} \] where $N$ is the length of
$c$. For two corpora $c_a$ and $c_b$ this produces two frequency
vectors, $f_{a}$ and $f_{b}$.Then the original counts in $c_a$ and
$c_b$ are redistributed according to the frequencies in $f_a$ and $f_b$:
\[\forall j \in a,b : c'_{ji} = \frac{f_{ji}(c_{ai}+c_{bi})}{f_{ai}+f_{bi}}\]
This redistributes the total of a pair from $a$ and $b$ based on
their relative frequencies. In other words, the total for each feature
remains the same:
\[ c_{ai} + c_{bi} = c'_{ai} + c'_{bi} \]
but the values of $c_{ai}$ and $c_{bi}$ are scaled by their frequency
within their respective vectors.

For example, assume that the two corpora have 10 sentences each, with
a corpus $a$ with only 40 words and another, $b$, with 100 words. This
results in $N_a = 40$ and $N_b = 100$. Assume also that there is a
feature $i$ that occurs in both: $c_{ai} = 8$ in $a$ and $c_{bi} = 10$ in
$b$. This means that the relative frequencies are $f_{ai} = 8/40 = 0.2$
and $f_{bi} = 10/100 = 0.1$. The first normalization will redistribute the
total count ($10 + 8 = 18$) according to relative frequencies. So
\[c_{ai}' = \frac{0.2(18)}{0.2+0.1} = 3.6 / 0.3 = 12\] and
\[c_{bi}' = \frac{0.1(18)}{0.2+0.1} = 1.8 / 0.3 = 6\] Now that 8 has
been scaled to 12 and 10 to 6, the fact that corpus $b$ has more words
has been neutralized. This reflects the intuition that something that
occurs 8 of 40 times is more important than something that occurs 10
of 100 times.

% this is the (*2n / N) bit
The second normalization normalizes all values in both permutations
with respect to each other. This is simple: find the average number of
times each feature appears, then divide each scaled count by it. This
produces numbers whose average is 1.0 and whose values are multiples
of the amount that they are greater than the average.  The average
feature count is $N / 2n$, where $N$ is the number of feature
occurrences in both the permutations and $n$ is the number of feature
types. Division by two is necessary since we are multiplying counts
from a single permutation by merged counts from both
permutations. Each entry in the ratio vector now becomes \[\forall j
\in a,b : r_{ji} = \frac{2nc_{ji}'}{N}\]

For example, given the previous example numbers, this second
normalization first finds the average. Assuming 5 unique features for
$a$'s 40 total features and 30 for $b$'s total 100 features gives \[n
= 5 + 30 = 35\] and
\[N = 40 + 100 = 140\]
Therefore, the average feature has $140 / 2(35) = 2$
occurrences in $a$ and $b$ respectively. Dividing $c_{ai}' = 12$ and
$c_{bi}' = 6$ by this average gives $r_{ai} = 6$
and $r_{bi} = 3$. In other words, $r_{ai}$ occurs 6 times more
than the average feature.

\subsection{Syntax Features}
\label{syntactic-features}

In order to investigate hypothesis 1, the distance measure, such as
$R$, needs features that capture the dialect syntax of the interview
corpora given as input. Following Nerbonne and Wiersma 2006, I will
start with parts of speech, then add the leaf-ancestor paths that I
tried on the ICE-GB, and finally add dependency-ancestor paths, as
well as variants on these three feature sets. These feature sets each
depend on a different type of automatic annotation, which described in
section \ref{parsers}.

\namecite{nerbonne06} argue that POS trigrams can accurately represent
at least the important parts of syntax, similar to the way chunk
parsing can capture the most important information about a
sentence. If this is true, POS trigrams are a good starting point for
a language model; they are simple and easy to obtain in a number of
ways. They can either be generated by a tagger as Nerbonne
and Wiersma did, or taken from the leaves of the trees of a
syntactically annotated corpus as I did with the
International Corpus of English \cite{sanders07}.

On the other hand, it might be better to represent the upper structure
of trees in the feature set, assuming that syntax is in fact a
phenomenon that involves hidden structure above the visible words of
the sentence. \quotecite{sampson00} leaf-ancestor paths provide one
way to do this: for each leaf in the parse tree, leaf-ancestor paths produce
the path from that leaf back to the root. Generation is simple as long
as every sibling is unique. For example, the parse tree

\Tree[.S [.NP [.Det the ] [.N dog ] ] [.VP [.V barks ] ] ]

creates the following leaf-ancestor paths:

\begin{itemize}
\item S-NP-Det-The
\item S-NP-N-dog
\item S-VP-V-barks
\end{itemize}

For identical siblings, brackets must be inserted in the path to
disambiguate the first sibling from the second.
There is one path for each word, and the root appears
in all four. However, there can be ambiguities if some
node happens to have identical siblings. Sampson gives the example
of the two trees

\Tree[.A [.B p q ] [.B r s ] ]

and

\Tree[.A [.B p q r s ] ]

which would both produce

  \begin{itemize}
  \item A-B-p
  \item A-B-q
  \item A-B-r
  \item A-B-s
  \end{itemize}

  There is no way to tell from the paths which leaves belong to which
  B node in the first tree, and there is no way to tell the paths of
  the two trees apart despite their different structure. To avoid this
  ambiguity, Sampson uses a bracketing system; brackets are inserted
  at appropriate points to produce
  \begin{itemize}
  \item $[$A-B-p
  \item A-B]-q
  \item A-[B-r
  \item A]-B-s
  \end{itemize}
and
  \begin{itemize}
  \item $[$A-B-p
  \item A-B-q
  \item A-B-r
  \item A]-B-s
  \end{itemize}

Left and right brackets are inserted: at most one
in every path. A left bracket is inserted in a path containing a leaf
that is a leftmost sibling and a right bracket is inserted in a path
containing a leaf that is a rightmost sibling. The bracket is inserted
at the highest node for which the leaf is leftmost or rightmost.

It is a good exercise to derive the bracketing of the previous two trees in detail.
In the first tree, with two B
siblings, the first path is A-B-p. Since $p$ is a leftmost child,
a left bracket must be inserted, at the root in this case. The
resulting path is [A-B-p. The next leaf, $q$, is rightmost, so a right
bracket must be inserted. The highest node for which it is rightmost
is B, because the rightmost leaf of A is $s$. The resulting path is
A-B]-q. Contrast this with the path for $q$ in the second tree; here $q$
is not rightmost, so no bracket is inserted and the resulting path is
A-B-q. $r$ is in almost the same position as $q$, but reversed: it is the
leftmost, and the right B is the highest node for which it is the
leftmost, producing A-[B-r. Finally, since $s$ is the rightmost leaf of
the entire sentence, the right bracket appears after A: A]-B-s.

At this point, the alert reader will have
noticed that both a left bracket and right bracket can be inserted for
a leaf with no siblings since it is both leftmost and rightmost. That is,
a path with two brackets on the same node could be produced: A-[B]-c. Because
of this redundancy, single children are
excluded by the bracket markup algorithm. There is still
no ambiguity between two single leaves and a single node with two
leaves because only the second case will receive brackets.

% See for yourself:
% \[\xymatrix{
%   &\textrm{A} \ar@{-}[dl] \ar@{-}[dr] &\\
%   \textrm{B} \ar@{-}[d] &&\textrm{B} \ar@{-}[d] \\
%   \textrm{p} && \textrm{q} \\
% }
% \]

% \[\xymatrix{
%   &\textrm{A} \ar@{-}[d] &\\
%   &\textrm{B} \ar@{-}[dl] \ar@{-}[dr] & \\
%   \textrm{p} && \textrm{q} \\
% }
% \]
% \cite{sampson00} also gives a method for comparing paths to obtain an
% individual path-to-path distance, but this is not necessary for the
% permutation test, which treats paths as opaque symbols.


Sampson originally developed leaf-ancestor paths as an improved
measure of similarity between gold-standard and machine-parsed trees,
to be used in evaluating parsers. The underlying idea of a collection
of features that capture distance between trees transfers quite nicely
to this application. I replaced POS trigrams with leaf-ancestor paths
for the ICE corpus and found improved results on smaller corpora than
Nerbonne and Wiersma had tested \cite{sanders07}. The additional
precision that leaf-ancestor paths provide appears to aid in attaining
significant results.

\subsubsection{Leaf-Head Paths}
\label{leaf-head-paths}
% TODO: This section should probably have a lot more examples and maybe some
% examples of other applications besides my experiment.

For dependency parses, it is easy to create a variant of leaf-ancestor
paths called ``leaf-head paths''. Like leaf-ancestor paths, each word
in the sentence is associated with a single leaf-head path. The
difference is that the path is from the leaf to the head of the sentence via the
intermediate heads. For example, the same sentence, ``The dog barks'',
produces the following leaf-head paths, given the dependency parse in
figure \ref{example-dep-parse}:

\begin{figure}
\[\xymatrix{
& & root \\
DET \ar@/^/[r] & NP\ar@/^/[r] & V \ar@{.>}[u] \\
The & dog & barks
}
\]
\caption{Dependency parse for ``The dog barks.''}
\label{example-dep-parse}
\end{figure}

\begin{itemize}
\item root-V-N-Det-the
\item root-V-N-dog
\item root-V-barks
\end{itemize}

The biggest difference between leaf-ancestor paths and leaf-head paths
is the relative length of the paths: long
leaf-ancestor paths indicate deep nesting of structure, while short
ones indicate flatter structure. Length is a
weaker indicator of deep structure for leaf-head
paths; for example, the verb in a nested clause has a much shorter
leaf-head path than leaf-ancestor path, but its dependents have
comparable lengths between the two types of paths. Instead, length of
path measures centrality to the sentence; longer leaf-head paths
indicate less important words.

Leaf-head paths represent a compromise between leaf-ancestor paths and
trigrams. Like trigrams, they capture lexical context, but the context
is based on head dependendies, so long-distance context is
possible. Like leaf-ancestor paths, they capture information about the
nested structure of the sentence, although not as completely or
explicitly.

\subsection{Alternate Feature Sets}
\label{alternate-feature-sets}

This section describes the variants besides the main feature sets
already described above: trigrams, leaf-ancestor paths and leaf-head
paths. Most are variants on these three main sets.

\subsubsection{Phrase Structure Rules}

Phrase structure rules are extracted from the same parses as
leaf-ancestor paths, but instead of capturing a series of parent-child
relations, it captures single-level parent-child-sibling
relations. For example, given the tree in figure
\ref{psg-example-tree} the extracted rules are given in
figure \ref{psg-example}.

\begin{figure}
\Tree[.S [.NP [.Det the ] [.N dog ] ] [.VP [.V barks ] ] ]
 \caption{Example Tree}
  \label{psg-example-tree}
\end{figure}

\begin{figure}
  \begin{tabular}{ccc}
    \Tree[.S NP VP ] & \Tree[.NP Det N ] & \Tree[.VP V ] \\
  \end{tabular}
 \caption{Phrase-Structure Rules Extracted}
  \label{psg-example}
\end{figure}

Phrase structure rules are most similar to leaf-ancestor paths in
emphasising the upper structure of constituency parse trees. Unlike
leaf-ancestor paths, they capture some context to the left and
right. They also only cover one level in the tree, whereas
leaf-ancestor paths traverse it from leaf to root. Phrase structure
rules have the possibility to be useful in sentences where context is
important, but they also depend on having accurate parses even at the
top of the tree. This is difficult for automatic parsers to achieve.

\subsubsection{Grandparent Phrase Structure Rules}

Grandparent phrase structure rules are a variant of phrase structure
rules that include the grandparent as well. Given the tree in figure
\ref{psg-example-tree}, the extracted features are given in
figure \ref{grand-psg-example}.

\begin{figure}
  \begin{tabular}{ccc}
    \Tree[.ROOT [.S NP VP ] ] & \Tree[.S [.NP Det N ] ] &
    \Tree[.S [.VP V ] ] \\
  \end{tabular}
\caption{Grandparent Phrase-Structure Rules Extracted}
 \label{grand-psg-example}
\end{figure}

Grandparent phrase structure rules add some of the vertical information present in
leaf-ancestor paths, hopefully without introducing data sparseness
problems. However, they retain the advantage over
leaf-ancestor paths of capturing left and right context.

\subsubsection{Arc-Head Paths}

As described in section \ref{leaf-head-paths}, the usual labels for
leaf-head paths are the leaves of the tree: `root-V-N-Det-the' is the
first leaf-head path for ``The dog barks'' (Det N V). However, one can
also use the arc labels of the depdency parse to create arc-head
paths. These paths have the same shape as their corresponding
leaf-head-paths, but use different labels.

For the previous example, the sentence is now

\[\xymatrix{
& & root \\
The \ar@/^/[r]^{DT} & dog \ar@/^/[r]^{SS} & barks \ar@{.>}[u]
}
\]

and the arc-head paths are

\begin{itemize}
\item root-SS-DT-the
\item root-SS-dog
\item root-barks
\end{itemize}

\subsubsection{Tags from Berkeley Parser}

The Berkeley parser, as described in section \label{parsers}, can either tag
incoming sentences with its own part of speech tagger (based on the
rest of the parser) or with parts of speech specified externally. In
other words, it competes as a part-of-speech tagger with
T'n'T. Although the tags are not as good, it is interesting to see
how they change the overall results when given as input to the
distance measures and to MaltParser.

Specifically, instead of the input corpus being fed through T'n'T to
first to produce tags, the Berkeley parser is given the same
corpus. Along the way of parsing trees, it also tags words with parts
of speech. After parsing, these parts of speech are extracted from the
leaves of the parse trees. First, the parts of speech are used to
create trigram features. Second, the Berkeley-tagged words are
converted to CONLL format and given to MaltParser for dependency
parsing. This produces dependency parses based on parts of speech from
the Berkeley parser.

\subsubsection{Tags from MaltParser trained with Timbl}

Since MaltParser uses Nivre's oracle-based dependency parsing
algorithm, the default oracle, based on support vector machines, can
be replaced with Timbl, the Tilburg Memory-Based Learner. It is
possible that a memory-based learner will provide better parsing
because support vector machines depend on large
training corpora to provide good results. In contrast, a memory-based
learner can obtain good results on limited training if the training
happens to be representative and the right combination of parameters
can be found for Timbl.

This is, however, somewhat complicated since Timbl is quite sensitive
to parameter changes and usually requires specific tuning for
particular tasks. To find the best
parameters, I use a linear search across a number of the major
distance measures provided by Timbl, as well as fallback-combinations
from more complicated distance measures to less complicated ones.

Each combination was evaluated with ten-fold cross-validation on
Talbanken. The best combination, Jeffrey's distance with XX and YY
settings, was used as a basis for parsing and parsing proceeded
identically to previous runs.

\subsubsection{Within-clause Dependency/Leaf-ancestor paths}

I haven't done this. This is interview data and there are not many
nested clauses---probably less than 1 in 3 and I don't think it would make much
difference. Besides, it would be difficult to specify a set of
criteria for cutting off within-a-clause---simply removing everything
between the root and the first S would miss some nested clauses.

All right, so maybe you really could just use parts of speech to
tell. Even if it worked, I still don't think there would be much
difference because very few of the important features without
within-clause cutoff have multiple clauses--most are simple and
non-nested. Oh, right. Simplifying to ignore clause nesting would
increase the power of phenomena that happen regardless of nesting
level but don't occur enough to be visible otherwise.

\subsection{Combining Feature Sets}
\label{combine-feature-sets}

Combining feature sets gives the classifier more information about a
site by combining the information that each feature set
captures. This dissertation uses a simple linear combination. In other
words, when combined, all features are counted together with equal
weight. This is easy and should suffice in allowing the feature ranker
to find a greater variety features from different feature sets that capture the
same underlying syntactic information.

% Hey, these should really be feature *type* sets since they are sets
% of feature types, not sets of observed feature tokens. Maybe I
% should make a note of this early on in the dissertation and say
% "feature sets" is used in the rest of this dissertation to mean
% "feature type sets" (except maybe in the discussion of the
% normalisation, which should refer to vectors anyway)

A more sophisticated method of combining features can be
adapted from Martin Volk's method for deciding prepositional phrase
(PP) attachment \cite{volk02}. Volk uses a number of different feature
sets to decide where to attach PPs. However, he also weights each
feature set differently based on its reliability.

Volk used an a priori reliability measure for ranking
quality of combined feature types; I use number of significant
region differences for ranking: the top-ranked feature set will be
the one that produces the highest number of significant distances
between regions. So, if a feature set finds significant differences
between 90 out of 100 regions, its weight will be 0.90 and will be double
the weight for a feature set that only finds significant differences
betwen 45/100 regions.

% I don't think I'll do this (I probably won't even do the Volk thing
% since it involves some kind of Black Magic in icecore.h.
% Well, I *could* instead replicate each feature by its weight
% before combining, but this would probably make classification super
% slow because of the huge number of features.)
% ----
% Combinations of feature types will be ranked by
% averaging the number of significant distances that the constituent
% feature types produce.

\section{Pre- and Post-processing}
To investigate the first hypothesis, I need a dialect corpus that can
be syntactically annotated (\ref{syntactically-annotated-corpus}); if
it is not already annotated, it must be possible to annotate it
automatically so I can avoid time-consuming manual annotation.
Automatic annotation will require a syntactically annotated
training corpus (\ref{syntactically-annotated-training}) and a parser
(\ref{parsers}). A distance measure must be defined for the regions
within the dialect corpus (\ref{nerbonne06}), syntactic features must
be extracted for the distance measures (\ref{syntactic-features}), and
the results tested for significance (\ref{permutationtest}) and
clustered (\ref{cluster-analysis}) to determine which dialect regions
are found by the corpus. Finally, the most highly ranked features used
to produce the dialect distances must be enumerated
(\ref{feature-ranking}).

To investigate the second hypothesis, I need a method to combine
different types of features (\ref{combine-feature-sets}). I also need
a way to generate new features that include more information about
context (\ref{alternate-feature-sets}).

If the distance measure $R$ doesn't provide any significant distances
with any combination of features, I will experiment with different
distance measures (\ref{alternate-distance-measures}). For this, there
are quite a few possibilities; Kullback-Leibler divergence is one
example.

To investigate the third hypothesis, I need a phonological corpus and a method
for calculating phonological dialect distance, then a method to compare
phonological clusters with syntactic clusters. See my qualifying paper
\cite{sanders08b} for details.

\subsection{SweDiaSyn} % This is not a good subsection for the new organization
\label{syntactically-annotated-corpus}
The first hypothesis requires a dialect corpus that can
be syntactically annotated.
The dialect corpus used in this dissertation will be SweDiaSyn, the
Swedish part of the ScanDiaSyn.
% (CITE SweDiaSyn and ScanDiaSyn,
% except that they don't seem to have any references)
% Here is a citation for ScanDiaSyn if I could track it down and
% translate it
% Vangsnes, Øystein A. 2007. ScanDiaSyn: Prosjektparaplyen Nordisk dialektsyntaks. In T. Arboe (ed.), Nordisk dialektologi og sociolingvistik, Peter Skautrup Centeret for Jysk Dialektforskning, Århus Universitet. 54-72.
SweDiaSyn is a transcription of SweDia 2000 \cite{bruce99} collected
between 1998 and 2000 from 97 locations in Sweden and 10 in
Finland. Each location has 12 interviewees: three 30-minute interviews
for each of older male, older female, younger male and younger female.
However, the SweDiaSyn transcriptions do not yet include all of SweDia
2000; the completed transcriptions currently focus on older
speakers.

Currently there are 36,713 sentences of transcribed speech
from 49 sites, an average of 749 sentences per site.
However, the sites range from 110 to 1780 sentences because some sites
have fewer complete transcriptions than others. In order to detect
significant differences, the sites may need to be grouped by county,
traditional province or EU region; previous work on British English
used EU Government Office Regions with at least 850 sentences per
region. For example, grouping the Swedish corpora into the 25 provinces
boosts the average sentences per province to 1254, excluding provinces
with no transcriptions.

% TODO: Probably switch the second sentence to be first? It's the more
% important but might completely depend on details in the first.
In the SweDiaSyn, there are two types of transcription:
standard Swedish orthography, with glosses for words
not in standard Swedish, and a phonetic transcription for dialects
that differ greatly from standard Swedish. For this dissertation,
the orthographic/gloss transcription will be used so that lexical
items will be comparable across dialects.

\subsection{Talbanken}
\label{syntactically-annotated-training}

Because the first hypothesis requires a syntactically annotated
corpus, and because SweDiaSyn consists of untagged lexical items,
Talbanken05, a syntactically-annotated corpus, will be used to train a
POS tagger and parsers to be used to annotate SweDiaSyn.  Talbanken05
is a treebank of written and transcribed spoken Swedish, roughly
300,000 words in size. It is an updated version of Talbanken76
\cite{nivre06}; Talbanken76's trees are annotated following a custom
scheme called MAMBA; Talbanken05 adds phrase structure annotation and
dependency annotation using the standard annotation formats TIGER-XML
and Malt-XML.  In addition to syntactic annotation, Talbanken is
lexically annotated for morphology and part-of-speech.

% TODO: Should I keep this? It depends on how much detail I want.
% Talbanken's sources are X and Y and Z. It attempts to provide a
% valid sample of the Swedish language, both spoken and written. The
% spoken section is transcribed from conversation, interviews and
% debates, and the written section is taken from high school essays and
% professional prose (TODO:I could probably cite
% Jan Einarsson. 1976. Talbankens skriftspraakskonkordans. Lund
% University: Department of Scandinavian Languages (and
% talspraakskonkordans) IF I could legitimately claim that I got the
% information from there\ldots{} but of course I got it from
% spraakbanken.gu.se/om/eng/index.html actually.

\subsection{Parsing}
\label{parsers}
% TODO: This should have a lot more detail probably. Like
% mini-explanations of how the algorithms actually work.
In order to extract the features used to build the language models
described in the previous methods, SweDiaSyn will need to be POS
tagged and parsed. For this dissertation, both constituency
and dependency features will be provided to the classifier.

The Tags 'n' Trigrams (T'n'T) tagger \cite{brants00} will be used for tagging, with
the POS annotations from Talbanken05 used as training.
After POS tagging, the Talbanken sentences will be cleaned in order to
be usable for training the parsers.
Cleaning Talbanken's constituency annotations consists of removing
discontinuities of various types, especially disfluencies and
restarts, which may be reparable by a simple top-level strategy. If
more complicated uncrossing is needed, a strategy similar to the split
constituents proposed by \namecite{boyd07} may be needed.

For constituency parsing, the Berkeley parser \cite{petrov08} will be
trained on standard Swedish, again from Talbanken05. The Berkeley
parser has shown good performance on languages other than English,
which is not common for constituency parsers.
% TODO: CITE The paper that shows this. Also EXPLAIN it.

For dependency parsing, MaltParser will be used with the existing
Swedish model trained on Talbanken05 by Hall, Nilsson and
Nivre. MaltParser is an inductive dependency parser that uses a
machine learning algorithm to guide the parser at choice points
\cite{nivre06b}.  Dependency parsing will proceed similarly to
constituency parsing; the dependency structures of Talbanken05 will be
cleaned and normalized, then used to train a parser.

% TODO: Find out how much crossing occurs in Swedish corpora, and how
% much of it is from interruptions and self-corrections.

\subsection{Permutation test}
\label{permutationtest}

However, to find out if the value of $R$ is significant, we
must use a permutation test with a Monte Carlo technique described by
\namecite{good95}, following
closely the same usage by \namecite{nerbonne06}. The intuition behind
the technique is to compare the $R$ of the two corpora with the $R$ of
two random subsets of the combined corpora. If the random subsets' $R$s
are greater than the $R$ of the two actual corpora more than $p$ percent
of the time, then we can reject the null hypothesis that the two were
are actually drawn from the same corpus: that is, we can assume that
the two corpora are different.

The first hypothesis requires that the distances produced by a
distance measure be checked for significance; it is possible that
there may not be enough data for two regions to adequately distinguish
them from each other. A permutation test detects whether two corpora are
significantly different on the basis of the $R$ measure
described in section \ref{nerbonne06}. The test first calculates $R$
between samples of the two corpora. Then the corpora are mixed
together and $R$ is calculated between two samples drawn from the
mixed corpus. If the two corpora are different, $R$ should be larger
between the samples of the original corpora than $R$ from the mixed
corpus: any real differences will be randomly redistributed by the
mixing process, lowering the mixed $R$. Repeating this comparison
enough times will show if the difference is significant. Twenty times
is the minimum needed to detect significance for $p < 0.05$
significance; however, in the experiments, I will repeat the test 100
times, enough to detect significance for $p < 0.01$.

To see how this works, for example, assume that $R$ detects real
differences between the two British regions London
and Scotland such that $R(\textrm{London},\textrm{Scotland}) =
100$. The permutation test then mixes London and Scotland to
create LSMixed and splits it into two pieces. Since the real
differences are now mixed between the two shuffled corpora, we
would expect $R(\textrm{LSMixed}_1, \textrm{LSMixed}_2) < 100$.
This should be true at least 95\% of the time for the distance $100$
to be significant.

%% I don't think normalization is important enough to mention if I
%% have to add all the sections from the H2/H3.
% \subsection{Normalization}
% Afterward, the distance must be normalized to account for two things:
% the length of sentences in the corpus and the amount of variety in the
% corpus. If sentence length differs too much between corpora, there
% will be consistently lower token counts in one corpus, which would
% cause a spuriously large $R$. In addition, if one corpus has less
% variety than the other, it will have inflated type counts, because
% more tokens will be allocated to fewer types. To avoid
% this, all tokens are scaled by the average number of types per token
% across both corpora: $2n/N$ where $n$ is the type count and $N$ is
% the token count. The factor $2$ is necessary because the scaling
% occurs based on the token counts of the two corpora combined.

% this next subsection might need to be changed or deleted
\subsection{Cluster Analysis}
\label{cluster-analysis}
The first hypothesis requires a clustering method to allow
inter-region distances to be compared more easily. The dendrogram that
binary hierarchical clustering produces allows easy visual comparison
of the most similar regions. An example is given in figure
\ref{example-dendrogram}.

\begin{figure}
  \Tree[. [. [. A B ] C ] [. D E ] ]
 \caption{Hierarchical Cluster Dendogram}
  \label{example-dendrogram}
\end{figure}

A clustering algorithm provides understanding of which regions group
together. There are a variety of clustering algorithms, but
hierarchical clustering is the most appropriate method for this
problem because it does not specify the number of groups ahead of
time. Other clustering algorithms such as k-means or expectation
maximisation require the number of expected
clusters to be given. Hierarchical clustering creates its clusters implicitly by
grouping items using a binary merge operation. The merge is repeated
until a tree with a single root is formed. The implicit clusters can
be extracted by looking at which speakers share the same subtree, as
well as looking for large differences in internal node heights connecting subtrees.

The initial step for any clustering algorithm is to find distances
between all pairs of regions as described above.
These distances between all pairs of regions result in a set of
high-dimensional spatial relationships. While they could be analyzed
as such, such high-dimensional distances are difficult to
visualize. The job of a clustering algorithm is to reduce
the dimensionality and create a useful visualization of the relative
positions of the speakers. Hierarchical clustering does this by
creating a tree---if there are similarities between the speakers, it
should be obvious by looking at the tree.

There are some complications, however. Because the clustering
algorithm is nothing but repeated merges, it is
not always clear at what level the best clusters are formed. For example,
\namecite{clopper04} used a similar clustering algorithm on perceptual
dialect data from American English speakers and found two distinct
North/South clusters for most features. These two clusters had less
defined sub-clusters as well: the Western speakers of American English
usually grouped with the North cluster but slightly separated from the
other dialects in the North cluster. Of course as the sub-clusters
become smaller, they usually become less distinctive because the
distances are smaller. Ultimately, human judgment is necessary to
determine what is a cluster and what is not.

\subsubsection{Bottom-up hierarchical clustering}

With hierarchical clustering defined, ``bottom-up'' now needs a
definition. As in any tree-building problem, the two obvious ways one
can build a tree are top-down and bottom-up. Bottom-up clustering
works in the following way. First, each speaker is put into its own
group. Then the algorithm determines the distance between each pair of
groups. The closest two groups are merged into a single group and the
process is repeated until a single root group is created. This process
is bottom-up because it creates the terminal nodes of the tree first
and builds up the internal structure of the cluster tree from there.

For example, figures \ref{hierarchical-cluster-1} through
\ref{hierarchical-cluster-5} show the sequence of merges need to
produce the dendrogram in figure \ref{example-dendrogram}. On the
first step A and B are merged (figure \ref{hierarchical-cluster-2}),
followed by D and E (figure \ref{hierarchical-cluster-3}). Then C
merges with the A-B cluster (figure
\ref{hierarchical-cluster-4}). Finally the A-B-C cluster and the D-E
cluster merge to form a single tree (figure
\ref{hierarchical-cluster-5}).

\begin{figure}
  \includegraphics[scale=0.7]{hierarchical-cluster-1}
 \caption{Sites Before Clustering}
  \label{hierarchical-cluster-1}
\end{figure}

\begin{figure}
  \includegraphics[scale=0.7]{hierarchical-cluster-2}
 \caption{Sites After A-B Merge}
  \label{hierarchical-cluster-2}
\end{figure}

\begin{figure}
  \includegraphics[scale=0.7]{hierarchical-cluster-3}
 \caption{Sites After D-E Merge}
  \label{hierarchical-cluster-3}
\end{figure}

\begin{figure}
  \includegraphics[scale=0.7]{hierarchical-cluster-4}
 \caption{Sites After A-B-C Merge}
  \label{hierarchical-cluster-4}
\end{figure}

\begin{figure}
  \includegraphics[scale=0.7]{hierarchical-cluster-5}
 \caption{Sites After Clustering}
  \label{hierarchical-cluster-5}
\end{figure}

To find the two closest groups, \quotecite{ward63} method is used. At
each merge step, this method evaluates every possible binary
merge. Each merge is given a score that minimises some objective
function---for example, the average of distances between regions in
the new group. The best merge replaces its children and the process
repeats until a singly rooted tree is created. For $n$ regions, this
takes $n-1$ iterations, because at each step, two groups are merged.

\begin{table}
  \begin{tabular}{c|cccc}
    & B & C & D & E \\ \hline
    A & 10 & 20 & 40 & 50 \\
    B & & 20 & 50 & 40 \\
    C &&& 30 & 30\\
    D &&&& 12\\
  \end{tabular}
 \caption{Example dissimilarities}
  \label{cluster-distances}
\end{table}

For example, using the distances in table \ref{cluster-distances} for
the five example regions, the first merge is trivial since each region
starts in its own singleton tree (figure \ref{ward-cluster-1}): the
distance between A and B, 10, is the minimum and thus the best. This
produces the forest in figure \ref{ward-cluster-2}.

\begin{figure}
  \Tree[. A ]
  \Tree[. B ]
  \Tree[. C ]
  \Tree[. D ]
  \Tree[. E ]
 \caption{Ward's method, before clustering}
  \label{ward-cluster-1}
\end{figure}

\begin{figure}
  \Tree[. A B ]
  \Tree[. C ]
  \Tree[. D ]
  \Tree[. E ]
 \caption{Ward's method, after A-B merge}
  \label{ward-cluster-2}
\end{figure}

The distances between the A-B tree and the others are now more
complicated to calculate: the A-B-C merge has a distance of $10+20+20
/ 3$ = 16.6. This is smaller than the A-B-D merge ($10+40+50 / 3 =
33.3$), but larger than the D-E merge ($12 / 1 = 12$) which eventually
turns out to be the smallest merge, producing figure
\ref{ward-cluster-3}.

\begin{figure}
  \Tree[. A B ]
  \Tree[. C ]
  \Tree[. D E ]
 \caption{Ward's method, after D-E merge}
  \label{ward-cluster-3}
\end{figure}

The next merge is primarily concerned with where C will merge, whether
with A-B or D-E; an A-B-D-E merge is much larger at $10 + 40 + 50 + 50
+ 40 + 12 / 6 = 33.6$. As previously calculated, the A-B-C merge is
$16.6$, while a C-D-E merge is $30 + 30 + 12 / 3 = 24$. So the new
merge is A-B-C, producing figure \ref{ward-cluster-4}.

\begin{figure}
  \Tree[. [. A B ] C ]
  \Tree[. D E ]
 \caption{Ward's method, after A-B-C merge}
  \label{ward-cluster-4}
\end{figure}

The two remaining trees are merged. Here, the final value of the
objective function is the average of all distances in the table, that
is $302 / 10 = 30.2$. The final tree is given in figure
\ref{ward-cluster-5}.

\begin{figure}
  \Tree[. [. [. A B ] C ] [. D E ] ]
 \caption{Ward's method, after clustering}
 \label{ward-cluster-5}
\end{figure}

Ward's method is less efficient than other common clustering methods,
but it usually finds small, round clusters, making it worth the extra
computer time. In contrast, single-link distance, for example,
compares only the two closest elements of the two members of a
possible merge. This is faster, but is susceptible to creating thin,
oval groups---even though the bulk of a group may be distant, a single
outlier usually leads to a bad grouping, which recursively leads to
further outliers.

\subsubsection{Consensus Trees}

A weakness of cluster dendrograms is that small variations in
distances can cause large changes in the cluster membership of
sites. Consensus trees circumvent this weakness by combining the
results of multiple related dendrograms. Only the clusters that occur
in the majority of dendrograms appear in the consensus tree. For a
survey of consensus trees, see chapter 6 of \cite{bryant97}.

Consensus trees can be constructed by an algorithm with three primary
steps. First, the algorithm finds the spans of every internal
node in every tree. That is, each non-terminal node $N$ is replaced
with the terminal nodes $w_i \ldots w_j$ that it dominates. Second,
the algorithm counts span types and retains only the spans that occur
in a majority of dendrograms. For example, if there are $9$
dendrograms, the consensus tree will contain only spans that occur in
$5$ or more of them. Third, the spans are reconstructed into a single
tree. Nodes that no longer have a direct parent are added to a higher
ancestor.

\begin{figure}
  \Tree[. A [. [. [. B C ] D ] E ] ]
  \Tree[. A [. [. B [. C D ] ] E ] ]
  \Tree[. A [. B [. [. C D ] E ] ] ]
  \caption{Input cluster dendrograms}
  \label{consensus-example-input}
\end{figure}

\begin{figure}
  \Tree[. A [. [. B C D ] E ] ]
  \caption{Output consensus dendrogram}
  \label{consensus-example-output}
\end{figure}

For example, consider the three hierarchical dendrograms of figure
\ref{consensus-example-input}. They cluster the input set \{A B
C D E\} three different ways. The majority-rule consensus tree for
these three trees is given in figure \ref{consensus-example-output}.
This example is adapted from the one given by \namecite{amenta03}.

The spans for the internal nodes of the three trees are given in
figure \ref{consensus-tree-spans}. There is quite a bit of overlap at
higher levels, but near the leaves, \{B C\} and \{C D\} vary, as
do \{B C D\} and \{C D E\}. As a result, when the spans are
combined and counted, \{B C\} and \{C D E\} only appear once. This
means that they will be dropped from the consensus tree because they
appear in 1/3 of the trees and because 1/3 is less than or equal to 1/2,
they are not majority spans.

\begin{figure}
\begin{tabular}{l|l|l}
\hline
\{B C\}&       \{C D\}    &        \{C D\}           \\
\{B C D\}& \{B C D\}  &          \{C D E\}       \\
\{B C D E\}&  \{B C D E\} &  \{B C D E\}  \\  
\{A B C D E\}& \{A B C D E\}& \{A B C D E\}\\ 
\end{tabular}
\caption{Spans from input trees}
\label{consensus-tree-spans}
\end{figure}

\begin{figure}
  \begin{tabular}{l|ll}
  \{B C\} & 1 / 3 & * \\
  \{C D\} & 2 / 3 \\
  \{B C D\} & 2 / 3 \\
  \{C D E\} & 1 / 3 & * \\
  \{B C D E\} & 3 / 3 \\
  \{A B C D E\} & 3 / 3 \\
  \end{tabular}
  \caption{Span type frequencies (starred rows do not occur in the majority
  of trees)}
  \label{consensus-tree-span-types}
\end{figure}

\begin{figure}
  \begin{tabular}{l}
    \{C D\} \\
    \{B C D\} \\
    \{B C D E\} \\
    \{A B C D E\} \\
  \end{tabular}
  \caption{Majority span types}
  \label{consensus-tree-majority-spans}
\end{figure}

Reconstruction is fairly simple; taken from the top down, both \{A\}
and \{B C D E\} must be children of \{A B C D E\}. Because \{A\} is
not a member of the majority spans, it is added directly as a child of
\{A B C D E\}. Although this occurs in all three original trees, this
is not the case when \{B C D\} adds \{B\} as a child. The result is
that \{B C D\} has ternary branching, which was not present in any of
the original trees.

As can be seen from the example, high degrees of branching in the
consensus tree near the leaves indicate that the original trees do not
agree well. Therefore, it is not safe to draw conclusions from an
original tree in the areas where it disagrees with other original
trees.

\subsection{Multi-dimensional scaling}

Multi-dimensional scaling (MDS) is an alternate approach to making the
high-dimensional dissimilarities more easily interpretable. Instead
of creating a tree, multi-dimensional scaling reduces the
high-dimensional dissimilarities to 3 dimensions, which can
then be represented using (Red,Green,Blue) colour triples. When
painted on a map, these colours provide a nice visualisation of
the regions that similar sites form as well as how sharp the boundaries are
with other regions.

MDS of dissimilarities uses \quotecite{kruskal64a} method. It reduces
$m$ dissimilarities to an $n$ dimensional space by distorting the
individual dissimilarities by the minimum amount needed to convert
them into distances in $n$ dimensions. Since dissimilarities do not
satisify the triangle inequality, this means finding the minimum
amount all dissimilarities need to change in order to satisfy it,
turning them into distances.

\begin{table}
  \begin{tabular}{c|cccc}
    & B & C & D & E \\ \hline
    A & 10 & 20 & 40 & 50 \\
    B & & 20 & 50 & 40 \\
    C &&& 30 & 30\\
    D &&&& 12\\
  \end{tabular}
 \caption{Example dissimilarities}
  \label{mds-distances}
\end{table}

\begin{figure}
  \includegraphics[width=0.4\textwidth]{Sverigekarta-Landskap-mds-dep}
  \label{mds-dep}
  \caption{Example of MDS on Swediasyn}
\end{figure}

Kruskal calls this measure of distortion Stress.  An initial stress is
obtained by sorting the dissimilarities by size and measuring how far
each dissimilarity would have to change in order for all to satisfy
the triangle inequality. This initial stress is reduced by a process
of gradient descent as described in \cite{kruskal64b}.

\subsubsection{Principal Components Analysis}

Principal Components Analysis (PCA) is useful in conjuction with MDS
because the 3 (or so) dimensions that MDS produces can be correlated
with features. Since the features correlated with the first few dimensions are
the most important, they can be used when comparing with dialectology
results.

TODO: Research this and get the details of the math.
Pretty sure that L04 does it for me. Probably. (The L04 tutorial also mentions
another method that I can't remember the name of.)

\subsubsection{Cluster Boundary Diagrams}

L04 does these too, Peter's site has an example. I'm not sure they'll
work but it's worth a try.

\subsection{Correlation}

Correlation is interesting on two levels. First, it is useful to find
out how well the results from various measures and feature sets
correlate. Second, it is useful to find out how well the results agree
with outside authorities. In particular, correlation with geographical
distance is strong circumstantial evidence that a distance measure is
doing its job. However, comparison to other work on the same corpus,
such as phonological dialectometry \cite{leinonen08}, is also
informative since it is possible that phonological distance matches
syntactic distance.

However, the significance from Pearson's $R$, the usual measure of
correlation, is overestimated for inter-region distances because of
the way that all regions connect to all other regions. A large part of
the significance found by Pearson's $R$ would arise from the massively
overlapping information in the distances between fully connected
regions. Mantel's test provides tests Pearson's $R$ for significance
between inter-connected sites \cite{mantel67}. Mantel's test is much like
the permutation test for significance described above.  It first finds
the correlation between two sets of distances.  One distance result
set is permuted repeatedly and at each step correlated with the other
set. The original correlation is significant if the permuted
correlation is lower than the original correlation more than 95\% of
the time.


\subsection{Feature Ranking}
\label{feature-ranking}

Feature ranking is needed to compare the results of $R$ qualitatively
to the Swedish dialectology literature; $R$'s most important features
should be similar to those discussed most by dialectologists when
comparing regions. Without feature ranking of some kind, there is no
way to relate the quantitative distances between regions with the
features that contribute most to the distances.

A simple feature ranking for $R$ is easy for
one-to-one region comparisons; each feature's normalized weight is
equal to its importance in determining the distance between the two
regions. See figure \ref{feature-ranking-1-1}: features that appear
more often in one region of the compared pair are negative, while features
that appear more often in the other region are positive. In the
example, the bigram AB-AJ occurs more often in the left-hand-region,
while NN-VB and PN-VB occur more often in the right-hand-region.

\begin{figure}
  \includegraphics[scale=0.8]{feature-ranking-1-1}
  \label{feature-ranking-1-1}
  \caption{Feature-ranking 1:1}
\end{figure}

Features can be ranked between a single region and multiple regions by
averaging. For example, in figure \ref{feature-ranking-1-many}, the
binary comparison between the left-hand region and each of the three
right-hand regions produces three sets of features. The features can
be combined by averaging the score for each feature type. NN-VB's
averaged score would be $50 + 20 + 80 / 3 = 50$, for example.

\begin{figure}
  \includegraphics[scale=0.8]{feature-ranking-1-many}
  \label{feature-ranking-1-many}
  \caption{Feature-ranking 1:Many}
\end{figure}

This average can be extended to compare two sets of regions. As can be
seen in figure \ref{feature-ranking-many-many}, each feature on the
right-hand side is no longer a single number, but an average of the
comparison against each region on the left-hand side. Therefore, in
this example, NN-VB's overall average score is

\[ \bigg(\frac{50+30}{2} + \frac{20+30}{2} + \frac{80+30}{2}\bigg) / 3 =
\frac{50+30+20+30+80+30}{6} = 40\]

\begin{figure}
  \includegraphics[scale=0.8]{feature-ranking-many-many}
  \label{feature-ranking-many-many}
  \caption{Feature-ranking Many:Many}
\end{figure}

This method is similar to a ranking by \namecite{wiersma09}, but
without the additional normalisation needed to make his test for
significance work; we do not need this test because I sample by
sentence rather than by speaker. However, Wiersma also runs a third
normalisation in which each type count is divided by the average of
that type count over all {\it permutations}. I can't see why this
works, since permutations necessarily mix the tokens from both corpora
together.

The main reason is that Wiersma samples per-speaker from a large
number of speakers and small number of sentences from each
speaker. This allows him to avoid the assumption that each sentence is
independent and instead assume that each {\it speaker} is
independent. However, this also requires an additional per-speaker
normalisation because some speakers may contribute more sentences to
the sample than others. This is not needed for the method used here
because it assumes all sentences to be independent, regardless of
speaker.

TODO: This isn't complete; I'm not convinced that Wiersma's
last normalisation helps with the ranking, but that's because I don't
understand why he expects it to help.


\subsection{Alternate Distance Measures}
\label{alternate-distance-measures}

There are several reasons to test distance measures besides
$R$. There are a couple of a priori reasons for this: $R$ is fairly
simple, so more complicated variations on it may provide better
sensitivity at the expence of sensitivity to noise. Also, variations
explore the measure space better in case that $R$ is not significant
for some combination of corpus/feature set.

Post-hoc, there are interesting patterns of statistical significance
produced by the combination of distance measure and feature set. These
patterns are not trivially obvious. This is not expected, but may
provide insight into the measure/feature combination, which helps
resolving Hypotheses 1 and 2.

% Another possibility is a return to Goebl's Weighted Identity Value;
% this classifier is similar in some ways to $R$, but has not been
% tested with large corpora, to my knowledge at least. (This is not
% particularly useful and I don't believe that WIV would actually be
% good, so I should probably just drop this.)

% (maybe it was relative entropy or just normal-kind entropy).
% TODO: WIV, also Kullback-Leibler Divergence could work.
% Maybe also k-NN/MBL, HMM binary classifier (?), maybe even a
% neural net

\subsubsection{KL divergence}

% TODO: Find original citation for KL divergence

Kullback-Leibler divergence, or relative entropy, is described in
\namecite{manningschutze}.  Relative entropy is similar to $R$ but
more widely used in computational linguistics. The name relative
entropy implies an intuitive interpretation: it is the number of bits
of entropy incurred when compressing a corpus $b$ with the optimal
compression scheme for a second corpus $a$. Unless the two corpora
are identical, the relative entropy $KL(a||b)$ is non-zero because
$a$'s optimal compression scheme will over-compress $b$'s
features that are more common in $a$ than in $b$, whereas it will
under-compress features that are less common in $a$ than in $b$.

For example, assume that corpus $a$ has two features with type counts
\{S-NP-N : 20, S-VP-PP-N : 10\}. An optimal compression scheme for $a$
would compress S-NP-N twice as much as S-VP-PP-N because it occurs
twice as often. However, if this compression scheme is used on a
corpus $b$ with the feature counts \{S-NP-N : 15, S-VP-PP-N : 15\},
efficiency will be worse; S-NP-N and S-VP-PP-N occur the same number
of times in $b$, so the smaller compressed size of S-NP-N will be used
less often than expected, while the larger compressed size of
S-VP-PP-N will be used more. This difference can be measured precisely
for each feature:

\[ c_{ai} \log\frac{c_{ai}}{c_{bi}} \]

where $c_{ai}$ is type count of the $i$th feature in $a$ and $c_{bi}$
is the type count of the $i$th feature in $b$. This measures the
number of bits lost, or entropy, for each feature $i$. Like $R$'s
differences, the per-feature entropy can be summed to find the total
entropy. In the example above, the entropy for S-NP-N is $20
\log\frac{20}{15} = 5.75$.

However, Kullback-Leibler divergence as defined is a divergence: it
measures the divergence of features in the corpus $c_b$ from the
features of corpus $c_a$. A dissimilarity is required for
dialectology, which means that the divergence must additionally be
symmetric. A divergence can be made symmetric by calculating it twice:
the divergence from $c_a$ to $c_b$ added to the one from $c_b$ to
$c_a$. The complete formula is given in equation \ref{klmeasure} and
the complete example is worked in equation \ref{klexample}.

\begin{equation}
KL(a||b) = \sum_i {c_{ai} \log\frac{c_{ai}}{c_{bi}} + c_{bi} \log\frac{c_{bi}}{c_{ai}}}
\label{klmeasure}
\end{equation}

\begin{equation}
 (20 \log\frac{20}{15} + 15 \log\frac{15}{20}) + (10
  \log\frac{10}{15} + 15 \log\frac{15}{10}) = (5.75 - 4.32) + (-4.05 +
  6.08) = 3.46
  \label{klexample}
\end{equation}

\subsubsection{Jensen-Shannon divergence}

Several variants of relative entropy exist that lift various restrictions from the input
distributions. One is Jenson-Shannon divergence \cite{lin91}, which
was designed as a dissimilarity from the start. It uses the same
denominator for both directions: the average of the two
probabilities. That means that each feature's entropy is found using
the following formula:

\[ c_{ai} \log\frac{c_{bi}}{(c_{ai} + c_{bi}) / 2} +
c_{bi} \log\frac{c_{ai}}{(c_{ai} + c_{bi}) / 2} \]

There is a common subexpression in this value: $(c_{ai} + c_{bi}) /
2$: the average of the two features. If we let $\bar{c_i} = (c_{ai} + c_{bi}) /
2$ rewrite the formula to take advantage of this simplification, we
get equation \ref{jsmeasure}.

\begin{equation}
JS = \sum_i {c_{ai} \log\frac{c_{bi}}{\bar{c_i}} + c_{ai}
  \log\frac{c_{bi}}{\bar{c_i}}}
\label{jsmeasure}
\end{equation}

Unlike Kullback-Leibler divergence, Jensen-Shannon divergence does not
require the feature counts to be absolutely continuous; in other
words, that if $c_{ai}$ is non-zero, then $c_{bi}$ has to be non-zero
too. Since the current implementation of Kullback-Leibler divergence
simply skips zero values, this means it ignores features unique to a
particular corpus. Jensen-Shannon divergence avoids this problem.
% In addition, it provides bounds on variational distance
% and the Bayes probability of error.
% TODO: Find out what this means

\subsubsection{Cosine similarity}

Cosine similarity is used in many parts of computational linguistics
and related areas such as information extraction and data
mining. \namecite{nerbonne06} use it as reference point for
comparison to previous work in these areas. Cosine similarity measures
the similarity between two high-dimensional points in space. Each
feature is modelled as a dimension, and the type count from each
corpus is plotted as a point on that dimension. The result is equation
\ref{cosmeasure} or the non-vectorised version in
\ref{cosmeasureiterative}.

\begin{equation}
  \frac{c_{a}\cdot c_{b}} {||c_{ai}||||c_{bi}||}
  \label{cosmeasure}
\end{equation}

\begin{equation}
  \frac{\sum_i {c_{ai}c_{bi}}} {\sqrt{\sum_i {c_{ai}^2}} +
    \sqrt{\sum_i{c_{bi}^2}}}
  \label{cosmeasureiterative}
\end{equation}

Interestingly, the results for this measure are very different from
the other distance measures, possibly because, unlike the others
described, it is not a linear sum.

% \subsubsection{LSA} Nope, didn't work out

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
