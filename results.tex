\chapter{Results}
\label{results-chapter}
% TODO: Many tables are ugly
% TODO: Re-order features in order of importance in all tables
% eg unigram last, preceded by deparc, timbl-dep (redep), etc

These results are meant to answer two main questions: first, how well does
this approach to syntactic dialectometry agree with dialectology?
Second, what combinations of distance measures, feature
sets and other settings produce the best results for linguistic
analyses? Additionally, the results are meant to allow comparison with
phonological dialectometry.

The organisation of this chapter mirrors the order of the methods
chapter, particularly the output analysis (section
\ref{output-analysis}). First, there is an overview of the different
parameter settings, the combinations of distance measure and feature
set, as well as other settings. Then the number of significant
distances for each parameter setting is given, which is followed by
the correlation with geography and travel distance for each parameter
setting. These sections focus mainly on detecting which settings do not
produce valid results, so that they can be ignored in the rest of the
chapter. At a high level, they answer the question of the suitability
of statistical syntactic dialectometry: whether or not significant
results can be found.

Next, the specific dialectological results are examined. First,
cluster dendrograms provide a visualisation of which regions the
distance measures find to be similar. In addition, to improve the
reliability of the dendrograms, consensus trees and composite cluster
maps are produced. Next, multi-dimensional scaling gives an smoother
view of similarity than clusters. Finally, features are ranked and
extracted from each cluster in the consensus tree.

\section{Distances}

There are 180 parameter settings investigated in this chapter. This
number arises from the four parameters: measure, feature set, sampling
method and number of normalisation iterations. 5 measures, 9 feature
sets, 2 sampling methods and 2 numbers of normalisation iterations
gives $5\times 9 \times 2 \times 2=180$ different settings. The
settings are given in table \ref{parameter-settings}.

\begin{table}
\begin{tabular}{|c|} \hline
  Feature Set \\\hline
  Leaf-Ancestor Path \\
  Part-of-speech Trigram \\
  Leaf-Head Path \\
  Phrase Structure Rule \\
  PSR with Grandparent \\
  Part-of-speech Unigram \\
  Leaf-Head Path, based on Timbl training \\
  Leaf-Arc Path \\
  All features combined \\ \hline
\end{tabular}
\begin{tabular}{|c|} \hline
  Measure \\ \hline
  $R$ \\
  $R^2$ \\
  Kullback-Leibler divergence \\
  Jensen-Shannon divergence \\
  cosine dissimilarity\\\hline
\end{tabular}
\begin{tabular}{|c|c|c|} \hline
  Sampling Method & Iterations of normalisations \\ \hline
  1000 sentences & 1 \\
  All sentences & 5 \\\hline
\end{tabular}
\caption{Settings for the five parameters tested}
\label{parameter-settings}
\end{table}
% Actually, all this should probably go in methods too, somewhere as a summary.

\section{Significant Distances}

Significant distances help answer the question whether a syntactic
measure has succeeded in finding reliable distances. The measures
should have zero non-significant distances, or at least a small
number. In the tables, the total number of comparisons between all 30
regions is the $435=30(30-1) / 2$. In the first set,
\ref{sig-1-1000} -- \ref{sig-1-full}, the results are shown
from one iteration of the normalisation step. In the second set,
tables \ref{sig-5-1000} -- \ref{sig-5-full}, the results
from five normalisation iterations are shown.

Bold numbers in the tables indicate that more than 5\% of the
distances were not significant. They are not marked in table
\ref{sig-5-full}, the 5-iteration table that compares full corpora,
because so few distances are significant. In fact, the only
combination with {\it less} than 5\% non-significant results is cosine
dissimilarity with unigram features. Note that here, 5\% is an
arbitrary cutoff point not related to the usual significance cutoff $p
< 0.05$; the basis for these tables are themselves number of
significant distances found.
% TODO:Make sure this is true; Wybo's discussion of family errors
% doesn't seem to apply here but it might.

\begin{table}
\begin{tabular}{l|rrrrr}
  & $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor &0&0&11&0&0 \\
  Trigram &0&0&0&0&0 \\
  Leaf-Head &0&0&0&0&0 \\
  Phrase-Structure Rules &0&0&\textbf{95}&0&0 \\
  Phrase-Structure with Grandparents &0&0&\textbf{273}&0&0 \\
  Unigram &0&0&0&0&0 \\
  Leaf-Head with MaltParser trained by Timbl &0&0&\textbf{47}&0&0 \\
  Leaf-Arc Labels&0&0&0&0&0 \\
  All Features Combined &0&0&0&0&0 \\
\end{tabular}
\caption{Number of significant distances for sample size 1000, 1
  normalisation}
\label{sig-1-1000}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&7&11&12&\textbf{35}&9 \\
  Trigram&4&1&0&\textbf{24}&1 \\
  Leaf-Head&10&12&20&\textbf{44}&19 \\
  Phrase-Structure Rules&\textbf{26}&17&\textbf{24}&\textbf{49}&20 \\
  Phrase-Structure with Grandparents&\textbf{58}&\textbf{35}&\textbf{38}&\textbf{71}&\textbf{33}
   \\
  Unigram&1&2&0&0&2 \\
  Leaf-Head with MaltParser trained by Timbl&11&21&18&\textbf{74}&\textbf{30}
   \\
  Leaf-Arc Labels&14&19&\textbf{37}&\textbf{94}&17 \\
  All Features Combined&0&0&1&8&2 \\
\end{tabular}
 \caption{Number of significant distances for complete regions, 1
   normalisation}
 \label{sig-1-full}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&5 & \textbf{56} & \textbf{34} & 0 & 0\\
  Trigram&3 & 2 & 0 & 0 & 0\\
  Leaf-Head&3 & 14 & 4 & 0 & 0\\
  Phrase-Structure Rules&11 & 4 & \textbf{66} & 1 & 0\\
  Phrase-Structure with Grandparents&18 & 0 & \textbf{109} & 4 & 0\\
  Unigram&\textbf{52} & \textbf{53} & 15 & 17 & 0\\
  Leaf-Head with MaltParser trained by Timbl&7 & 20 & \textbf{45} & 0 & 0\\
  Leaf-Arc Labels&6 & \textbf{54} & 17 & 1 & 0\\
  All Features Combined&0 & 4 & 0 & 0 & 0\\
\end{tabular}
\caption{Number of significant distances for sample size 1000, 5
  normalisations}
\label{sig-5-1000}
\end{table}
\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&\textbf{290} & \textbf{284} & \textbf{287} & \textbf{278} & \textbf{204}\\
  Trigram&\textbf{284} & \textbf{283} & \textbf{283} & \textbf{276} & \textbf{196}\\
  Leaf-Head&\textbf{293} & \textbf{286} & \textbf{285} & \textbf{279} & \textbf{211}\\
  Phrase-Structure Rules&\textbf{289} & \textbf{294} & \textbf{286} & \textbf{275} & \textbf{236}\\
  Phrase-Structure with Grandparents&\textbf{285} & \textbf{290} & \textbf{286} & \textbf{270} & \textbf{258}\\
  Unigram&\textbf{297} & \textbf{296} & \textbf{294} & \textbf{293} &
  \textit{9}\\
  Leaf-Head with MaltParser trained by Timbl&\textbf{294} & \textbf{289} & \textbf{288} & \textbf{284} & \textbf{222}\\
  Leaf-Arc Labels&\textbf{294} & \textbf{290} & \textbf{291} & \textbf{293} & \textbf{162}\\
  All Features Combined&\textbf{279} & \textbf{279} & \textbf{279} & \textbf{269} & \textbf{191}\\
\end{tabular}
 \caption{Number of significant distances for complete regions, 5
   normalisations}
 \label{sig-5-full}
\end{table}


\section{Correlation}

Correlation with geographic distance is another indicator that a
syntactic measure is finding significant distances. Although
correlation with geographic is not a guaranteed indicator that the
measure is successful, the geographic basis of dialect variation is
well-known \cite{chambers98}. In addition to straight-line geographic
distance, travel distance is presented. Travel distance can be more
informative than straight-line distance because it accounts for
natural obstacles such as mountains, lakes and islands.

The tables that present geographic and table correlation,
\ref{cor-1-1000} -- \ref{travel-cor-5-full}, mark significant
correlations with a star for $p < 0.05$, two stars for $p < 0.01$ and
three stars for $p < 0.001$. However, these correlations are only
trustworthy in the case that the underlying distances are
significant. Significant correlations from significant distances (as
cross-referenced from tables \ref{sig-1-1000} -- \ref{sig-5-full}) are
marked by italics.

Besides this, correlation between combinations of measure/feature set
can show how closely related they are--in other words, how similarly
they view the underlying data which remains the same for all.

This is similar to the reasoning behind correlation with
geography---but the assumption is that geography is a factor
underlying dialect formation; while the distance measure measures some
aspect of the language which we hope is dialects, it is indirectly
(even less directly) measuring the geography. Therefore, correlation
with geography should occur.

Third, correlation with corpus size is not predicted and is probably
an undesired defect in sampling or normalisation. Correlation with
corpus size is presented in tables \ref{size-cor-1-1000} --
\ref{size-cor-5-full}.

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&-0.01 & 0.03 & 0.02 & -0.02 & 0.08\\
  Trigram&0.17 & 0.17 & 0.10 & 0.19 & 0.13\\
  Dependency&-0.06 & 0.03 & 0.00 & -0.07 & 0.05\\
  Phrase-Structure Rules&0.01 & \textit{0.18*} & 0.16 & 0.01 & 0.12\\
  Phrase-Structure with Grandparents&0.03 & \textit{0.25*} & 0.21* & 0.03 & 0.12\\
  Unigram&\textit{0.18*} & 0.17 & \textit{0.29**} & \textit{0.30**} & \textit{0.18*}\\
  Dependencies, MaltParser trained by Timbl&-0.07 & 0.02 & -0.00 & -0.08 & 0.05\\
  Dependency, Arc Labels&-0.07 & 0.06 & -0.06 & -0.09 & 0.00\\
  All Features Combined&-0.02 & 0.03 & 0.01 & -0.02 & 0.07\\
\end{tabular}
 \caption{Geographic correlation for sample size 1000, 1 normalisation iteration}
 \label{cor-1-1000}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&0.02 & 0.09 & 0.11 & -0.00 & 0.09\\
  Trigram&\textit{0.27*} & \textit{0.26*} & \textit{0.30**} & 0.21* & 0.08\\
  Dependency&-0.03 & 0.12 & 0.14 & -0.06 & 0.02\\
  Phrase-Structure Rules&0.13 & \textit{0.36**} & 0.30** & 0.11 & \textit{0.20*}\\
  Phrase-Structure with Grandparents&0.15 & 0.41** & 0.36** & 0.14 & 0.19*\\
  Unigram&\textit{0.20*} & \textit{0.20*} & \textit{0.33**} & \textit{0.33**} & \textit{0.22*}\\
  Dependencies, MaltParser trained by Timbl&-0.02 & 0.14 & 0.16 & -0.05 & 0.02\\
  Dependency, Arc Labels&-0.06 & 0.13 & -0.01 & -0.12 & -0.03\\
  All Features Combined&0.03 & 0.11 & 0.16 & -0.00 & 0.04\\
\end{tabular}
 \caption{Geographic correlation for complete corpora, 1 normalisation iteration}
 \label{cor-1-full}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&0.14 & 0.14 & 0.16 & 0.15 & 0.08\\
  Trigram&\textit{0.22*} & 0.17 & \textit{0.22*} & \textit{0.22*} & 0.16\\
  Dependency&0.10 & 0.11 & 0.15 & 0.12 & 0.10\\
  Phrase-Structure Rules&0.14 & 0.10 & 0.14 & 0.15 & 0.06\\
  Phrase-Structure with Grandparents&0.16 & 0.14 & 0.14 & 0.15 & 0.05\\
  Unigram&0.12 & 0.11 & 0.14 & 0.13 & 0.17\\
  Dependencies, MaltParser trained by Timbl&0.09 & 0.12 & 0.16 & 0.11 & 0.11\\
  Dependency, Arc Labels&0.08 & 0.10 & 0.14 & 0.10 & 0.09\\
  All Features Combined&0.19 & 0.16 & \textit{0.20*} & \textit{0.21*} & 0.11\\
\end{tabular}
 \caption{Geographic correlation for sample size 1000, 5
   normalisations}
 \label{cor-5-1000}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&-0.14 & -0.16 & -0.15 & -0.15 & -0.08\\
  Trigram&-0.09 & -0.07 & -0.09 & -0.09 & -0.09\\
  Dependency&-0.22 & -0.21 & -0.18 & -0.22 & -0.10\\
  Phrase-Structure Rules&-0.19 & -0.14 & -0.11 & -0.20 & -0.01\\
  Phrase-Structure with Grandparents&-0.17 & -0.11 & -0.09 & -0.18 & -0.02\\
  Unigram&-0.10 & -0.06 & -0.07 & -0.08 & 0.14\\
  Dependencies, MaltParser trained by Timbl&-0.19 & -0.18 & -0.18 & -0.19 & -0.10\\
  Dependency, Arc Labels&-0.21 & -0.18 & -0.18 & -0.21 & -0.10\\
  All Features Combined&-0.18 & -0.18 & -0.16 & -0.18 & -0.09\\
\end{tabular}
 \caption{Geographic correlation for complete corpora, 5 normalisations}
 \label{cor-5-full}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&-0.03 & 0.02 & 0.01 & -0.04 & 0.07\\
  Trigram&0.20 & 0.19 & 0.11 & \textit{0.23*} & 0.14\\
  Dependency&-0.07 & 0.01 & -0.01 & -0.08 & 0.05\\
  Phrase-Structure Rules&0.01 & \textit{0.18*} & 0.17 & 0.00 & 0.14\\
  Phrase-Structure with Grandparents&0.03 & \textit{0.26*} & 0.22* & 0.03 & 0.15\\
  Unigram&\textit{0.20*} & \textit{0.19*} & \textit{0.30**} & \textit{0.31**} & \textit{0.21*}\\
  Dependencies, MaltParser trained by Timbl&-0.08 & 0.02 & -0.01 & -0.09 & 0.05\\
  Dependency, Arc Labels&-0.08 & 0.05 & -0.06 & -0.10 & 0.00\\
  All Features Combined&-0.03 & 0.03 & 0.01 & -0.03 & 0.06\\
\end{tabular}
 \caption{Travel correlation for sample size 1000, 1 normalisation iteration}
 \label{travel-cor-1-1000}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&0.02 & 0.08 & 0.11 & 0.00 & 0.08\\
  Trigram&\textit{0.31*} & \textit{0.28*} & \textit{0.32**} & 0.26* & 0.09\\
  Dependency&-0.02 & 0.12 & 0.13 & -0.05 & 0.01\\
  Phrase-Structure Rules&0.15 & \textit{0.37**} & 0.32** & 0.13 & \textit{0.22*}\\
  Phrase-Structure with Grandparents&0.17 & 0.43** & 0.38** & 0.16 & 0.22*\\
  Unigram&\textit{0.22*} & \textit{0.22*} & \textit{0.33**} & \textit{0.34**} & \textit{0.24*}\\
  Dependencies, MaltParser trained by Timbl&-0.01 & 0.14 & 0.17 & -0.04 & 0.02\\
  Dependency, Arc Labels&-0.06 & 0.12 & -0.02 & -0.12 & -0.03\\
  All Features Combined&0.04 & 0.10 & 0.16 & 0.01 & 0.04\\
\end{tabular}
 \caption{Travel correlation for complete corpora, 1 normalisation iteration}
 \label{travel-cor-1-full}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&0.17 & 0.19* & 0.17* & 0.18 & 0.07\\
  Trigram&\textit{0.24*} & \textit{0.20*} & \textit{0.25*} & \textit{0.26*} & 0.16\\
  Dependency&0.14 & 0.16 & 0.17 & 0.15 & 0.10\\
  Phrase-Structure Rules&0.17 & 0.14 & 0.16* & 0.18 & 0.06\\
  Phrase-Structure with Grandparents&0.19 & \textit{0.18*} & 0.17* & 0.19 & 0.06\\
  Unigram&0.15 & 0.13 & \textit{0.17*} & 0.16 & \textit{0.20*}\\
  Dependencies, MaltParser trained by Timbl&0.12 & 0.16 & 0.18 & 0.14 & 0.11\\
  Dependency, Arc Labels&0.09 & 0.13 & 0.14 & 0.11 & 0.08\\
  All Features Combined&\textit{0.23*} & \textit{0.20*} & \textit{0.22*} & \textit{0.24*} & 0.11\\
\end{tabular}
 \caption{Travel correlation for sample size 1000, 5 normalisations}
 \label{travel-cor-5-1000}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&-0.13 & -0.13 & -0.10 & -0.13 & -0.04\\
  Trigram&-0.06 & -0.04 & -0.05 & -0.06 & -0.05\\
  Dependency&-0.20 & -0.17 & -0.13 & -0.19 & -0.06\\
  Phrase-Structure Rules&-0.15 & -0.08 & -0.05 & -0.15 & 0.04\\
  Phrase-Structure with Grandparents&-0.12 & -0.05 & -0.03 & -0.13 & 0.03\\
  Unigram&-0.07 & -0.03 & -0.04 & -0.05 & \textit{0.18*}\\
  Dependencies, MaltParser trained by Timbl&-0.18 & -0.15 & -0.12 & -0.18 & -0.05\\
  Dependency, Arc Labels&-0.20 & -0.17 & -0.14 & -0.19 & -0.06\\
  All Features Combined&-0.16 & -0.14 & -0.11 & -0.15 & -0.05\\
\end{tabular}
 \caption{Travel correlation for complete corpora, 5 normalisations}
 \label{travel-cor-5-full}
\end{table}

From the tables we see that parameter settings that correlate
significantly do so at rates around 0.2 to 0.3, with a high of 0.37
for phrase-structure-rule features measured by $R^2$, 1 normalistion
iteration and comparison of full corpora.  The significant
correlations are mostly concentrated in the trigram, unigram and
combined feature sets.

\subsection{Inter-measure Correlation}

Correlation between measures simply indicates that they are using
similar information from the corpus to do their classification. This
is expected since most measures are quite similar. The one that
differs the most, cosine similarity, also correlates the least with
the others. The average correlation between different measures is
given in table \ref{self-correlation-measures}. The correlations are
averaged over the correlations for all combinations of feature set
with 1000-sentence samples and with non-significant correlations
removed before averaging.

\begin{table}
  \begin{tabular}{r|cccc}
 & $R^2$ & $KL$ & $JS$ & cos \\ \hline
  $R$ & 0.85 & 0.85 & 0.98 & 0.39\\
  $R^2$&& 0.90 & 0.83 & 0.57\\
  $KL$ &&& 0.88 & 0.67\\
  $JS$ &&&& 0.44
\end{tabular}
\caption{Average Inter-measure-correlation of measures}
 \label{self-correlation-measures}
\end{table}

\subsection{Correlation with Corpus Size}

As previously stated, correlation with corpus size is not predicted and is probably
an undesired defect in sampling or normalisation. Correlation with
corpus size is presented in tables \ref{size-cor-1-1000} --
\ref{size-cor-5-full}.

Corpus size between two regions can be measured in two different ways:
either by the sum of the regions' sizes, or by the difference. Here
the sum is used: a larger sum means more tokens. If there is a
correlation with size, it must arise because higher token counts are
not properly normalised. In other words, two large regions will
have more tokens, leading to higher type counts, which directly leads
to higher distances. Smaller regions will lead to lower distances.

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&-0.38 & -0.26 & -0.37 & -0.40 & -0.37\\
  Trigram&0.12 & -0.12 & -0.16 & 0.14 & -0.18\\
  Dependency&-0.39 & -0.26 & -0.35 & -0.43 & -0.39\\
  Phrase-Structure Rules&0.06 & 0.15 & 0.00 & 0.03 & -0.10\\
  Phrase-Structure with Grandparents&0.08 & 0.19 & 0.07 & 0.04 & -0.09\\
  Unigram&-0.08 & -0.14 & -0.09 & -0.09 & -0.10\\
  Dependencies, MaltParser trained by Timbl&-0.35 & -0.23 & -0.28 & -0.37 & -0.37\\
  Dependency, Arc Labels&-0.44 & -0.26 & -0.40 & -0.48 & -0.34\\
  All Features Combined&-0.37 & -0.26 & -0.38 & -0.42 & -0.40\\
\end{tabular}
\caption{Size correlation for sample size 1000, 1 normalisation}
\label{size-cor-1-1000}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&-0.19 & -0.15 & -0.16 & -0.24 & -0.36\\
  Trigram&\textit{0.30*} & 0.08 & 0.19 & 0.08 & -0.39\\
  Dependency&-0.17 & -0.06 & -0.08 & -0.26 & -0.41\\
  Phrase-Structure Rules&0.52** & \textit{0.40**} & 0.30* & 0.47** & -0.21\\
  Phrase-Structure with Grandparents&0.54** & 0.43** & 0.37** & 0.50** & -0.22\\
  Unigram&-0.09 & -0.13 & -0.11 & -0.13 & -0.13\\
  Dependencies, MaltParser trained by Timbl&-0.08 & 0.02 & 0.09 & -0.14 & -0.39\\
  Dependency, Arc Labels&-0.32 & -0.16 & -0.26 & -0.40 & -0.35\\
  All Features Combined&-0.15 & -0.11 & -0.10 & -0.25 & -0.42\\
\end{tabular}
\caption{Size correlation for complete corpora, 1 normalisation}
\label{size-cor-1-full}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&\textit{0.35*} & 0.36** & 0.06 & 0.27 & -0.32\\
  Trigram&\textit{0.75**} & \textit{0.63**} & \textit{0.46**} & \textit{0.68**} & -0.24\\
  Dependency&\textit{0.46**} & \textit{0.44**} & 0.14 & \textit{0.38**} & -0.33\\
  Phrase-Structure Rules&\textit{0.85**} & \textit{0.59**} & 0.36** & \textit{0.85**} & -0.34\\
  Phrase-Structure with Grandparents&\textit{0.88**} & \textit{0.66**} & 0.40** & \textit{0.88**} & -0.36\\
  Unigram&0.38** & 0.35** & 0.14 & 0.19 & -0.04\\
  Dependencies, MaltParser trained by Timbl&\textit{0.44**} & \textit{0.41**} & 0.16 & \textit{0.39*} & -0.30\\
  Dependency, Arc Labels&0.20 & 0.28* & -0.00 & 0.09 & -0.28\\
  All Features Combined&\textit{0.58**} & \textit{0.48**} & 0.21 & \textit{0.47**} & -0.31\\
\end{tabular}
 \caption{Size correlation for sample size 1000, 5 normalisations}
 \label{size-cor-5-1000}
\end{table}

\begin{table}
\begin{tabular}{l|rrrrr}
& $R$ & $R^2$ & KL & JS & cos  \\ \hline
  Leaf-Ancestor&-0.55 & -0.38 & -0.26 & -0.53 & -0.17\\
  Trigram&-0.29 & -0.27 & -0.19 & -0.26 & -0.14\\
  Dependency&-0.61 & -0.43 & -0.27 & -0.58 & -0.18\\
  Phrase-Structure Rules&-0.21 & -0.08 & -0.04 & -0.22 & -0.14\\
  Phrase-Structure with Grandparents&-0.24 & -0.08 & -0.03 & -0.26 & -0.14\\
  Unigram&-0.38 & -0.25 & -0.30 & -0.32 & -0.08\\
  Dependencies, MaltParser trained by Timbl&-0.52 & -0.33 & -0.20 & -0.51 & -0.15\\
  Dependency, Arc Labels&-0.59 & -0.45 & -0.33 & -0.54 & -0.20\\
  All Features Combined&-0.61 & -0.44 & -0.26 & -0.55 & -0.18\\
\end{tabular}
\caption{Size correlation for complete corpora, 5 normalisations}
\label{size-cor-5-full}
\end{table}

A large number of parameter settings that include 5 iterations of
normalisation correlate with corpus size. However, corpus size also
correlates with geographic distance at a rate of 0.31 for $p < 0.01$
and travel distance at a rate of 0.32 for $p < 0.01$. This makes the
conclusion that these distances are invalid difficult to defend. Therefore,
results with 5 normalisations will be presented in the rest of this
chapter.

\section{Clusters}

Cluster dendrograms provide a visualisation of which regions the
distance measures find to be similar.  Clusters answer the question of
whether $R$ is useful for dialectometry more precisely than correlation by
inducing grouping regions. These groups can be compared to
regions proposed by syntactic dialectology.

Within the same settings for sampling and number of normalisation
iterations, the clusters based on sentence-length normalisation alone are fairly
similar, regardless of measure and feature set. Changing the sampling
settings or the number of normalisations substantial reconfiguration.

For example, the clusters produced by $R$ (figure
\ref{cluster-1-r-trigram}) and Jensen-Shannon divergence are fairly
similar (figure \ref{cluster-1-js-trigram}). Both are based on trigram
features with sentence-length normalisation only. Those dendrograms
differ from their 5-normalised equivalents, figures
\ref{cluster-5-r-trigram} and \ref{cluster-5-js-trigram}.

\begin{figure}
  \includegraphics[width=0.9\textwidth]{dist-1-1000-r-trigram-ratio-clusterward}
 \caption{Dendrogram With $R$
    measure and trigram features, 1 normalisation, 1000 samples}
  \label{cluster-1-r-trigram}
\end{figure}

\begin{figure}
  \includegraphics[width=0.9\textwidth]{dist-1-1000-js-trigram-ratio-clusterward}
 \caption{Dendrogram With Jensen-Shannon
    measure and trigram features, 1 normalisation, 1000 samples}
  \label{cluster-1-js-trigram}
\end{figure}

\begin{figure}
  \includegraphics[width=0.9\textwidth]{dist-1-full-r_sq-psg-ratio-clusterward}
 \caption{Dendrogram With $R^2$ measure and phrase-structure-rule features,
 1 normalisation, complete corpora}
  \label{cluster-1-r_sq-psg}
\end{figure}


\begin{figure}
  \includegraphics[width=0.9\textwidth]{dist-5-1000-r-trigram-ratio-clusterward}
 \caption{Dendrogram With $R$ measure and trigram features, 5 normalisations, 1000 samples}
  \label{cluster-5-r-trigram}
\end{figure}

\begin{figure}
  \includegraphics[width=0.9\textwidth]{dist-5-1000-js-trigram-ratio-clusterward}
 \caption{Dendrogram With Jensen-Shannon
    measure and trigram features, 5 normalisations, 1000 samples}
  \label{cluster-5-js-trigram}
\end{figure}

The highest correlation of 1-normalised distances with travel
distance, 0.37, is given by $R^2$ measured over phrase-structure-rule
features, comparing full corpora. Those parameter settings produce the
dendrogram in figure \ref{cluster-1-r_sq-psg}. The highest
correlation of 5-normalised distances with travel distance, 0.26, is
given by the Jensen-Shannon measure and trigram features, comparing
1000-sentence samples of corpora. Its dendrogram is in figure
\ref{cluster-5-js-trigram}.

Unlike the significances, cosine similarity's dendrograms are fairly
similar to those of other features. See for example figure
\ref{cluster-5-cos-trigram}, with cosine, trigram features and
5 iterations of normalisation.

\begin{figure}
 \includegraphics[width=0.9\textwidth]{dist-5-1000-cos-trigram-ratio-clusterward}
 \caption{Dendrogram with cosine measure and trigram features, 5
   normalisations}
  \label{cluster-5-cos-trigram}
\end{figure}


\subsection{Consensus Trees}

Consensus trees combine the results of cluster dendrograms, which
avoids the dendrograms's problem of instability, where small changes
in distances cause large re-arrangements in the tree. Only dendrograms
whose input distances were at least 95\% significant were used. That is, a
measure/feature set combination had to be non-bold in tables
\ref{sig-1-1000} to \ref{sig-5-full} to be included. The consensus
tree for full-corpora comparisons and 5 rounds of normalisation is not
given because there is only one dendrogram that qualifies.

In addition, more dendrograms were used to build
the consensus tree of figure \ref{consensus-5-1000} than were used in
figures \ref{consensus-1-1000} and \ref{consensus-1-full}. Despite this, figure
\ref{consensus-5-1000} retains much more detail, indicating that its
constituent dendrograms, based on 5 rounds of normalisation,
agree more than those with only 1 round of normalisation.

The consensus trees are also grouped into clusters, which are then
mapped in figures \ref{map-consensus-1-1000} --
\ref{map-consensus-5-1000}. The maps of Sweden were provided by Therese
Leinonen and are the same as in her (2010) dissertation.

\begin{figure}
\includegraphics[scale=0.7]{consensus-1-1000}
% \Tree[. {Villberga\\Viby\\Vaxtorp\\Torso\\Tors\.as\\StAnna\\Sproge\\Sorunda\\Skinnskatteberg\\Segerstad\\Ossjo\\Orust\\Norra Rorum\\Loderup\\Leksand\\K\"ola\\Jamshog\\Indal\\Frillesas\\Fole\\Faro\\Bredsatra\\Boda\\Bara\\Asby\\Arsunda\\Anundsjo\\Ankarsrum} [. {Floby\\Bengtsfors}  ] ]
\caption{Consensus Tree for 1000-samples and 1 normalisation}
\label{consensus-1-1000}
\end{figure}

\begin{figure}
\includegraphics[scale=0.7]{consensus-1-full}
% \Tree[. {Villberga\\Viby\\Torso\\Tors\.as\\Sorunda\\Segerstad\\Ossjo\\Orust\\Norra Rorum\\Loderup\\Leksand\\K\"ola\\Indal\\Fole\\Boda\\Bara\\Asby\\Arsunda\\Anundsjo\\Ankarsrum}
%   [. {Vaxtorp\\Skinnskatteberg}  ]
%   [. {StAnna\\Frillesas}  ]
%   [. {Sproge\\Faro}  ]
%   [. {Jamshog\\Bredsatra}  ]
%   [. {Floby\\Bengtsfors}  ] ]
\caption{Consensus Tree for full corpus comparison and 1 normalisation}
\label{consensus-1-full}
\end{figure}

\begin{figure}
\includegraphics[scale=0.7]{consensus-5-1000}
% \Tree[. {Villberga\\Viby\\Vaxtorp\\Torso\\StAnna\\Sproge\\Sorunda\\Skinnskatteberg\\Segerstad\\Orust\\Norra Rorum\\Leksand\\K\"ola\\Indal\\Frillesas\\Fole\\Floby\\Faro\\Boda\\Bengtsfors\\Bara\\Asby\\Arsunda\\Anundsjo\\Ankarsrum} [. {Loderup\\Bredsatra}  ] [. {Tors\.as\\Ossjo\\Jamshog}  ] ]
\caption{Consensus Tree for 1000-samples and 5 normalisations}
\label{consensus-5-1000}
\end{figure}

\begin{figure}
\includegraphics[scale=0.85]{Sverigekarta-Landskap-consensus-1-1000}
\caption{Consensus Tree for 1000-samples and 1 normalisation, Mapped}
\label{map-consensus-1-1000}
\end{figure}

\begin{figure}
\includegraphics[scale=0.85]{Sverigekarta-Landskap-consensus-1-full}
\caption{Consensus Tree for full corpus comparison and 1 normalisation, Mapped}
\label{map-consensus-1-full}
\end{figure}

\begin{figure}
\includegraphics[scale=0.85]{Sverigekarta-Landskap-consensus-5-1000}
\caption{Consensus Tree for 1000-samples and 5 normalisations, Mapped}
\label{map-consensus-5-1000}
\end{figure}

% It would still be cool to eliminate only the non-significant distances
% and re-run the clusters. (I can't remember if that's easily possible
% with R though, it may only be a feature of MDS.)

% TODO: Try these two again, excluding cosine. Because I believe cosine sucks
% or at least is a Rogue Element.
% Later: Probably not worth it.

% Just the ratio ones that are significantly correlated with travel
% distance.
% However: This is even more of a mess than the freq results.
% [. {s0} [. {}
%     [. {} [. {Köla} [. {Ossjo} [. {Torsås\\Jamshog}  ] ] ]
%           [. {Villberga\\Viby\\Torso\\StAnna\\Sorunda\\Norra Rorum\\Frillesas\\Boda\\Bara}
%              [. {Loderup\\Bredsatra}  ] ] ]
%     [. {Orust\\Leksand\\Indal\\Fole\\Faro\\Asby\\Arsunda\\Anundsjo}
%        [. {Vaxtorp\\Skinnskatteberg}  ]
%        [. {Ankarsrum}
%           [. {Segerstad} 
%              [. {Bengtsfors}
%                 [. {Sproge\\Floby}  ] ] ] ] ] ] ]


\subsection{Composite Cluster Maps}

Composite cluster maps start with the same data as consensus trees:
multiple cluster trees. They, too, provide a stabler view of the
groups that regions form when clustered. However, they provide a better way to
visualise the boundaries between regions.

The two composite cluster maps in figures \ref{map-composite-1-1000}
-- \ref{map-composite-5-1000} are the composite of the same
dendrograms used as input for the consensus trees: all-significant
parameter settings, divided by type of normalisation (sentence-length
only or ratio added as well).

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-cluster-1-1000}
\caption{Composite Cluster Map for 1000-sample, 1 normalisation}
\label{map-composite-1-1000}
\end{figure}

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-cluster-1-full}
\caption{Composite Cluster Map for complete corpora, 1 normalisation}
\label{map-composite-1-full}
\end{figure}

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-cluster-5-1000}
\caption{Composite Cluster Map for complete corpora, 5 normalisations}
\label{map-composite-5-1000}
\end{figure}

\section{Multi-Dimensional Scaling}

\begin{sloppypar}
Multi-dimensional scaling (MDS) plays a similar role to clusters,
condensing the high-dimensional information into an
easier-to-understand form. The difference is that
the grouping here is ``soft'' rather than ``hard'': only enough
scaling is done to produce the desired number of dimensions, and there
is no exclusive membership of a groups. This also means that
MDS maps are more stable than dendrograms.
\end{sloppypar}

The maps shown here in figures \ref{mds-1-1000-js-trigram} --
\ref{mds-5-1000-js-trigram} are from the same parameter settings as the
dendrograms, with the addition of the all-combined feature set. This
provides a combined MDS view somewhat analoguous to the consensus trees or
composite clusters for the cluster dendrograms.

The outline maps are used by permission of Therese Leinonen (CITE her
dissertation after I get a copy to read). The MDS and composite
clusters were both generated by the L04 package from the University of
Groningen.

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-mds-1-1000-r-trigram-ratio}
\caption{$R$ measure with trigram features, 1000-sentence sampling and
  1 round of normalisation}
\label{mds-1-1000-r-trigram}
\end{figure}

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-mds-1-1000-js-trigram-ratio}
\caption{Jensen-Shannon measure with trigram features, 1000-sentence sampling and
  1 round of normalisation}
\label{mds-1-1000-js-trigram}
\end{figure}

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-mds-1-full-r-trigram-ratio}
\caption{$R$ measure with trigram features, full-corpus comparison and
  1 round of normalisation}
\label{mds-1-full-r-trigram}
\end{figure}

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-mds-1-full-r_sq-psg-ratio}
\caption{$R^2$ measure with phrase-structure-rule features, full-corpus comparison and
  1 round of normalisation}
\label{mds-1-full-r_sq-psg}
\end{figure}

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-mds-5-1000-r-trigram-ratio}
\caption{$R$ measure with trigram features, 1000-sentence sampling and
  5 rounds of normalisation}
\label{mds-5-1000-r-trigram}
\end{figure}

\begin{figure}
\includegraphics[scale=0.82]{Sverigekarta-mds-5-1000-js-trigram-ratio}
\caption{Jensen-Shannon measure with trigram features, 1000-sentence sampling and
  5 rounds of normalisation}
\label{mds-5-1000-js-trigram}
\end{figure}

% should probably still put cos-trigram and maybe r-all or something

\section{Features}
\label{feature-ranking}

Ranked features answer the question of agreement with dialectology
more precisely than the previous two sections. Feature ranking has two
advantages in precision: first, it can reveal aggregate differences that may
not be noticeable without counting a large corpus; second, it can
point out rare features that only occur in one kind of corpus. The
first kind of features are unlikely to be noticed by linguists without
the aid of computers, whereas the second kind are the rare features
that are easy for linguists to notice.

There are two sets of rankings on display here; the first set is the
previous normalisation for sentence size, whereas the second is normalised for
relative overuse, based on \quotecite{wiersma09} normalisation.

These results compare clusters from the consensus trees
based on 1 round of normalisation (figures \ref{consensus-1-1000} and
\ref{consensus-1-full}) as well as the consensus tree based on 5
rounds of normalisation and a 1000-sentence sample (figure
\ref{consensus-5-1000}. The consensus tree for 5 rounds or
normalisation and full-corpus comparisons only had one tree for input
and was not usable. Given these three consensus trees, the groups in
table \ref{feature-ranking-clusters} are the relevant ones for analysis.

There are four groups, three small and one containing the remainder of
the sites. Cluster A, containing Floby and Bengtsfors, appears in all
three consensus trees. Its features are coloured blue in the following
figures. Cluster B, containing Jamshog, Torsas and Ossjo, appears in the
second two trees. It features are coloured red. Cluster C, containing
Loderup and Bredsatra, appears only in the third tree. Its features
are coloured yellow. The remainder of the sites are in Cluster D;
the third consensus tree differs from the first two in splitting the
remainder into two groups, but this division is ignored here to reduce
the number of comparisons. Between large groups of sites, such
comparisons are unlikely to be informative anyway.

\begin{table}
  \begin{enumerate}
   \item[A] Floby, Bengtsfors
    \item[B] J\"amshog, \"Ossj\"o, Tors\.as
    \item[C] L\"oderup, Breds\"atra
    \item[D] Segerstad, K\"ola, S:t Anna, Sorunda, Norra Rorum,
      Villberga, Torso, Boda, Frilles\.as, Indal, Leksand, Anundsj\"o,
      \.Arsunda, Asby, Orust, V\.axtorp, Fola, Sproge, F\.ar\"o,
      Ankarsrum, Skinnskatteberg
  \end{enumerate}
  \caption{Clusters discussed}
  \label{feature-ranking-clusters}
\end{table}

The first subsection, \ref{feature-ranking-complete}, shows all
comparisons between regions for a single parameter setting: trigram
features, 1000-sentence sampling and sentence-size normalisation
only. Besides unigrams, these are the parameters that give the highest
correlation with travel distance for 1000-sentence sampling.

In the next subsection, \ref{feature-ranking-overuse}, the overuse
normalisation is added, keeping other parameter settings the same.

The third subsection, \ref{feature-ranking-feature-sets},
a single comparison between cluster A and cluster B is given for
all feature sets.

In the final subsection, \ref{feature-ranking-psg}, the high-ranked
phrase-structure rules are given.

In addition, only features that appear in both groups were ranked;
although features that only appear in one or the other can be
interesting, they tend to be noisy in features extracted from
automatically annotated corpora. It is not possible to tell which
unique features are interesting and which are noise, especially when
using the overuse normalisation, which makes rarely occurring features
equal to common ones.

The most common parts of speech are given below. The complete list is
given in Appendix X. (TODO: Move most of this list to an
appendix).

\begin{itemize}
\item $++$ = coordinating conjunction
\item AB = adverb
\item AJ = adjective
\item AN = adjectival noun
\item AV = verb ``vara'' (be)
\item BV = verb ``bli(va)'' (become) %
% \item EH = hesitation
\item EN = indefinite article
\item FV = verb ``f\.a'' (get) %
\item GV = verb ``g\"ora'' (do) %
\item HV = verb ``hava'' (have) %
\item I? = question mark %
\item IC = quotation mark %
\item ID = idiom
\item IG = other punctuation mark %
\item IK = comma, correction
\item IM = infinitive marker
\item IP = period
% \item IQ = colon
% \item IR = parenthesis
% \item IS = semicolon
% \item IT = dash
\item IU = exclamation mark
\item KV = ``komma at'' (periphrastic future)
\item MN = meta-noun
\item MV = verb ``m\.aste'' (must)
\item NJ = falling juncture
\item NN = noun
\item PN = proper name
\item PO = pronoun
\item PR = preposition
\item PU = list item
% \item QQ = ?
\item QV = verb ``kunna'' (can)
% \item RJ = level juncture
\item RO = numeral
\item SP = present participle
\item SV = verb ``skola'' (shall)
\item TP = perfect participle
% \item UJ = rising juncture
\item UK = subordinating conjunction
% \item UU = exclamation
\item VN = verbal noun
\item VV = other verb
\item WV = verb ``vilja'' (want)
\item YY = Interjection
\item XX = Unclassifiable
\end{itemize}

\subsection{Trigram Features}
\label{feature-ranking-complete}
The figures in this section show the highest ranked features for
trigram features.

\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-trigram-ratio}
  \caption{cluster A $\Leftrightarrow$ cluster B, trigram features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterC-feat-5-1000-trigram-ratio}
  \caption{cluster A $\Leftrightarrow$ cluster C, trigram features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterD-feat-5-1000-trigram-ratio}
  \caption{cluster A $\Leftrightarrow$ cluster D, trigram features}
\end{figure}


\begin{figure}
  \includegraphics[scale=1.2]{clusterB-clusterC-feat-5-1000-trigram-ratio}
  \caption{cluster B $\Leftrightarrow$ cluster C, trigram features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterB-clusterD-feat-5-1000-trigram-ratio}
  \caption{cluster B $\Leftrightarrow$ cluster D, trigram features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterC-clusterD-feat-5-1000-trigram-ratio}
  \caption{cluster C $\Leftrightarrow$ cluster D, trigram features}
\end{figure}

\subsection{Trigrams with Overuse Normalisation}
\label{feature-ranking-overuse}
The figures in this section show the highest ranked features for
trigram features with 1000-sentence sampling. Overuse normalisation is
added.  The results are somewhat different from the previous section
without the overuse normalisation.

\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-trigram-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, trigram features
    with overuse normalisation}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterC-feat-5-1000-trigram-over}
  \caption{cluster A $\Leftrightarrow$ cluster C, trigram features
    with overuse normalisation}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterD-feat-5-1000-trigram-over}
  \caption{cluster A $\Leftrightarrow$ cluster D, trigram features
    with overuse normalisation}
\end{figure}


\begin{figure}
  \includegraphics[scale=1.2]{clusterB-clusterC-feat-5-1000-trigram-over}
  \caption{cluster B $\Leftrightarrow$ cluster C, trigram features
    with overuse normalisation}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterB-clusterD-feat-5-1000-trigram-over}
  \caption{cluster B $\Leftrightarrow$ cluster D, trigram features
    with overuse normalisation}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterC-clusterD-feat-5-1000-trigram-over}
  \caption{cluster C $\Leftrightarrow$ cluster D, trigram features
    with overuse normalisation}
\end{figure}


\subsection{Variation Across Feature Sets}
\label{feature-ranking-feature-sets}

Here, I show only comparisons between clusters A and B across feature
sets. Overuse normalisation is used.

\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-path-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, leaf-ancestor path features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-trigram-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, trigram features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-dep-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, dependency features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-psg-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, phrase-structure
    rule features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-grand-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, phrase-structure
    rules features, with grandparent}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-unigram-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, unigram features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-redep-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, dependency features,
MaltParser trained by Timbl}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-deparc-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, dependency features
    with arc labels}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-all-over}
  \caption{cluster A $\Leftrightarrow$ cluster B, all combined features}
\end{figure}

\clearpage

\subsection{Phrase-structure rule features}
\label{feature-ranking-psg}

\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterB-feat-5-1000-psg-ratio}
  \caption{cluster A $\Leftrightarrow$ cluster B, phrase-structure
    rule features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterC-feat-5-1000-psg-ratio}
  \caption{cluster A $\Leftrightarrow$ cluster C, phrase-structure rule features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterA-clusterD-feat-5-1000-psg-ratio}
  \caption{cluster A $\Leftrightarrow$ cluster D, phrase-structure rule features}
\end{figure}


\begin{figure}
  \includegraphics[scale=1.2]{clusterB-clusterC-feat-5-1000-psg-ratio}
  \caption{cluster B $\Leftrightarrow$ cluster C, phrase-structure rule features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterB-clusterD-feat-5-1000-psg-ratio}
  \caption{cluster B $\Leftrightarrow$ cluster D, phrase-structure rule features}
\end{figure}
\begin{figure}
  \includegraphics[scale=1.2]{clusterC-clusterD-feat-5-1000-psg-ratio}
  \caption{cluster C $\Leftrightarrow$ cluster D, phrase-structure rule features}
\end{figure}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
