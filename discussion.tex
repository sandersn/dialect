\chapter{Discussion}

This chapter discusses three topics:

\begin{enumerate}
\item Analysis of results: dissertation work on its own.
\item Comparison to Swedish syntactic dialectology.
\item Comparison to Swedish phonological dialectometry. (If Therese
  sends it to me in time.)
\item Comparison to syntactic dialectometry.
  That is, previous work. That is, MY previous work, plus
  Wybo's. That's pretty much it you know.
\item Conclusions: summary of discussion
\item and then summary of dissertation: contribution to dialectometry at large.
\end{enumerate}

A big question is why trigrams are so good. All of the fancier feature
sets do worse than trigrams. I should address this in the summary for
sure.

\section{Analysis of Results}

% TODO: Need to add references to results chapter and also any
% citations

\subsection{Significance of Dialect Distance}

Analysis of the significance of dialect distance provides a measure of
how reliable the distances to be analysed later in this chapter are. A
distance that does not find significant distances between of 30
regions is not suitable for precise inspection, although small numbers
of non-significant distances will still allow less precise methods to
return interpretable results.

The highest number of significant distances are found in the first
case (figure \ref{sig-1-1000}): 1 round of normalisation with a
fixed-size sample of 1000 sentences. From there, both full-corpus
comparisons (figure \ref{sig-1-full}) and 5 rounds of normalisation
(figure \ref{sig-5-1000}) have fewer significant distances, although
the number is still usable. However, the combination of the two, with
5 rounds of normalisation over full-corpus comparisons, has only one
combination with fewer than 5\% of distances that are {\it not}
significant. Although both full-corpus comparisons and multiple rounds
of normalisation may increase the precision of the results, their
combined effect on significance is so detrimental that its results are
useless. For the rest of the analysis, the combination of full-corpus
comparison and 5 rounds of normalisation will be skipped.

\subsubsection{Significance by Measure}

The distance measures most likely to find significance are, in order,
cosine dissimilarity, Jensen-Shannon divergence and $R$. Each method
had different parameter settings for which it was stronger. For
1000-sentence sampling, cosine similarity resulted in all significant
distances, even for part-of-speech unigrams, which are intended as the
baseline feature set. Excluding unigrams, Jensen-Shannon divergence
has similar performance. For full-corpus comparisons, both perform
considerably worse; surprisingly, both perform better on unigram
features, Jensen-Shannon so much so that it's the only feature set for
which it finds all significant distances. $R$, on the other hand,
performs decently on all combinations of parameter settings; its low
significance for phrase structure rules is shared by Kullback-Leibler
and Jensen-Shannon divergences.
% TODO : Maybe more on cosine later. Maybe not.

When comparing the performance of Kullback-Leibler and Jensen-Shannon
divergence it is not surprising that Jensen-Shannon outperforms
Kullback-Leibler on fixed-size sampling. Although both are called
``divergence'', Jensen-Shannon divergence is actually a
dissimilarity. Recall that the divergence from point A to B may differ
from the divergence from point B to A. A divergence like
Kullback-Leibler can be converted to a dissimilarity by measuring
$KL(A,B) + KL(B,A)$. However, this dissimilarity must skip features
unique to a single corpus in order to avoid division by zero. This
means that for smaller corpora Kullback-Leibler loses information that
Jensen-Shannon is able to use.  On the other hand, while this may
explain Kullback-Leibler's improved performance for full-corpus
comparisons, it doesn't explain Jensen-Shannon's much worse
performance.

\subsubsection{Significance by Feature Set}

% \item Unigrams do form an adequate baseline; they are bad but not too
%   bad.

% The feature sets most likely to find significance are the combined
% features and unigrams., in order,
% trigrams, all combined features and leaf-head paths (both with
% support-vector-machine training and with Timbl's instance-based
% training). Without ratio normalisation, the other feature sets are not
% much worse, but with it included, these three are the best by some
% distance.

For 1 round of normalisation, the best feature sets are the simple
ones: trigrams and unigrams, as well all combined features. On the
other hand, trigrams and leaf-head paths (with its variations) are the
best feature sets with 5 rounds of normalisation. However, the
variation isn't strong; any feature set can give good results with the
right distance measure. The problem is that no clear patterns emerge.

The relatively high quality of trigrams and unigrams does not make
sense given only the linguistic facts; however, it is likely that the
entirely automatic annotation used here introduces more and more
errors as more annotators run, operating on previous automatic
annotations. Trigrams are the result of only one automatic annotation,
and one for which the state of the art is near human performance. So
the fact that these particular parts of speech are of higher quality
than the corresponding dependencies or constituencies is probably the
deciding factor in their higher number of significant
distances.

% Although it is impossible to tell from my results, I
% predict that a manually annotated dialect corpus would show that
% non-flat syntactic structure is helpful in producing significant
% distances.

Given the above facts, the question should rather be: why do leaf-head
paths perform as well as they do? Better, for example, than the
leaf-ancestor paths on which they're modelled: why does more
normalisation hurt leaf-ancestor paths but not leaf-head paths?  It
could be that there is less room for error; many of the common
leaf-head paths are short: short interview sentences with simple
structure make for shorter leaf-head paths than leaf-ancestor
paths. As a result, the important leaf-head paths consist mainly of a
couple of parts-of-speech.

Another reason could be a difference in parsers: MaltParser has been
tested before with Swedish (CITE). Besides English, the Berkeley
parser has been tested prominently on German and Chinese. Therefore,
the difference would better be explained by appealing to the
difference in parsers rather than an unsuitability of Swedish for
constituent analysis.

It is disappointing linguistically that trigrams provide the most
reliable results so far; a linguist would expect that including
syntactic information would make it easier to measure the differences
between regions. If it is, as hypothesised here, an effect of chaining
machine annotators, a study using manually annotated corpora could
detect this. However, it still means that trigrams are the most useful
feature set from a practical view, because automatic trigram tagging
is very close to human performance with little training. That means
the only required human work is the transcription of interviews in
most cases.

On the other hand, if additional features sets are to be developed for
a corpus, then combining all available features seems to be a
successful strategy. The distance measures seem to be able to use all
available information for finding significant distances.

\subsection{Correlation}
% TODO: ALSO I should point out that geographic correlation IS the
% default expected by normal old boring linguists. The Nordic Languages
% is pretty clear on this plus I have some references from
% dialectology papers.

In dialectology, the default expectation for dialect distance is that
it correlates with geographic distance \cite{chambers98}. A lack of
correlation does not necessarily mean that a measure is useless, but
presence of correlation means that the distance measure substantiates the
well-known tendency of dialect distributions to be more or less
smoothly gradient over physical space.

In addition, the distance measures are more likely to correlate
significantly with travel distance than with straight-line geographic
distance. This makes sense since the difficulty of moving from place
to place is what influences dialect formation, and taking roads into
account is an improved estimate over straight-line distance.

As with the number of significant distances, trigrams and unigrams are
the most likely to to correlate with geographic and travel distance,
as well as the combined feature set for the 5-normalisation parameter
setting.
% As before, a possible explanation is that unigrams are
% simpler, so the type count is a higher than for other measures. With
% more rounds of normalisation, more correlations shift over to
% trigrams.
Note that in figures \ref{cor-1-1000} --
\ref{travel-cor-5-full}, the significant correlations are marked with
an asterisk, but only the italicised correlations are based on at
least 95\% significant distances. For example, this means that most of
the significant correlations based on phrase-structure rules are not valid.

It is worthwile noting, however, that the valid and significant
correlations based on phrase-structure grammars give the highest
correlations: 0.37 for $R^2$ with full-corpus comparisons and 1 round
of normalisation.
The addition of more data and more normalisation is interesting in
expanding the correlating parameter settings beyond those that include
unigram features. It may be that this is an instance of the noise/quality tradeoff.
These additions appear to extract more detail from
the data, at the cost of additional interference from noisy data.

% Goes here: Fevered speculation about why travel correlation is *better* with
% the methods that correlate *less*, for 1-full at least.
% OK never mind this isn't true.

\subsubsection{Inter-measure correlation}

In table \ref{self-correlation-measures}, $R$ and Jensen-Shannon
produce nearly identical results. Also, cosine similarity arrives
at different results than the other measures, though the correlation
is still higher than with travel distance.

\subsubsection{Correlation with Corpus Size}

The correlation of corpus size and dialect distance is a problem. It
is not a predicted as a side effect of the way dialect distance is
measured. The fact that travel distance also correlates with corpus
size at a rate of 0.32 confuses the issue further. Is corpus size the
determining variable? Or is there an unknown variable influencing all
three? Some possibilities are ``interviewer boundaries'', common in
corpora collected by multiple people \cite{chambers98}, or perhaps the
interviewers got better over time and collected longer interviews as
they moved throughout the country, or perhaps cultural differences
between the interviewer and interviewees caused some participants in
one area to talk more than in another area.

Definitely the high correlations between corpus size and the
5-normalisation distances are worrying. They are so much higher than
the correlation of corpus size and travel distance that 5-normalised
distances might not be reliable.
% It appears that multiple rounds of
% normalisation inadvertently re-introduce a dependency on size.
% TODO: This probably IS a bug in that only freq norm can be
% iterated. Ratio norm should probably be in a separate loop like so:
% #ifdef RATIO_NORM
%   for(sample::iterator i = ab.begin(); i!=ab.end(); i++) {
%     i->second.first *= 2 * types / tokens;
%     i->second.second *= 2 * types / tokens;
%   }
% #endif
However, if 5-normalisation introduces a dependency on corpus size,
then the distances from full-corpus comparisons should correlate even
more highly. This is not the case.

% TODO: I also should write this up when I have time
Alternatively, it is possible that the fixed-size sampling method is not doing its
job in eliminating size differences between corpora. Future work
should develop a method for normalising a comparison between two full
corpora. It should avoid sampling, but also take the relative number
of sentences into account. It is not difficult to come up
with a simple method to do so, but it needs some checking to make sure
that the method is valid.

\subsection{Cluster Dendrograms}

The cluster dendrograms in chapter \ref{results-chapter} are dangerous
to interpret too closely on their own; the instability of a single
dendrogram means that small clusters cannot be analysed reliably. For
example, in figure \ref{cluster-5-r-trigram}, a two-way split
between the regions on the top and bottom of the page is
obvious, and a three-way split is easy to argue for, but outliers like
Anundsj\"o and \.Arsunda are likely to shift from group to group in
other dendrograms.

It is safer to analyse the consensus trees; the smoothing effect of
taking the majority rule of each cluster will show where the optimal
cutoff for splitting clusters is. The three consensus trees in figures
\ref{consensus-1-1000} -- \ref{consensus-5-1000} vary in amount of
detail but share most details.

For 1000-sentence samples and 1 round of normalisation, there is one
cluster: Floby and Bengtsfors. Full-corpus comparison finds
another cluster: J\"amshog, \"Ossj\"o and Tors\.as. Finally,
1000-sentence samples and 5 rounds of normalisation finds another
cluster consisting of L\"oderup and Breds\"atra. It also finds
a large two-way split between the regions and adds Sproge to the first
cluster with Floby and Bengtsfors. To aid further analysis, the
clusters are assigned colours, which are detailed in figures
\ref{blue-cluster} -- \ref{orange-cluster}. 

\begin{figure}
\begin{itemize}
\item Floby
\item Bengtsfors
\item Sproge (for 1000-sample, 5-normalisation)
\end{itemize}
\caption{Blue Cluster}
\label{blue-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item J\"amsh\"og
\item Tors\.as
\item \"Ossj\"o
\end{itemize}
\caption{Red Cluster}
\label{red-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Breds\"atra
\item L\"oderup
\end{itemize}
\caption{Yellow Cluster}
\label{yellow-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Leksand
\item Indal
\item Segerstad
\item Floby
\item Bengtsfors
\item Sproge
\item Skinnskatteberg
\item Orust
\item V\.axtorp
\item F\.ar\"o
\item Asby
\item \.Arsunda
\item Anundsj\"o
\item Ankarsrum
\item Fole
\end{itemize}
\caption{Cyan Cluster}
\label{cyan-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Viby
\item Bara
\item S:t Anna
\item Frilles\.as
\item J\"amshog
\item Tors\.as
\item \"Ossj\"o
\item K\"ola
\item L\"oderup
\item Breds\"atra
\item Villberga
\item Tors\"o
\item Norra R\"orum
\item Sorunda
\item B\"oda
\end{itemize}
\caption{Orange Cluster}
\label{orange-cluster}
\end{figure}

When these clusters are mapped onto the geography of Sweden, some
patterns are visible. Since figure \ref{consensus-5-1000} is strictly
more complex than the preceding two, it is used as the basis for this
analysis--see map \ref{map-consensus-5-1000}. The large two-way split
is between the orange and cyan clusters. The orange cluster, which
includes red and yellow clusters, forms two horizontal bands across
Sweden. The centres of the orange cluster appear to be Stockholm and
Malm\"o. Meanwhile, the red and yellow clusters form a boundary along
the northern border of Sk\.ane and Blekinge counties.

Meanwhile, the cyan cluster, which includes the blue cluster, seems to
represent the countryside of Sweden. On the other hand, because the
blue cluster is near G\"oteborg, it might be better characterised
simply as ``non-Stockholm''.

Most of this analysis agrees with the existing dialectology
literature; the north-to-south gradient is well-attested in (CITE: The
Nordic Languages, plus I'm pretty sure I read this somewhere
else). In the south, it is also well-known that the boundary with
Sk\.ane is stronger; this boundary extends along Blekinge as well, it
seems. However, the difference between city and countryside is not
well attested in the literature, nor is the possible difference
between Stockholm/Malm\"o and G\"oteborg.

\subsection{Composite Cluster Maps}

Composite cluster maps use an underlying technique similar to consensus
trees--cluster dendrograms, but they combine and present the information in
a very different way. The result looks much more like the traditional
isogloss boundaries of dialectology. The composite cluster maps are in
maps\ref{map-composite-1-1000} -- \ref{map-composite-5-1000}.

All three composite clusters maps provide a picture similar to the
consensus tree map \ref{map-consensus-5-1000} of the previous
section. The north-to-south gradient is supported by the
weak horizontal boundaries present up and down Sweden.

Of these boundaries, the one between Sk\.ane and the rest of Sweden is
the strongest. Due to the lack of interview sites in the middle of
south Sweden, the boundary is drawn further north than it
traditionally appears, but this is an effect of the software the
produced the figure. Notice that there is also a boundary between
J\"amshog, Tors\.as, and \"Ossjo\" and the other sites, especially
visible in maps\ref{map-composite-1-1000} and
\ref{map-composite-5-1000}. Their presence along the northern border
of Sk\.ane is one reason why its boundary with the rest of Sweden is
so strong.

Compared to the consensus tree maps, the composite cluster maps cannot
support the city/country distinction because there is no way to
identify distant areas by their colour. On the other hand, it is
possible to detect the relative strength of a boundary. To combine
these two features, multi-dimensional scaling is needed.
% But of course MDS maps can't be combined into a consensus...

% However, K\"ola and Frilles\.as still separate fairly well from their
% neighbours. These sites are on the edges of the country and have strong borders
% with surrounding, Like the cluster J\"amshog, Tors\.as and \"Ossj\"o,
% these sites are different from the others. However, they don't have
% any geographic coherence, so it is more likely these are remnants of a
% dialect that was historically wider spread and has since receded.

\subsection{Multi-Dimensional Scaling}

Multi-dimensional scaling (MDS) operates similarly to cluster
dendrograms in that it reduces the high-dimensional distances to a
lower-dimensionality representation. It differs, however, in producing
gradient numbers, not binary trees. This means that the MDS maps
naturally have gradient borders. Also, because of the way that the
3-dimensional points map to colours, the maps vary. However, they are
still comparable: if two regions are blue in one map and both are
orange in another, then they have the same relation to each other.

Despite the differences between MDS and the preceding methods, the
similar results are evident; the maps (figures
\ref{mds-1-1000-r-trigram} -- \ref{mds-5-1000-js-trigram}) all show
the same patterns as the other methods. That is, there is a general
north-to-south gradience, especially easy to see in map
\ref{mds-1-1000-js-trigram}. There is a strong southern cluster,
visible in all of the diagrams. And there is a general two-way
distinction between city and country.

The main contribution that the MDS maps make is that the
north-to-south gradient is more obviously gradient. In other words, it
is easier to see the gradation from north to south. For example, in
figure \ref{mds-1-full-r_sq-psg}, looking from the north to south, the
colours change quickly close to Stockholm, then fade to green further
south, then transition back to blues and purples further south, in
Sk\.ane.

The Stockholm and Malm\"o areas, which are in the same cluster in the
consensus tree maps, are here seen to be similar without being
identical. For example, in figure \ref{mds-5-1000-js-trigram}, the
Stockholm area is a shade of blue-green while the Malm\"o area is a
shade of blue-grey. Also in figure \ref{mds-5-1000-js-trigram},
Sk\.ane and Blekinge are grey: clearly similar but not identical to
Malm\"o.

\subsection{Features}

%TODO: Put back in the freq/ratio features for $R$ instead of the
%overuse-ranked features. I guess overuse isn't that useful after all.

Next I rank and analyse the input features by comparing their
differences. The features analysed here are the ten highest ranked
features for a particular comparison, based on the consensus tree
clusters of figures \ref{consensus-1-1000} --
\ref{consensus-5-1000}. The clusters are labelled A to D from top to
bottom. The clusters are reproduced in table
\ref{feature-ranking-clusters-again}. Although each feature set has
ten features ranked here, it's actually two sets of five features,
because these are differences. The top five positive features are
shown as are the top five negative features.

\begin{table}
  \begin{enumerate}
   \item[A] (Blue) Floby, Bengtsfors
    \item[B] (Red) J\"amshog, \"Ossj\"o, Tors\.as
    \item[C] (Yellow) L\"oderup, Breds\"atra
    \item[D] (Cyan) Segerstad, K\"ola, S:t Anna, Sorunda, Norra Rorum,
      Villberga, Torso, Boda, Frilles\.as, Indal, Leksand, Anundsj\"o,
      \.Arsunda, Asby, Orust, V\.axtorp, Fola, Sproge, F\.ar\"o,
      Ankarsrum, Skinnskatteberg
  \end{enumerate}
  \caption{Clusters discussed}
  \label{feature-ranking-clusters-again}
\end{table}

This has two advantages. It splits the features so that both the
positive and negative evidence are always visible; otherwise, in some
cases, if one side is strong enough, the other would be pushed out of
the top ten. However, it still allows the relative weight of evidence
to be estimated. For example, if some cluster has some idiosyncratic
features, most of the features will be positive, meaning that most of
the distance comes from features typical of this cluster. The two-part
feature will show this: the five positive features will have much
higher values than the five negative features.

For feature ranking, there is an additonal normalisation used
called overuse normalisation, which ranks rarely used features more
highly.  This can be useful; without it, the top-ranked features will
tend to be the most common ones, those found in almost every sentence
in the interview. These common features tend to highlight
gradient differences: differences in quantity but not in quality. In contrast,
the overuse normalisation allows us to see which features happen only
a few times in one side of the comparison and not at all in the
other. This is closer to a traditional linguistic analysis.

\subsubsection{Analysis}

The analysis will start with trigram features without the overuse
normalisation, since trigrams have the highest rate of significance of
the non-combined feature sets. (The combined feature set is difficult
to read because of the mixing of feature types.)

As mentioned above, the top-ranked trigrams are common, typical
of the core of the sentence. Cluster A's typical trigrams, for
example, typically involve a pronoun or a verb or both: PO-AV-AB
(pronoun-copula-adverb), $++$-PO-AV (conjunction-pronoun-copula) and
PO-VV-AB (pronoun-verb-adverb). The same is of the
other clusters for the most part. Unfortunately, this makes it say
interesting things about the difference in feature distribution. It
does appear that clusters B and C have heavier use of adverbs and of
conjunctions. The comparison between cluster A and cluster B even
highlights the trigram AB-AB-AB as important.

Given this lack of information, there are two dimensions along which
the comparisons can be altered: normalisation and feature
set. Starting with normalisation, let us add the overuse normalisation
technique. Differences appear immediately. First, the balance of
feature weight obviously differs here. For example, in the comparison
between cluster A and cluster B, the features of cluster A are more
important in distinguishing the two than the features of cluster
B. The comparison between cluster A and cluster D is so lop-sided that
cluster D contributes no features at all.

With the overuse normalisation, cluster A has two interesting
patterns. First, the trigrams it overuses are filled with indefinite
articles (EN) and prepositions (PR). Examples include VV-EN-AB
(verb-indefinite-adverb), PR-EN-AB (preposition-indefinite-adverb) and
PR-EN-VN (preposition-indefinite-verbal noun), as well as IM-PR-NN
(infinitive marker-preposition-noun) and PR-ID-PR
(preposition-idiom-preposition). Second, the trigrams it underuses
mostly end with pronouns: 4 of 5 trigrams in the comparison with
cluster B and 4 or 5 in the comparison with cluster C. Even in the
comparison with cluster D, 4 of 5 of the ``least overused'' trigrams
end with pronouns. (The low values in the bottom half of the
comparison with cluster D are not underused by cluster A, because
cluster D has no unique features here. Instead they are the ``least
overused'' by cluster A.)

Cluster B shows one interesting pattern: overuse of sk\"ola (shall),
including an interesting trigram SV-QV-AB (shall verb-can
verb-adverb). Although this could be a mistake on the part of the
tagger, the different forms of this verb are limited, so this is
unlikely: identifying them is not hard. Instead it points to the
possibility of double modals.
%% a quick search suggests that Fennell and Butters (1996) finds
%% evidence in German and Scandinavian languages...but it's a book ro
%% something. Google Scholar has no link, just a wimpy citation.
%% Also:
%% Modals and double modals in the Scandinavian languages
%% Working papers in Scandinavian syntax
%% Thrainsson and Vikner 1995 (but focussing on Danish and Icelandic)

Cluster C doesn't gain any interesting patterns with overuse
normalisation except for a surprising variety in the verbs: g\"ora
(do), hava (have), kunna (get), sk\"ola (shall), vara (be) and vilja
(want). Many uses of adverbs show up as well. It is not clear what
either of these patterns mean linguistically, however.
% I have no idea whether to make that verb singular or plural.
% So like whatever.

Cluster D gives no information whatsoever when the overuse
normalisation is added, simply because it has no informative
features. This is expected, given its nature as a combination of many
sites. The tradeoff of more informative features for the smaller
clusters is worthwhile.

Moving to other features sets with overuse normalisation,
leaf-ancestor paths and leaf-head paths give additional information
about cluster A that lead to the conclusion its defining
characteristic is simple sentences, simpler at least than the other
clusters. Specifically, cluster A's overused leaf-ancestor paths
include few nested sentences. This contrasts sharply with cluster B
and cluster C, which include many nested sentences. Cluster A does
have complex paths, but they feature prepositional phrases. (Note: NAC
stands for ``not a constituent'' and indicates that the parser could
not decide what the correct constituent was at that point.) (Or that
there are crossing branches, which is less common.)

This characteristic of cluster A appears in the leaf-head paths as
well; cluster A's paths contain many [adjective]-noun-preposition
sequences, but few verb-verb sequences that indicate nested
phrases. Again, cluster B and cluster C have many of these
sequences. Both clusters have a number of overused adverb features as
well, similar to the trigram results. Note that comparison to clsuter
D is less interesting. Because it has fewer unique characteristics,
when compared to it, clusters A, B and C show more generic
characteristics. For example, all three clusters show that their
sentences are generally more complex than the general sites in cluster D.

Analysis of the phrase-structure-rule features is difficult because of
all the noise. Features like S$\to++$-AB (conjunction-adverb)
S$\to$FV-PO-AB-VV (get verb-pronoun-adverb-verb) are hard to describe
as anything but junk rules created by the parser. On the other hand,
there are a lot of linguistically odd but reasonable rules like
S$\to$PO-AV-NP-IP (pronoun-copula-noun phrase-period), which makes a
certain kind of sense if you can be persuaded that copular sentences
are special enough to deserve their own rule. (Remember that
statistical parsers trained on interview data are particularly
susceptible to this kind of persuasion.)

Overall both normalisations leave something to be desired; without
overuse normalisation, only very common features appear. These
features convey only basic information, making it hard to identify
characteristics of a cluster. On the other hand, the overuse
normalisation is susceptible to noise, especially for more error-prone
feature sets. Even though more detail may be available with this
normalisation step, the features must be inspected for general trends
because individual features are not necessarily reliable.

\section{Comparison to Syntactic Dialectology}

\subsection{Delsing's Survey of the Norse Nominal Phrase}

\namecite{delsing03} surveys a number of dialectology
studies. According to him, the literature's heyday was 1880-1930. It
is reasonable to believe that some of the dialect features may have
changed in the intervening 70-120 years. This is particularly true in
the northern dialect areas, where improved travel and communication
will have added a centralising effect.

HOWEVER, I will soldier on nonetheless and look at each one.

TODO: It would be cool if I could figure out how to paste his maps
into this pdf. Not sure how.

\subsubsection{``Partitive'' Article}

Use of the partitive article varies in the northern parts of Sweden
and in the Swedish-speaking parts of Finland. There are two variants;
the northern one includes the sites Indal and Anundsj\"o, and the
southern one has Leksand and \.Arsunda on its border. Because of the age of
the data, Leksand and \.Arsunda.

Unforunately, the set of part-of-speech tags used for tagging is too
small to represent nouns with article suffixes. This makes it
impossible to identify this feature.

NEXT.

\subsubsection{Proper-Noun Articles}

In Northern Sweden (as well as Iceland and most of Norway), first
names are preceded by an indefinite article. Standard Swedish does not include
this feature. This feature is found along the Swedish border with
Norway as well as Northern Sweden, which includes the sites K\"ola,
Indal and Anundsj\"o.

Unlike the partitive article suffix, this feature is easy to detect in
our corpora. It should be visible in the trigram feature set as a
EN-PN (indefinite article-pronoun noun) sequence. The same sequence is
expected leaf-head paths. The phrase-structure-rule features should
look something like NP$\to$EN-PN. Syntactically, of course, this article
is somewhat different from indefinite articles, but at the shallow
level of syntactic analysis we are doing here, it looks the same.

\begin{table}
  \begin{tabular}{r|ccccc}
    & trigram & dep & redep & psg & grand \\ \hline
    Leksand & \\
    Indal & \\
    K\"ola & \\
    \end{tabular}
\end{table}
% trigram, dep, redep, (psg, grand)

% trigram-Indal
In Indal, 6 of the proper nouns are preceded by
meta-nouns. Whatever THOSE are. Otherwise it is precded by the normal
assortment of verbs and nouns, with fewer adverbs and pronouns. Oh,
and plenty of conjunctions and prepositions, but I filtered those out
because there were so many. But no ENs.
%dep-Indal
Plenty of ID-PN sequences, but the trigram evidence makes it look like
these IDs are all after the PN. No sign of ENs depending on PNs.
%redep-indal
Same here.
% psg-indal/grand-indal
Nothing of interest but of course it's possible to miss things with
psg because you only see one level.

So, maybe it's the fault of the tagger, or maybe the pattern just
isn't there. I suspect the latter, but you never know. I'm just
pessimistic.

%trigram-indal
There is a POST-nominal occurence, but that's not predicted. And
there is only one and with trigrams it's impossible to tell whether
it's a chance juxtaposition, and with only one occurrence it probably
is.

% Leksand
Nothing for Leksand either, although I didn't check for meta-nouns.

% K\"ola
K\"ola has 4 or 5 trigrams with post-nominal indefinite articles. None
before the proper noun, though.

For leaf-head paths, K\"ola has a consistent PN-ID-EN-AV-AV sequence
that happens several times. Like 5. The surroundings vary but the
whole thing is pretty consistent. The sequence PN-ID shows up in the
trigrams but that's it.

The meta-noun I found here came *after* the proper noun.

\subsubsection{Possessives and the article}

Somehow you get possessives on definite-article-suffix nouns. That's
weird, but the suffixes are already weird (and behave in such a way as
to suggest that they aren't {\it really completely} definite articles
anyway). Detecting this is difficult since the parts of speech don't
mark possessiveness. If there is a difference between regions, it will
have to be statistical; the regions that allow possessives with a
definite-suffix noun will have more possessive-noun combinations than
those that don't, due to the missing possessive$+$definite-noun
combination. However, the combinations will appear the same in terms
of part-of-speech.

There are three relevant varieties (besides the standard)

First, Faro/Fole/Sproge, same as southern Finland: the type that uses
both ``mitt/Pers huset'' and ``mitt/Pers det gamla huset''. This
predicts both PO-NN and PO-PO-NN etc etc (see the second option).

Does not work: 0.06393 vs 0.07316

Second, Bara (and Malm\"o of course), same as Denmark: the type that
uses ``mitt/Pers det gamla hus(et)''. This translates to n-grams like
PO-PO-NN or PO-PO-AJ-NN or NN-PO-NN or NN-PO-AJ-NN. There is no
separate tag in our increasingly crappy tagset for definite articles;
they are represented as Pronoun. Crazy, I know.

This works! Yay! 0.00686 vs 0.00594. Coming next time: is this
significant? Probably not, but I haven't measured it yet.

As an aside, much of this missing information IS available to me, so I
could look manually. But none of it made it through to the distance
measures, and this analysis compares the way the distance measures
make the decisions with the way that linguists make their
decisions. So I have to use only the information that the distance
measures used.

Third, northern Sweden, which includes Indal and Anundsj\"o: the type
that uses both ``Pers huset'' and ``huset Pers''. Here we should
expect both PN-NN or PO-NN and NN-PN or NN-PO. That's pretty broad,
but it's because we don't have the markup for possessiveness.

Does not work: 0.00435 vs 0.00594

\subsubsection{Pronominal Possessives}

In normal Norse (which is confusingly similar to Swedish, except
where it differs), genitive constructions can have an intervening
pronoun, as in {\it Per sitt hus} ``Per's house'' (gloss: Per
3-poss-pronoun house). In Swedish, this is known as the reflexive genitive
construction. However, this is not allowed with possessive pronouns:
*{\it han sitt hus}. The prepositional genitive (which exists in
English as well) behaves the same way: {\it huset till Per} ``Per's
house'' (gloss: house-the of Per) is legal but *{\it huset till meg}
``my house'' (gloss: house-the of me) is not.

There is an exception for kinship words, which I don't understand
yet. But somehow ``far min'' is different (maybe just because it's not
``min far''?)

So basically standard Swedish allows trigrams sequences like NN-PO-NN
({\it Per sitt hus}) but not  PO-PO-NN ({\it han sitt hus}). It also
allows

Does not work (is too close to call): 0.02229 vs 0.2429

Reversing the bigram, looking for PO-NN in the south gives
Works (but is still super close): 0.04243 vs 0.03998

It looks like one set just uses more nouns than the others or
something. Conclusion: inconclusive, leaning toward no---it looks like
they're the same.

\subsubsection{Prepropriell Possessives}

Redundent post-nominal possessive pronoun for post-nominal proper-noun
possessors. That is, {\it Huset hans Per} ``The house his Per''.

This overlaps partly with possessives and the article; the nothern
section is alternative 3 there, which was found not to work.

There is some overlap into Sweden from Norway of this pattern. Of the
interview regions, it covers K\"ola.

This predicts K\"ola to have more trigrams of the type NN-PO-PN, as
opposed to NN-PN elsewhere, or PN-NN in the north. Note that K\"ola is
pretty iffy--it's right on the border, but Delsing's map is hard to
interpret (it is distorted in a different way than most maps of
Sweden), so it's quite possible that it isn't predicted to differ anyway.

Does not work: 0 vs 0.00001

So it doesn't occur anywhere, save for some noise. Conclusion:
inconclusive, leaning toward no---it could be a lack of data or more
likely neither has this construction.

\subsubsection{Noun possessives}

Specifically s-genitives in the dative case. I'm pretty sure my wimpy
part-of-speech distinction can't catch this, and the grammar is the
same as without the dative case, so this is impossible. Oh well.

\subsubsection{Adjective attributes}


\subsection{That Other Analysis from Dialektboka}

I think there was another one. (Later) It is more likely that the
other two papers on Swedish are analyses of individual patterns
discussed in Delsing's survey paper. Interesting, but not relevant to
this dissertation until basic equivalent is established between the
features and I have moved on to rating how well the features capture
actual syntactic phenomena.

That is not likely to happen at this rate.

\subsection{Rosenkvist's Analysis of the South Swedish Apparent Cleft}

Assuming I can get decent features from his analysis, and some kind of
distribution. I guess ``south'' probably means Sk\.ane and Blekinge
but I'm not sure.

\section{Comparison to Phonological Dialectometry}

Come to think of it, this would be better after syntactic
dialectometry. Either way, the basic conclusion is that the
north-to-south thing is the same, but the cities is new. And that a
quantitative study would be amazing but neither of us had time.

\section{Comparison to Syntactic Dialectometry}

In previous work on British English, this method failed to find
agreement between syntactic distance ($R$) and phonological distance
(Levenshtein) distance---there was no
significant correlation between the two methods. Although both showed
something like a North/South distinction in Britain, its orientation was much more
obvious from phonological distance. This lack of agreement was a
preliminary answer to the question of whether multiple
ways of measuring linguistic distance give the same results.

However, there were at least five reasonable explanations for the difference
between the two distance measures.
% First, and
% least satisfying, is the possibility that one of the distances is not
% measuring what it is supposed to. Second, the corpora may not agree
% because of the 40 year difference in age and differing collection
% methodologies. Third, syntactic and phonological dialect markers may
% not share the same boundaries.

\begin{enumerate}
\item One or both of the distances does not measure what it is supposed to.
\item The two corpora may not agree on dialect boundaries because of
  their 40-year difference in age.
\item Place of birth, as recorded in the ICE, may not correlate well
  with spoken dialect, especially given variations in speaker
  education level and place of residence.
\item Dialect boundaries may appear from systematic variation in
  annotation practices rather than the speech.
\item Syntactic and phonological dialect boundaries may be different.
\end{enumerate}

Of these, this dissertation addresses the second, third and fourth
problems directly by using a single corpus, Swedia2000, annotated by a single
person. (TODO: This may not be true for the phonological annotation.)
By finding significant distances between all interview sites of
Swedia2000, it also suggests that $R$ is measuring syntax
distance. PROBABLY.

The last is the most interesting because previous work will not have
exposed this difference. Traditional dialectometry focuses on a strong
agreement among a few features from each collection site. Because
syntactic features are fewer in number than phonological ones, they
are under-represented in this type of analysis. Unfortunately, this
means that the syntactic contribution to isogloss bundles is
correspondingly reduced. In addition, because of isogloss bundles'
insensitivity to rare variations, syntactic features rarely contribute
to isogloss bundles of successful dialect boundaries.
% I really need to CITE this.

In contrast, computational analysis, such as \cite{shackleton07},
captures feature variation precisely using statistical analysis and
sophisticated algorithms. The resulting analysis displays dialects as
gradient phenomena, displaying much more complexity than the
corresponding isogloss analysis. But current specialized computational
methods only apply to phonology. Syntactic data cannot be analyzed
without a syntax-specific method.

This paper attempts to address that lack, and provide some first steps
to show whether syntax and phonology assist each other in establishing
dialects, or whether their dialect regions are unrelated. If they are not
related, and syntactic gradients can be as weak as phonological ones,
then some new dialect regions may become apparent that were not visible in
previous phonology-only analyses.

\subsection{Improvements on British Dialect Experiment}

This dissertation improves on the British experiment in a number of
ways. It addresses the obvious criticism that syntax distance on the
ICE requires so much data that the results are no more informative
syntax those of traditional dialectology---its precision lags
phonological distance methods badly. However, $R$ works with much
smaller corpora when run on Swedia2000. This shows that the problem
with the British experiment is not the distance method, but the
corpus, which fails to capture dialect differences. Most likely is
that the interviewees, mostly in a college setting, actively tried to
suppress dialect differences during the interview.

Another problem with the current study is the 40-year difference in
collection dates between the phonological corpus and the syntactic
corpus. A recent phonological corpus would likely show the same sort
of changes in the North/South divide that show up in the syntactic
corpus. The British population became more mobile during the second
half of the 20th century, and the SED survey explicitly attempted to capture
the dialects that existed before this happened \cite{orton78}.
It would also be nice to have data from the rest of the United Kingdom for
comparison as well, or at least Scotland and Wales as with the ICE.


% TODO: integrate this.
% Alternatively, I could just look at the region pairs that fail to
% achieve significance in the syntactic permutation test and check to
% see if their phonological distance is lower than the other pairs. I
% don't do this (yet).

One interesting question is
how phonological and syntactic distances correlate with geographic
distance---\namecite{gooskens04a} shows that often the correlation is
very good. This would also allow better visualization of dialect areas
than a hierarchical dendrogram.

\section{Future Work}

Try all those smarter variants of $R$.

Include the rest of Nodalida once it is done.

Better normalisation and feature ranking are needed. It appears that
the current normalisations vary either in favouring differences
only in high-frequency features OR in favouring rare features so much
that the most important appear to be those that only occur in one of
the two regions.

\section{The End}

This dissertation contributes a better understanding of syntax with
respect to dialectometry. It establishes that statistical methods can
find interesting things, and with not much more data than is expected
of previous dialectometry in other areas. Remember, the majoriy of
interviews used here were less than 1000 sentences. Previous work
pointed the way (Nerbonne \& Wiersma (2006) and Sanders (2008)) but
failed to establish the utility and reliability of these methods
either by lack of dialect application or by lack of consistent
results. This dissertation addresses these shortcomings
comprehensively.

In addition, it points the way toward future work in Swedish; while
the results here are interesting, it is difficult to corraborate them
solidly because of the lack of study on Swedish dialects, both in
dialectology and dialectometry. This gap in the literature is on its
way to being remedied with the work of Leinonen in dialectometry and
X,Y,Z in dialectology.

Like its findings, future directions based on this work are
twofold. In general dialectometry, syntactic investigations should
begin, hopefully extending to languages for which the syntactic
variation is already well-studied.

In Swedish, I hope that this investigation of syntactic dialect
variation will lead to further work in this area; there is little
enough right now, and perhaps a computer-generated overview of the
interesting features will spark some new avenues of investigation for
linguists.

%%%%%%%%% cut two (raw) %%%%%%%%%

This dissertation establishes that statistical methods for syntactic
dialectometry can be useful. The results show that significant
distances can be obtained, which was shown by earlier work and not
much else. They show that dialect distances correlate with geographic
distance for many parameter settings. Clustering dialect distances
reproduces two well-known aspects of Swedish dialects: the Scanian
border, and a north-to-south gradient, as well as a previously unremarked
aspect: differences between the cities and the
countryside. Mult-dimensional scaling produces the same
results. Finally, interesting features have been discovered from these
same clusters, but features in the dialectology literature do not
necessarily appear.

In summary, this dissertation has answered its questions with some
degree of accuracy. Although the more complete verification is still
missing, it points the way for practical studies in the future: many
parameter variations are explored and the most efficacious are pointed
out.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
