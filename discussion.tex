\chapter{Discussion}

This chapter discusses three topics:

\begin{enumerate}
\item Analysis of results: dissertation work on its own.
\item Comparison to Swedish syntactic dialectology.
\item Comparison to Swedish phonological dialectometry.
\item Conclusions: summary of discussion
\item and then summary of dissertation: contribution to dialectometry at large.
\end{enumerate}

However, I can get some of the discussion from my first qualifying
paper; the syntax distance method hasn't changed, it just works a lot
better on a dialect corpus.

A big question is why trigrams are so good. All of the fancier feature
sets do worse than trigrams.


\section{Analysis of Results}

% TODO: Need to add references to results chapter and also any
% citations

\subsection{Significance of Dialect Distance}

Analysis of the significance of dialect distance provides a measure of
how reliable the distances to be analysed later in this chapter are. A
distance that does not find significant distances between of 30
regions is not suitable for precise inspection, although small numbers
of non-significant distances will still allow less precise methods to
return interpretable results.

From the very few significant distances found by comparing whole
regions to regions, It is obvious that fix-sized sampling is needed to
work around size differences; sentence length normalisation does not
adequately adjust for the effects of having more sentences, nor is the
difference negligable as might be hoped. For the rest of the analysis,
whole-region sampling will be skipped in favour of 1000-sentence
fixed-size samples.

TODO: Comparison of frequencies is slightly more successful than
  comparison of ratios. (This also makes sense because of the noisy
  parsing as well as the smallish corpora)

\subsubsection{Significance by Measure}

The distance measures most likely to find significance are, in order,
cosine dissimilarity, Jensen-Shannon divergence and $R$. Cosine
similarity resulted in all significant distances, even for
part-of-speech unigrams. Unigrams are intended as a baseline and as
such, it is unexpected for. More on cosine similarity will explored
in later sections.

Of the two pairs of similar methods, it is not surprising that
Jenson-Shannon divergence provides more reliable results than
Kullback-Leibler divergence. Although both are called ``divergence'',
JS divergence is originally a dissimalarity, while KL divergence must
be converted to a dissimarility, which is symmetric, by measuring it
twice. Because of the placement of the logarithm in KL divergence, it
can only compare features that occur at least once in both corpora (see equation
\ref{klmeasure} in chapter \ref{methods-chapter}). It must skip
features unique to a single corpus in order to avoid division by zero.

On the other hand, there is no reason to expect that $R^2$ would
provide less reliable results than $R$---the only difference is that
$R$ sums the absolute difference in feature counts rather than the
squared difference. Yet $R$ consistently finds more significant
distances than $R^2$. This is backward from the expected result that
$R^2$ would exaggerate the most important feature differences by
squaring them. Furthermore, the difference is not uniform across feature
type; the differences are largest for leaf-ancestor paths and
arc-head paths.

In the following analysis, I will further investigate the causes for the difference
between $R$ and $R^2$, as well as investigate whether cosine
similarity's ability to find significant distances between practically
anything will extend to good results based on measures besides
statistical reliability.

\subsubsection{Significance by Feature Set}

% \item Unigrams do form an adequate baseline; they are bad but not too
%   bad.

The feature sets most likely to find significance are, in order,
trigrams, all combined features and leaf-head paths (both with
support-vector-machine training and with Timbl's instance-based
training). Without ratio normalisation, the other feature sets are not
much worse, but with it included, these three are the best by some
distance.

The relatively high quality of trigrams does not make sense given only
the linguistic facts; however, it is likely that the entirely
automatic annotation used here introduces more and more errors the
more annotators run, operating on the output of previous automatic
annotators. Trigrams are the result of only one
automatic annotation, and one for which the state of the art is at or
near human performance. So the fact that these particular trigrams are
of higher quality than these particular dependencies or constituencies
is probably the deciding factor in their higher number of significant
distances. Although it is impossible to tell from my results, I
predict that a manually annotated dialect corpus would show that
non-flat syntactic structure is helpful in producing significant
distances.

Given the above facts, the question should rather be: why are
leaf-head paths perform as well as they do? Better, for example, than
the leaf-ancestor paths on which they're modelled. It could be that
there is less room for error; many of the common leaf-head paths are
short: short interview sentences with simple structure make for
shorter leaf-head paths than leaf-ancestor paths. As a result, the
important leaf-head paths consist mainly of a couple of
parts-of-speech. But in that case, adding the ratio normalisation
should remove their advantage over leaf-ancestor paths, but it does
not.

Another reason could be a difference in parsers: MaltParser has been
tested before with Swedish (CITE). Besides English, the Berkeley
parser has been tested prominently on German and Chinese. Therefore,
the difference would better be explained by appealing to the
difference in parsers rather than an unsuitability of Swedish for
constituent analysis.

It is disappointing linguistically that trigrams provide the most
reliable results so far; a linguist would expect that including
syntactic information would make it easier to measure the differences
between regions. If it is, as hypothesised here, an effect of chaining
machine annotators, a study using manually annotated corpora could
detect this. However, it still means that trigrams are the most useful
feature set from a practical view, because automatic trigram tagging
is very close to human performance with little training. That means
the only required human work is the transcription of interviews in
most cases.

On the other hand, if additional features sets are to be developed for
a corpus, then combining all available features seems to be a
successful strategy. The distance measures seem to be able to use all
available information for finding significant distances.

\begin{enumerate}
\item In addition, their results call the cosine measure into question. Why is it so
  good at finding significant distances? Why are the maps it produces (below) so
  confusing and different from the others?
\end{enumerate}
\subsection{Correlation}
TODO: ALSO I should point out that geographic correlation IS the
default expected by normal old boring linguists. The Nordic Languages
is pretty clear on this plus I have some references from
dialectology papers.

Correlation with geographic distance or travel distance indicates that
a distance measure follows the expectations of linguists about
distribution of dialects. A lack of correlation does not mean that a
measure is useless, but presence of correlation means that the
distance measure asserts the well-known tendency of dialect
distributions to be more or less smoothly gradient over physical
space.

Taken as a whole, the distance measures are more likely to correlate
significantly with travel distance than with straight-line geographic
distance. This makes sense since the difficulty of moving from place
to place is what influences dialect formation, and taking roads into
account is an improved estimate over straight-line distance. It is
interesting, though, that there is such a difference given that geographic
and travel distance are themselves highly correlated.

Unlike the number of significant distances, there is a strong effect
of the ratio normalisation on the travel distance
correlations. Without the ratio normalisation, only cosine similarity
with trigram and unigram POS features correlate significantly with
travel distance. With ratio normalisation, quite a few of the
parameter settings correlate significantly with travel
distance. Notably, the trigram feature set and combined feature set
all correlate, except as measured by cosine similarity. As measured by
Kullback-Leibler divergence, a number of feature sets besides trigrams
also correlate significantly, but most of these are not valid because
the distances on which the correlation is based are not all significant.

% (On cosine's n-gram correlation with geography)
% TODO: This analysis ends with idle speculation. I should replace it
% with fevered speculation.
With ratio normalisation, once again cosine similarity measured over
unigram POS features correlates significantly with travel
distance. What's more, this is the {\it only} significant correlation
in the ratio normalised case for cosine. This seems strange. A
possible explanation is that unigrams are simpler, so the type count
is a higher than for other measures. But this distinction doesn't show up in
the 1000-sample size, which should have lower type counts because of
its limited size.

The results of travel distance correlation indicate that ratio
normalisation improves the results above the sentence-length
normalisation alone. This contradicts the conclusion one can take from
the significant distance analysis, which says that the ratio
normalisation harms significance and is therefore less
successful. This is an instance of the noise/quality tradeoff,
however: ratio normalised features appear to listen more closely to
the data, at the expense of being interfered with by the noise there.
% wow that last sentence sucks. but you get the idea

\subsubsection{Self-Correlation}

It appears that a cosine is arriving at different results than the
other measures, though the correlation is still much higher than with
travel distance. A more precise investigation of this anomaly appears
below in section \ref{feature-ranking}.

\subsubsection{Correlation with Corpus Size}

The correlation of corpus size and dialect distance is a problem. It
is not a predicted as a side effect of the way dialect distance is
measured. The correlation of travel distance with corpus size makes
the situation murkier: the ratio-normalised distances correlate more
highly with travel distance than non-ratio-normalised distances, but
they also correlate more highly with corpus size. Is corpus size the
determining variable? Or is there an unknown variable influencing all
three? Some possibilities are ``interviewer boundaries'', common in
corpora collected by multiple people \cite{chambers98}, or perhaps the
interviewers got better over time and collected longer interviews as
they moved throughout the country,
or perhaps cultural differences between the interviewer and
interviewees caused some participants in one area to talk more than in
another area.

% TODO: It would be cool if this were *present work* instead
In any case it is possible that the fixed-size sampling method is not doing its
job in eliminating size differences between corpora. Future work
should develop a method for normalising a comparison between two full
corpora. It should avoid sampling, but also take the relative number
of sentences into account. It is not difficult to come up
with a simple method to do so, but it needs some checking to make sure
that the method is valid.

\subsection{Cluster Dendrograms}

The cluster dendrograms in chapter \ref{results-chapter} are dangerous
to interpret too closely on their own; the instability of a single
dendrogram means that small clusters cannot be analysed reliably. For
example, in figure \ref{cluster-r-trigram-freq}, a two-way split
between the regions on the top and bottom of the page is
obvious, and a three-way split is easy to argue for, but small
clusters like the first four sites (Fole, Skinnskatteberg, Faro and
Vaxtorp) are not likely to be found in that exact configuration in
other dendrograms.

It is safer to analyse the consensus trees; the smoothing effect of
taking the majority rule of each cluster will show where the optimal
cutoff for splitting clusters is. The consensus tree for
sentence-length normalisation has 5 main clusters, one of which has
two additional subclusters. Additionally, Viby and Bara don't cluster
with any other interview site.

In contrast, the consensus tree for ratio normalisation only has two
subtrees: Loderup/Bredsatra and Jamshog/Ossjo/Torsas. By this measure,
adding the ratio normalisation is less successful than leaving it out:
the variations over measure and feature set do not come to much of an
agreement, so there is less information available from the ratio
normalised consensus tree.

\begin{figure}
\begin{itemize}
\item S:t Anna
\item Sorunda
\end{itemize}
\caption{Red Cluster}
\label{red-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item J\"amsh\"og
\item Tors\.as
\item \"Ossj\"o
\end{itemize}
\caption{Pink Cluster}
\label{pink-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Breds\"atra
\item K\"ola
\item L\"oderup
\end{itemize}
\caption{Brown Cluster}
\label{brown-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item B\"oda
\item Frilles\.as
\item Norra R\"orum
\item Tors\"o
\item Villberga
\end{itemize}
\caption{Yellow Cluster}
\label{yellow-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Indal
\item Leksand
\end{itemize}
\caption{Green Cluster}
\label{green-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Bengtsfors
\item Floby
\item Segerstad
\end{itemize}
\caption{Cyan Cluster}
\label{cyan-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Ankarsrum
\item Anundsj\"o
\item Asby
\item F\.ar\"o
\item Fole
\item Orust
\item Skinnskatteberg
\item Sproge
\item V\.axtorp
\item \.Arsunda
\end{itemize}
\caption{Blue Cluster}
\label{blue-cluster}
\end{figure}

When these clusters are mapped onto the geography of Sweden, some
patterns are visible. The largest cluster, The blue-green cluster
forms a non-north non-metropolitan set. Specifically, the green
cluster is in the north, as are the blue and cyan cluster for the most
part. The Gotland interview sites also form part of the blue
cluster. However, the yellow cluster is interspersed through this
cluster and others. I see no obvious interpretation of it, despite the
fact that its five members clustered together in the majority of
dendrograms. The red cluster is easier to explain; since it is close
to Stockholm, it probably represents the metropolitan
dialect. Finally, the brown cluster appears to be only on the edge of
country, so perhaps its sites are remnants of an older dialect.

Most clear is the pink cluster containing J\"amshog, Tors\.as and
\"Ossj\"o. These three sites are clearly related both by dialect and
by geography. However, one item of note is that \"Ossj\"o is quite
different from its close neighbours V\"axtorp and Norra R\"orum. So
too is Tors\.as in a different cluster from the nearby Segerstad,
although that site is on an island.

I should probably spend time talking about the relation of these
findings to linguistics, unless I want to do that in a later
section. It looks like I will do it in multiple later sections, so
best to keep this short for now I guess.

\subsection{Composite Cluster Maps}

Composite cluster maps use a similar underlying technique as consensus
trees--cluster dendrograms, but combine and present the information in
a very different way. The result is much more like the traditional
isogloss boundaries of dialectology.

Both composite cluster maps, with and without the ratio normalisation,
support the north-to-south gradient held by dialectologists. (CITE:
The Nordic Languages). There are a number of horizontal boundaries
throughout Sweden, none of them particularly strong.

However, K\"ola and Frilles\.as still separate fairly well from their
neighbours. These sites are on the edges of the country and have strong borders
with surrounding, Like the cluster J\"amshog, Tors\.as and \"Ossj\"o,
these sites are different from the others. However, they don't have
any geographic coherence, so it is more likely these are remnants of a
dialect that was historically wider spread and has since receded.

Of all the boundaries, the one between Scania and the rest of Sweden
is the strongest. Due to the lack of interview sites in the middle of
south Sweden, the boundary is drawn further north than it
traditionally appears, but this is an effect of the software the
produced the figure. Notice that there is also a boundary between
J\"amshog, Tors\.as, and \"Ossjo\" and the other sites in
Scania. Their presence along the northern border of Scania is one
reason why its boundary with the rest of Sweden is so strong.

\subsection{Multi-Dimensional Scaling}

Multi-dimensional scaling (MDS) operates similarly to cluster
dendrograms in that it reduces the high-dimensional distances to a
lower-dimensionality representation. It differs, however, in producing
gradient numbers, not binary trees. This means that the MDS maps
naturally have gradient borders.

Despite this difference, the similarities between MDS and cluster
dendrograms are evident in the MDS results; for sentence-length
normalisation, there is a clear equivalence between the blue-green
cluster in the consensus tree and the the orange sites in the MDS map
based on $R$ measured over trigram features. The light brown cluster
of K\"ola, Loderup and Breds\"atra is replicated as well, this time in
dark brown. The pink sites of the consensus tree are blue-green this
time, which reveals an important fact that is easy to gloss over from
looking at the consensus tree: J\"amshog, in the middle, is almost as
different from Tors\.as and \"Ossj\"o as it is from all the other
sites.

The main place that the $R$ results differ from the consensus tree is
in its pink sites. They group together the areas around the
major cities of Sweden: besides the sites near Stockholm and Uppsala,
the Scanian sites near Malm\"o are a similar colour. This differs
somewhat from the consensus tree which has yellow and red sites around
Stockholm and Uppsala, but doesn't cluster Viby or Bara at all. Here,
Viby and Bara appear close to the other pink sites.

MDS maps based on other parameter settings produce similar results,
but with some colour shifting, a side effect of the method of mapping
3-dimensional results onto red,green,blue colour triples. For example,
with all features combined, the MDS map for $R$ uses purple for
K\"ola, Loderup and Breds\"atra instead of dark brown, and light pink
in place of orange. The pattern is essentially the same, but the
more similar colours indicate that trigram features separate
the different regions more strongly.

The results change slightly, after adding the ratio normalisation. The
city group loses members to the country group. What's more, the
originally blue-green cluster move much close to the country group
as well--in figure \ref{mds-r-trigram-freq}, with
$R$ measured over trigram features, the countryside group is green,
and the blue-green cluster has become dark green. When all features
are combined, the city group also becomes much closer in colour to the
country group.

Basically, the results from MDS are similar to those of cluster
dendrograms. Within the MDS maps, the clearest pattern was visible in
the $R$ distances measured over trigram features. Note that this is
predicted by the number of significant distances, too: $R$ and
Jensen-Shannon divergence have the highest number of significant
distances of the measures, and trigrams and combined features have the
highest number of significant distances of the feature sets. And the
ratio-normalised results have consistently fewer significant distances
than those only normalised for sentence-length.

\subsection{Features}

%TODO: Put back in the freq/ratio features for $R$ instead of the
%overuse-ranked features. I guess overuse isn't that useful after all.

Analysis of specific features

Ratio and sentence-length normalisation are the same for this case
because ratio normalisation is a scaling that is identical across
features. There is an additonal normalisation used here, called
overuse normalisation. It ranks rarely used features more highly.
This is useful because the very common features are very common in all
sites---they differ only in degree. This is difficult to analyse,
especially to compare to previous linguistic analyses.

There actually is a difference for the Jensen-Shannon trigram
distances. The trigrams don't seem very informative, but here is what
I noticed: Overall, the ratio normed trigrams are very common, typical
of the core of the sentence. This fits with the nature of $R$ and
Jensen-Shannon; both are sums and trigrams that occur in nearly all
sentences are the most likely to be high-ranked.

Odd patterns of adverbs generally; adverbs are generally more common
in cluster B's trigrams. Upon closer inspection, non-B clusters do
have uses of adverbs, but they differ in their content. (This is what
you would expect when ranking lots of common trigrams: subtle
differences in the distribution based on statistical variation.)
Specifically, the adverbs are usually close to the verb in cluster
B. This is not true in the other clusters---adverbs typically appear
near nouns, usually at the end of sentence. When they do appear next
to the verb, it is a copula (at least, a form of the verb {\it vara}
``to be''). Another odd adverb pattern is that adverbs don't commonly
occur at the end of sentences in cluster B, unlike the others. Either
it's still there but less common, or it just doesn't happen, I'm not
sure.

preposition-noun-conjuction and noun-conjunction-pronoun showed up
commonly as well. I'm not sure what to make of that except to say that
it seems conjunctions are more common in cluster B. This might also
explain the lack of advers at end of sentence--if the overall length
of sentences is longer, then more conjunctions is expected, along with
fewer words at the ends of sentences (because, for each feature, there are fewer
sentences to end).

Overuse ranked features seem to highlight conjunctuions. But there is
no other pattern--just a lot of variation and juxtapositions that
don't make any sense. It seems like overuse ranking is susceptible to
noise from the parser.

The bulk of evidence varies for/against cluster B. I don't see any
clear patterns. This is true for both overuse and ratio/freq norms; in
fact, it often reverses between the two.

Variation over $R$ features:  the same results show up here, but some
of the patterns are easier to see because the features are easier to
read than trigrams.

In particular, leaf-ancestor paths show off well the tendency for
nested sentences with features like S-NP-S-VV and S-NP-S-S-AB. It
makes me wish one of the features were just plain old sentence
length. Boring but it might be important. Those last two features, by the
way, are also adverb features S-S-S-AB.

(Note that leaf-heads I didn't reverse, so they are read from leaf to
root, while lead-ancestors read from root to leaf. I should fix this
and give relevant examples all the way through this discussion)

Overall both normalisations leave something to be desired; freq/ratio
only gives us features that are safe from noise, but are mostly the
same so they're hard to interpret, these relatively small variations
in common trigrams. On the other hand, overuse normed features are
basically full of noise and thus impossible to interpret.

Yeah, basically overuse norming is useless. That sucks. I guess
freq/ratio it is. Distort the number as little as possible eh, even if
you miss out on some informations.

%% oops I left this in %%
Things I noticed: the freq top-5 features are basically useless:
they're always just the simplest sentence types like PO-VV-AB,
regardless of side. There IS a difference though, it's just that it
flips back and forth, so from top-5 freq features you can only tell there is
a difference, but not the content, since the top-5 features are always
the same.

Cosine overuse features seem to be the same as the others. So no
difference there.

Ratio top-5 has the same problem as freq top-5, the values are just
smaller. In fact, I think the ratios when scaled up give the same
values as the freqs, so there's no point in including them. I should
just note it and skip them.

Overuse top-5 features focus heavily on rare features. So very long
features show up. How important are these really? It's hard to
tell. They do seem to be very specific to cluster B, however.

TODO: See if there's some pattern to the top-5 overuse features.

\subsection{Cosine}

What is up with cosine? Seriously.

\section{Compared to Previous Work}

In previous work on British English, this method failed to find
agreement between syntactic distance ($R$) and phonological distance
(Levenshtein) distance---there was no
significant correlation between the two methods. Although both showed
something like a North/South distinction in Britain, its orientation was much more
obvious from phonological distance. This lack of agreement was a
preliminary answer to the question of whether multiple
ways of measuring linguistic distance give the same results.

However, there were at least five reasonable explanations for the difference
between the two distance measures.
% First, and
% least satisfying, is the possibility that one of the distances is not
% measuring what it is supposed to. Second, the corpora may not agree
% because of the 40 year difference in age and differing collection
% methodologies. Third, syntactic and phonological dialect markers may
% not share the same boundaries.

\begin{enumerate}
\item One or both of the distances does not measure what it is supposed to.
\item The two corpora may not agree on dialect boundaries because of
  their 40-year difference in age.
\item Place of birth, as recorded in the ICE, may not correlate well
  with spoken dialect, especially given variations in speaker
  education level and place of residence.
\item Dialect boundaries may appear from systematic variation in
  annotation practices rather than the speech.
\item Syntactic and phonological dialect boundaries may be different.
\end{enumerate}

Of these, this dissertation addresses the second, third and fourth
problems directly by using a single corpus, Swedia2000, annotated by a single
person. (TODO: This may not be true for the phonological annotation.)
By finding significant distances between all interview sites of
Swedia2000, it also suggests that $R$ is measuring syntax
distance. PROBABLY.

The last is the most interesting because previous work will not have
exposed this difference. Traditional dialectometry focuses on a strong
agreement among a few features from each collection site. Because
syntactic features are fewer in number than phonological ones, they
are under-represented in this type of analysis. Unfortunately, this
means that the syntactic contribution to isogloss bundles is
correspondingly reduced. In addition, because of isogloss bundles'
insensitivity to rare variations, syntactic features rarely contribute
to isogloss bundles of successful dialect boundaries.
% I really need to CITE this.

In contrast, computational analysis, such as \cite{shackleton07},
captures feature variation precisely using statistical analysis and
sophisticated algorithms. The resulting analysis displays dialects as
gradient phenomena, displaying much more complexity than the
corresponding isogloss analysis. But current specialized computational
methods only apply to phonology. Syntactic data cannot be analyzed
without a syntax-specific method.

This paper attempts to address that lack, and provide some first steps
to show whether syntax and phonology assist each other in establishing
dialects, or whether their dialect regions are unrelated. If they are not
related, and syntactic gradients can be as weak as phonological ones,
then some new dialect regions may become apparent that were not visible in
previous phonology-only analyses.

\subsection{Improvements on British Dialect Experiment}

This dissertation improves on the British experiment in a number of
ways. It addresses the obvious criticism that syntax distance on the
ICE requires so much data that the results are no more informative
syntax those of traditional dialectology---its precision lags
phonological distance methods badly. However, $R$ works with much
smaller corpora when run on Swedia2000. This shows that the problem
with the British experiment is not the distance method, but the
corpus, which fails to capture dialect differences. Most likely is
that the interviewees, mostly in a college setting, actively tried to
suppress dialect differences during the interview.

Another problem with the current study is the 40-year difference in
collection dates between the phonological corpus and the syntactic
corpus. A recent phonological corpus would likely show the same sort
of changes in the North/South divide that show up in the syntactic
corpus. The British population became more mobile during the second
half of the 20th century, and the SED survey explicitly attempted to capture
the dialects that existed before this happened \cite{orton78}.
It would also be nice to have data from the rest of the United Kingdom for
comparison as well, or at least Scotland and Wales as with the ICE.


% TODO: integrate this.
% Alternatively, I could just look at the region pairs that fail to
% achieve significance in the syntactic permutation test and check to
% see if their phonological distance is lower than the other pairs. I
% don't do this (yet).

One interesting question is
how phonological and syntactic distances correlate with geographic
distance---\namecite{gooskens04a} shows that often the correlation is
very good. This would also allow better visualization of dialect areas
than a hierarchical dendrogram.

\section{Future Work}

Try all those smarter variants of $R$.

Include the rest of Nodalida once it is done.

Better normalisation and feature ranking are needed. It appears that
the current normalisations vary either in favouring differences
only in high-frequency features OR in favouring rare features so much
that the most important appear to be those that only occur in one of
the two regions.

\section{The End}

This dissertation contributes a better understanding of syntax with
respect to dialectometry. It establishes that statistical methods can
find interesting things, and with not much more data than is expected
of previous dialectometry in other areas. Remember, the majoriy of
interviews used here were less than 1000 sentences. Previous work
pointed the way (Nerbonne \& Wiersma (2006) and Sanders (2008)) but
failed to establish the utility and reliability of these methods
either by lack of dialect application or by lack of consistent
results. This dissertation addresses these shortcomings
comprehensively.

In addition, it points the way toward future work in Swedish; while
the results here are interesting, it is difficult to corraborate them
solidly because of the lack of study on Swedish dialects, both in
dialectology and dialectometry. This gap in the literature is on its
way to being remedied with the work of Leinonen in dialectometry and
X,Y,Z in dialectology.

Like its findings, future directions based on this work are
twofold. In general dialectometry, syntactic investigations should
begin, hopefully extending to languages for which the syntactic
variation is already well-studied.

In Swedish, I hope that this investigation of syntactic dialect
variation will lead to further work in this area; there is little
enough right now, and perhaps a computer-generated overview of the
interesting features will spark some new avenues of investigation for
linguists.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
