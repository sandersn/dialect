\chapter{Discussion}

This chapter discusses three topics:

\begin{enumerate}
\item Analysis of results: dissertation work on its own.
\item Comparison to Swedish syntactic dialectology.
\item Comparison to Swedish phonological dialectometry. (If Therese
  sends it to me in time.)
\item Comparison to syntactic dialectometry.
  That is, previous work. That is, MY previous work, plus
  Wybo's. That's pretty much it you know.
\item Conclusions: summary of discussion
\item and then summary of dissertation: contribution to dialectometry at large.
\end{enumerate}

A big question is why trigrams are so good. All of the fancier feature
sets do worse than trigrams. I should address this in the summary for
sure.

\section{Analysis of Results}

% TODO: Need to add references to results chapter and also any
% citations

\subsection{Significance of Dialect Distance}

Analysis of the significance of dialect distance provides a measure of
how reliable the distances to be analysed later in this chapter are. A
distance that does not find significant distances between of 30
regions is not suitable for precise inspection, although small numbers
of non-significant distances will still allow less precise methods to
return interpretable results.

The highest number of significant distances are found in the first
case (figure \ref{sig-1-1000}): 1 round of normalisation with a
fixed-size sample of 1000 sentences. From there, both full-corpus
comparisons (figure \ref{sig-1-full}) and 5 rounds of normalisation
(figure \ref{sig-5-1000}) have fewer significant distances, although
the number is still usable. However, the combination of the two, with
5 rounds of normalisation over full-corpus comparisons, has only one
combination with fewer than 5\% of distances that are {\it not}
significant. Although both full-corpus comparisons and multiple rounds
of normalisation may increase the precision of the results, their
combined effect on significance is so detrimental that its results are
useless. For the rest of the analysis, the combination of full-corpus
comparison and 5 rounds of normalisation will be skipped.

\subsubsection{Significance by Measure}

The distance measures most likely to find significance are, in order,
cosine dissimilarity, Jensen-Shannon divergence and $R$. Each method
had different parameter settings for which it was stronger. For
1000-sentence sampling, cosine similarity resulted in all significant
distances, even for part-of-speech unigrams, which are intended as the
baseline feature set. Excluding unigrams, Jensen-Shannon divergence
has similar performance. For full-corpus comparisons, both perform
considerably worse; surprisingly, both perform better on unigram
features, Jensen-Shannon so much so that it's the only feature set for
which it finds all significant distances. $R$, on the other hand,
performs decently on all combinations of parameter settings; its low
significance for phrase structure rules is shared by Kullback-Leibler
and Jensen-Shannon divergences.
% TODO : Maybe more on cosine later. Maybe not.

When comparing the performance of Kullback-Leibler and Jensen-Shannon
divergence it is not surprising that Jensen-Shannon outperforms
Kullback-Leibler on fixed-size sampling. Although both are called
``divergence'', Jensen-Shannon divergence is actually a
dissimilarity. Recall that the divergence from point A to B may differ
from the divergence from point B to A. A divergence like
Kullback-Leibler can be converted to a dissimilarity by measuring
$KL(A,B) + KL(B,A)$. However, this dissimilarity must skip features
unique to a single corpus in order to avoid division by zero. This
means that for smaller corpora Kullback-Leibler loses information that
Jensen-Shannon is able to use.  On the other hand, while this may
explain Kullback-Leibler's improved performance for full-corpus
comparisons, it doesn't explain Jensen-Shannon's much worse
performance.

\subsubsection{Significance by Feature Set}

% \item Unigrams do form an adequate baseline; they are bad but not too
%   bad.

% The feature sets most likely to find significance are the combined
% features and unigrams., in order,
% trigrams, all combined features and leaf-head paths (both with
% support-vector-machine training and with Timbl's instance-based
% training). Without ratio normalisation, the other feature sets are not
% much worse, but with it included, these three are the best by some
% distance.

For 1 round of normalisation, the best feature sets are the simple
ones: trigrams and unigrams, as well all combined features. On the
other hand, trigrams and leaf-head paths (with its variations) are the
best feature sets with 5 rounds of normalisation. However, the
variation isn't strong; any feature set can give good results with the
right distance measure. The problem is that no clear patterns emerge.

The relatively high quality of trigrams and unigrams does not make
sense given only the linguistic facts; however, it is likely that the
entirely automatic annotation used here introduces more and more
errors as more annotators run, operating on previous automatic
annotations. Trigrams are the result of only one automatic annotation,
and one for which the state of the art is near human performance. So
the fact that these particular parts of speech are of higher quality
than the corresponding dependencies or constituencies is probably the
deciding factor in their higher number of significant
distances.

% Although it is impossible to tell from my results, I
% predict that a manually annotated dialect corpus would show that
% non-flat syntactic structure is helpful in producing significant
% distances.

Given the above facts, the question should rather be: why do leaf-head
paths perform as well as they do? Better, for example, than the
leaf-ancestor paths on which they're modelled: why does more
normalisation hurt leaf-ancestor paths but not leaf-head paths?  It
could be that there is less room for error; many of the common
leaf-head paths are short: short interview sentences with simple
structure make for shorter leaf-head paths than leaf-ancestor
paths. As a result, the important leaf-head paths consist mainly of a
couple of parts-of-speech.

Another reason could be a difference in parsers: MaltParser has been
tested before with Swedish (CITE). Besides English, the Berkeley
parser has been tested prominently on German and Chinese. Therefore,
the difference would better be explained by appealing to the
difference in parsers rather than an unsuitability of Swedish for
constituent analysis.

It is disappointing linguistically that trigrams provide the most
reliable results so far; a linguist would expect that including
syntactic information would make it easier to measure the differences
between regions. If it is, as hypothesised here, an effect of chaining
machine annotators, a study using manually annotated corpora could
detect this. However, it still means that trigrams are the most useful
feature set from a practical view, because automatic trigram tagging
is very close to human performance with little training. That means
the only required human work is the transcription of interviews in
most cases.

On the other hand, if additional features sets are to be developed for
a corpus, then combining all available features seems to be a
successful strategy. The distance measures seem to be able to use all
available information for finding significant distances.

\subsection{Correlation}
% TODO: ALSO I should point out that geographic correlation IS the
% default expected by normal old boring linguists. The Nordic Languages
% is pretty clear on this plus I have some references from
% dialectology papers.

In dialectology, the default expectation for dialect distance is that
it correlates with geographic distance \cite{chambers98}. A lack of
correlation does not necessarily mean that a measure is useless, but
presence of correlation means that the distance measure substantiates the
well-known tendency of dialect distributions to be more or less
smoothly gradient over physical space.

In addition, the distance measures are more likely to correlate
significantly with travel distance than with straight-line geographic
distance. This makes sense since the difficulty of moving from place
to place is what influences dialect formation, and taking roads into
account is an improved estimate over straight-line distance.

As with the number of significant distances, trigrams and unigrams are
the most likely to to correlate with geographic and travel distance,
as well as the combined feature set for the 5-normalisation parameter
setting.
% As before, a possible explanation is that unigrams are
% simpler, so the type count is a higher than for other measures. With
% more rounds of normalisation, more correlations shift over to
% trigrams.
Note that in figures \ref{cor-1-1000} --
\ref{travel-cor-5-full}, the significant correlations are marked with
an asterisk, but only the italicised correlations are based on at
least 95\% significant distances. For example, this means that most of
the significant correlations based on phrase-structure rules are not valid.

It is worthwile noting, however, that the valid and significant
correlations based on phrase-structure grammars give the highest
correlations: 0.37 for $R^2$ with full-corpus comparisons and 1 round
of normalisation.
The addition of more data and more normalisation is interesting in
expanding the correlating parameter settings beyond those that include
unigram features. It may be that this is an instance of the noise/quality tradeoff.
These additions appear to extract more detail from
the data, at the cost of additional interference from noisy data.

% Goes here: Fevered speculation about why travel correlation is *better* with
% the methods that correlate *less*, for 1-full at least.
% OK never mind this isn't true.

\subsubsection{Inter-measure correlation}

In table \ref{self-correlation-measures}, $R$ and Jensen-Shannon
produce nearly identical results. Also, cosine similarity arrives
at different results than the other measures, though the correlation
is still higher than with travel distance.

\subsubsection{Correlation with Corpus Size}

The correlation of corpus size and dialect distance is a problem. It
is not a predicted as a side effect of the way dialect distance is
measured. The fact that travel distance also correlates with corpus
size at a rate of 0.32 confuses the issue further. Is corpus size the
determining variable? Or is there an unknown variable influencing all
three? Some possibilities are ``interviewer boundaries'', common in
corpora collected by multiple people \cite{chambers98}, or perhaps the
interviewers got better over time and collected longer interviews as
they moved throughout the country, or perhaps cultural differences
between the interviewer and interviewees caused some participants in
one area to talk more than in another area.

Definitely the high correlations between corpus size and the
5-normalisation distances are worrying. They are so much higher than
the correlation of corpus size and travel distance that 5-normalised
distances might not be reliable.
% It appears that multiple rounds of
% normalisation inadvertently re-introduce a dependency on size.
% TODO: This probably IS a bug in that only freq norm can be
% iterated. Ratio norm should probably be in a separate loop like so:
% #ifdef RATIO_NORM
%   for(sample::iterator i = ab.begin(); i!=ab.end(); i++) {
%     i->second.first *= 2 * types / tokens;
%     i->second.second *= 2 * types / tokens;
%   }
% #endif
However, if 5-normalisation introduces a dependency on corpus size,
then the distances from full-corpus comparisons should correlate even
more highly. This is not the case.

% TODO: I also should write this up when I have time
Alternatively, it is possible that the fixed-size sampling method is not doing its
job in eliminating size differences between corpora. Future work
should develop a method for normalising a comparison between two full
corpora. It should avoid sampling, but also take the relative number
of sentences into account. It is not difficult to come up
with a simple method to do so, but it needs some checking to make sure
that the method is valid.

\subsection{Cluster Dendrograms}

The cluster dendrograms in chapter \ref{results-chapter} are dangerous
to interpret too closely on their own; the instability of a single
dendrogram means that small clusters cannot be analysed reliably. For
example, in figure \ref{cluster-5-r-trigram}, a two-way split
between the regions on the top and bottom of the page is
obvious, and a three-way split is easy to argue for, but outliers like
Anundsj\"o and \.Arsunda are likely to shift from group to group in
other dendrograms.

It is safer to analyse the consensus trees; the smoothing effect of
taking the majority rule of each cluster will show where the optimal
cutoff for splitting clusters is. The three consensus trees in figures
\ref{consensus-1-1000} -- \ref{consensus-5-1000} vary in amount of
detail but share most details.

For 1000-sentence samples and 1 round of normalisation, there is one
cluster: Floby and Bengtsfors. Full-corpus comparison finds
another cluster: J\"amshog, \"Ossj\"o and Tors\.as. Finally,
1000-sentence samples and 5 rounds of normalisation finds another
cluster consisting of L\"oderup and Breds\"atra. It also finds
a large two-way split between the regions and adds Sproge to the first
cluster with Floby and Bengtsfors. To aid further analysis, the
clusters are assigned colours, which are detailed in figures
\ref{blue-cluster} -- \ref{orange-cluster}. 

\begin{figure}
\begin{itemize}
\item Floby
\item Bengtsfors
\item Sproge (for 1000-sample, 5-normalisation)
\end{itemize}
\caption{Blue Cluster}
\label{blue-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item J\"amsh\"og
\item Tors\.as
\item \"Ossj\"o
\end{itemize}
\caption{Red Cluster}
\label{red-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Breds\"atra
\item L\"oderup
\end{itemize}
\caption{Yellow Cluster}
\label{yellow-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Leksand
\item Indal
\item Segerstad
\item Floby
\item Bengtsfors
\item Sproge
\item Skinnskatteberg
\item Orust
\item V\.axtorp
\item F\.ar\"o
\item Asby
\item \.Arsunda
\item Anundsj\"o
\item Ankarsrum
\item Fole
\end{itemize}
\caption{Cyan Cluster}
\label{cyan-cluster}
\end{figure}

\begin{figure}
\begin{itemize}
\item Viby
\item Bara
\item S:t Anna
\item Frilles\.as
\item J\"amshog
\item Tors\.as
\item \"Ossj\"o
\item K\"ola
\item L\"oderup
\item Breds\"atra
\item Villberga
\item Tors\"o
\item Norra R\"orum
\item Sorunda
\item B\"oda
\end{itemize}
\caption{Orange Cluster}
\label{orange-cluster}
\end{figure}

When these clusters are mapped onto the geography of Sweden, some
patterns are visible. Since figure \ref{consensus-5-1000} is strictly
more complex than the preceding two, it is used as the basis for this
analysis--see map \ref{map-consensus-5-1000}. The large two-way split
is between the orange and cyan clusters. The orange cluster, which
includes red and yellow clusters, forms two horizontal bands across
Sweden. The centres of the orange cluster appear to be Stockholm and
Malm\"o. Meanwhile, the red and yellow clusters form a boundary along
the northern border of Sk\.ane and Blekinge counties.

Meanwhile, the cyan cluster, which includes the blue cluster, seems to
represent the countryside of Sweden. On the other hand, because the
blue cluster is near G\"oteborg, it might be better characterised
simply as ``non-Stockholm''.

Most of this analysis agrees with the existing dialectology
literature; the north-to-south gradient is well-attested in (CITE: The
Nordic Languages, plus I'm pretty sure I read this somewhere
else). In the south, it is also well-known that the boundary with
Sk\.ane is stronger; this boundary extends along Blekinge as well, it
seems. However, the difference between city and countryside is not
well attested in the literature, nor is the possible difference
between Stockholm/Malm\"o and G\"oteborg.

\subsection{Composite Cluster Maps}

Composite cluster maps use an underlying technique similar to consensus
trees--cluster dendrograms, but they combine and present the information in
a very different way. The result looks much more like the traditional
isogloss boundaries of dialectology. The composite cluster maps are in
maps\ref{map-composite-1-1000} -- \ref{map-composite-5-1000}.

All three composite clusters maps provide a picture similar to the
consensus tree map \ref{map-consensus-5-1000} of the previous
section. The north-to-south gradient is supported by the
weak horizontal boundaries present up and down Sweden.

Of these boundaries, the one between Sk\.ane and the rest of Sweden is
the strongest. Due to the lack of interview sites in the middle of
south Sweden, the boundary is drawn further north than it
traditionally appears, but this is an effect of the software the
produced the figure. Notice that there is also a boundary between
J\"amshog, Tors\.as, and \"Ossjo\" and the other sites, especially
visible in maps\ref{map-composite-1-1000} and
\ref{map-composite-5-1000}. Their presence along the northern border
of Sk\.ane is one reason why its boundary with the rest of Sweden is
so strong.

Compared to the consensus tree maps, the composite cluster maps cannot
support the city/country distinction because there is no way to
identify distant areas by their colour. On the other hand, it is
possible to detect the relative strength of a boundary. To combine
these two features, multi-dimensional scaling is needed.
% But of course MDS maps can't be combined into a consensus...

% However, K\"ola and Frilles\.as still separate fairly well from their
% neighbours. These sites are on the edges of the country and have strong borders
% with surrounding, Like the cluster J\"amshog, Tors\.as and \"Ossj\"o,
% these sites are different from the others. However, they don't have
% any geographic coherence, so it is more likely these are remnants of a
% dialect that was historically wider spread and has since receded.

\subsection{Multi-Dimensional Scaling}

Multi-dimensional scaling (MDS) operates similarly to cluster
dendrograms in that it reduces the high-dimensional distances to a
lower-dimensionality representation. It differs, however, in producing
gradient numbers, not binary trees. This means that the MDS maps
naturally have gradient borders. Also, because of the way that the
3-dimensional points map to colours, the maps vary. However, they are
still comparable: if two regions are blue in one map and both are
orange in another, then they have the same relation to each other.

Despite the differences between MDS and the preceding methods, the
similar results are evident; the maps (figures
\ref{mds-1-1000-r-trigram} -- \ref{mds-5-1000-js-trigram}) all show
the same patterns as the other methods. That is, there is a general
north-to-south gradience, especially easy to see in map
\ref{mds-1-1000-js-trigram}. There is a strong southern cluster,
visible in all of the diagrams. And there is a general two-way
distinction between city and country.

The main contribution that the MDS maps make is that the
north-to-south gradient is more obviously gradient. In other words, it
is easier to see the gradation from north to south. For example, in
figure \ref{mds-1-full-r_sq-psg}, looking from the north to south, the
colours change quickly close to Stockholm, then fade to green further
south, then transition back to blues and purples further south, in
Sk\.ane.

The Stockholm and Malm\"o areas, which are in the same cluster in the
consensus tree maps, are here seen to be similar without being
identical. For example, in figure \ref{mds-5-1000-js-trigram}, the
Stockholm area is a shade of blue-green while the Malm\"o area is a
shade of blue-grey. Also in figure \ref{mds-5-1000-js-trigram},
Sk\.ane and Blekinge are grey: clearly similar but not identical to
Malm\"o.

\subsection{Features}

%TODO: Put back in the freq/ratio features for $R$ instead of the
%overuse-ranked features. I guess overuse isn't that useful after all.

Next I rank and analyse the input features by comparing their
differences. The features analysed here are the ten highest ranked
features for a particular comparison, based on the consensus tree
clusters of figures \ref{consensus-1-1000} --
\ref{consensus-5-1000}. The clusters are labelled A to D from top to
bottom. The clusters are reproduced in table
\ref{feature-ranking-clusters-again}. Although each feature set has
ten features ranked here, it's actually two sets of five features,
because these are differences. The top five positive features are
shown as are the top five negative features.

\begin{table}
  \begin{enumerate}
   \item[A] (Blue) Floby, Bengtsfors
    \item[B] (Red) J\"amshog, \"Ossj\"o, Tors\.as
    \item[C] (Yellow) L\"oderup, Breds\"atra
    \item[D] (Cyan) Segerstad, K\"ola, S:t Anna, Sorunda, Norra Rorum,
      Villberga, Torso, Boda, Frilles\.as, Indal, Leksand, Anundsj\"o,
      \.Arsunda, Asby, Orust, V\.axtorp, Fola, Sproge, F\.ar\"o,
      Ankarsrum, Skinnskatteberg
  \end{enumerate}
  \caption{Clusters discussed}
  \label{feature-ranking-clusters-again}
\end{table}

This has two advantages. It splits the features so that both the
positive and negative evidence are always visible; otherwise, in some
cases, if one side is strong enough, the other would be pushed out of
the top ten. However, it still allows the relative weight of evidence
to be estimated. For example, if some cluster has some idiosyncratic
features, most of the features will be positive, meaning that most of
the distance comes from features typical of this cluster. The two-part
feature will show this: the five positive features will have much
higher values than the five negative features.

For feature ranking, there is an additonal normalisation used
called overuse normalisation, which ranks rarely used features more
highly.  This can be useful; without it, the top-ranked features will
tend to be the most common ones, those found in almost every sentence
in the interview. These common features tend to highlight
gradient differences: differences in quantity but not in quality. In contrast,
the overuse normalisation allows us to see which features happen only
a few times in one side of the comparison and not at all in the
other. This is closer to a traditional linguistic analysis.

\subsubsection{Analysis}

The analysis will start with trigram features without the overuse
normalisation, since trigrams have the highest rate of significance of
the non-combined feature sets. (The combined feature set is difficult
to read because of the mixing of feature types.)

As mentioned above, the top-ranked trigrams are common, typical
of the core of the sentence. Cluster A's typical trigrams, for
example, typically involve a pronoun or a verb or both: PO-AV-AB
(pronoun-copula-adverb), $++$-PO-AV (conjunction-pronoun-copula) and
PO-VV-AB (pronoun-verb-adverb). The same is of the
other clusters for the most part. Unfortunately, this makes it say
interesting things about the difference in feature distribution. It
does appear that clusters B and C have heavier use of adverbs and of
conjunctions. The comparison between cluster A and cluster B even
highlights the trigram AB-AB-AB as important.

Given this lack of information, there are two dimensions along which
the comparisons can be altered: normalisation and feature
set. Starting with normalisation, let us add the overuse normalisation
technique. Differences appear immediately. First, the balance of
feature weight obviously differs here. For example, in the comparison
between cluster A and cluster B, the features of cluster A are more
important in distinguishing the two than the features of cluster
B. The comparison between cluster A and cluster D is so lop-sided that
cluster D contributes no features at all.

With the overuse normalisation, cluster A has two interesting
patterns. First, the trigrams it overuses are filled with indefinite
articles (EN) and prepositions (PR). Examples include VV-EN-AB
(verb-indefinite-adverb), PR-EN-AB (preposition-indefinite-adverb) and
PR-EN-VN (preposition-indefinite-verbal noun), as well as IM-PR-NN
(infinitive marker-preposition-noun) and PR-ID-PR
(preposition-idiom-preposition). Second, the trigrams it underuses
mostly end with pronouns: 4 of 5 trigrams in the comparison with
cluster B and 4 or 5 in the comparison with cluster C. Even in the
comparison with cluster D, 4 of 5 of the ``least overused'' trigrams
end with pronouns. (The low values in the bottom half of the
comparison with cluster D are not underused by cluster A, because
cluster D has no unique features here. Instead they are the ``least
overused'' by cluster A.)

Cluster B shows one interesting pattern: overuse of sk\"ola (shall),
including an interesting trigram SV-QV-AB (shall verb-can
verb-adverb). Although this could be a mistake on the part of the
tagger, the different forms of this verb are limited, so this is
unlikely: identifying them is not hard. Instead it points to the
possibility of double modals.
%% a quick search suggests that Fennell and Butters (1996) finds
%% evidence in German and Scandinavian languages...but it's a book ro
%% something. Google Scholar has no link, just a wimpy citation.
%% Also:
%% Modals and double modals in the Scandinavian languages
%% Working papers in Scandinavian syntax
%% Thrainsson and Vikner 1995 (but focussing on Danish and Icelandic)

Cluster C doesn't gain any interesting patterns with overuse
normalisation except for a surprising variety in the verbs: g\"ora
(do), hava (have), kunna (get), sk\"ola (shall), vara (be) and vilja
(want). Many uses of adverbs show up as well. It is not clear what
either of these patterns mean linguistically, however.
% I have no idea whether to make that verb singular or plural.
% So like whatever.

Cluster D gives no information whatsoever when the overuse
normalisation is added, simply because it has no informative
features. This is expected, given its nature as a combination of many
sites. The tradeoff of more informative features for the smaller
clusters is worthwhile.

Moving to other features sets with overuse normalisation,
leaf-ancestor paths and leaf-head paths give additional information
about cluster A that lead to the conclusion its defining
characteristic is simple sentences, simpler at least than the other
clusters. Specifically, cluster A's overused leaf-ancestor paths
include few nested sentences. This contrasts sharply with cluster B
and cluster C, which include many nested sentences. Cluster A does
have complex paths, but they feature prepositional phrases. (Note: NAC
stands for ``not a constituent'' and indicates that the parser could
not decide what the correct constituent was at that point.) (Or that
there are crossing branches, which is less common.)

This characteristic of cluster A appears in the leaf-head paths as
well; cluster A's paths contain many [adjective]-noun-preposition
sequences, but few verb-verb sequences that indicate nested
phrases. Again, cluster B and cluster C have many of these
sequences. Both clusters have a number of overused adverb features as
well, similar to the trigram results. Note that comparison to clsuter
D is less interesting. Because it has fewer unique characteristics,
when compared to it, clusters A, B and C show more generic
characteristics. For example, all three clusters show that their
sentences are generally more complex than the general sites in cluster D.

Analysis of the phrase-structure-rule features is difficult because of
all the noise. Features like S$\to++$-AB (conjunction-adverb)
S$\to$FV-PO-AB-VV (get verb-pronoun-adverb-verb) are hard to describe
as anything but junk rules created by the parser. On the other hand,
there are a lot of linguistically odd but reasonable rules like
S$\to$PO-AV-NP-IP (pronoun-copula-noun phrase-period), which makes a
certain kind of sense if you can be persuaded that copular sentences
are special enough to deserve their own rule. (Remember that
statistical parsers trained on interview data are particularly
susceptible to this kind of persuasion.)

Overall both normalisations leave something to be desired; without
overuse normalisation, only very common features appear. These
features convey only basic information, making it hard to identify
characteristics of a cluster. On the other hand, the overuse
normalisation is susceptible to noise, especially for more error-prone
feature sets. Even though more detail may be available with this
normalisation step, the features must be inspected for general trends
because individual features are not necessarily reliable.

\section{Comparison to Syntactic Dialectology}

The literature for Swedish syntactic dialectology is not large;
although it is not large anywhere, Swedish dialectology is not a very
large field. I will compare my results to two papers,
\namecite{delsing03} and \namecite{rosenkvist07}. The first paper is a
survey of syntactic dialectology from the late 19th and early 20th
centuries. In the same volume, other papers analyse specific phenomena
in more detail; the survey is mostly concerned with the differences
and distributions rather than the syntactic analysis. The second
is an analysis of the South Swedish Apparent Cleft.

\subsection{Delsing's Survey of the Norse Nominal Phrase}

\namecite{delsing03} surveys a number of dialectology studies. These
studies date from the height of the field in Sweden, from circa
1880--1930, which Delsing at times augments with modern data. It is
worth noting that the Swedia data in the comparison was collected
around 2000, so there were likely changes in the dialects in the
intervening 70--120 years. This is particularly true in the northern
dialect areas, where improved travel and communication will have added
a centralising effect.

% The gerunds in this paragraph suck and turn it into a
% single-sentence thing.
However, the eight phenomena in the survey are too useful to ignore, so
with that caveat in mind, I will investigate each in turn, starting
with a summary of the phenomenon for Swedish dialects, then
identifying the relevant interview sites. Next I represent the
phenomenon in terms of the feature sets used throughout this
dissertation. This is somewhat difficult; the features are mostly designed
for ease of automatic tagging and extraction. When they capture
syntactic differences between two regions, they do so in a way that
can be hard to translate to linguistic analyses. (NOTE: Trigrams are
used therefore because they are least simple, and also their tagger is
more reliable)

With the target sites and features defined, it is straightforward to count the
number of occurrences of each feature in each site and compare the
two. If the predicted dialect phenomenon is reflected in the data,
then the sites associated with the phenomenon will have more
occurrences of the target features than the non-associated sites. This
difference is precisely what the distance measures use.

This method is inadequate for two reasons: first, the translation of
linguistic analysis to feature representation will not be perfect and
may miss some valid instances of the linguistic phenomenon. Second,
more importantly, the differences are not yet checked for statistical
significance. As such, the comparison can only be suggestive;
checking for statistical significance will have to wait for future
work.

% As an aside, much of this missing information IS available to me, so I
% could look manually. But none of it made it through to the distance
% measures, and this analysis compares the way the distance measures
% make the decisions with the way that linguists make their
% decisions. So I have to use only the information that the distance
% measures used.

\subsubsection{``Partitive'' Article}

Northern Sweden uses the suffixed article much more than the rest of
Sweden. Delsing says the reason is that some uses of the suffix
article are not definite in the north; they have a partitive function,
similar to the partitive article in French, which is not marked in the
rest of the country. (There is also a use of the suffixed article in
predicative constructions, but this is probably not related). See
figure \ref{partitive-article} for an example.


\begin{figure}
  {\it H\"a finns vattne d\"ari hinken.} \\
  Here found the-water in the-bucket \\
  There is water in the bucket. \\
  \caption{Suffix marking for partitive}
  \label{partitive-article}
\end{figure}

Unfortunately, the part-of-speech tag set used for this
dissertation does not record whether nouns are marked with the
definite suffix. Therefore, there is no way to tell the difference
between suffixed dialect usage and bare standard usage. This feature
must be skipped because it cannot be compared.

\subsubsection{Proper-Noun Articles}

In Northern Scandinavia, first names are preceded by an indefinite
article, and sometimes last names as well. It also includes the
kinship terms that are used as proper names. Standard Swedish does not
include this feature. In Sweden, this feature is found along the border
with Norway as well as Northern Sweden. In our data, this includes the
interview sites K\"ola, Indal, and Anundsj\"o. An example is given in
figure \ref{indefinite-article-proper-noun}

\begin{figure}
  \includegraphics[scale=0.7]{dialektboka-karta3}
  \caption{Proper-Noun Articles}
  \label{indefinite-article-proper-noun-map}
\end{figure}

\begin{figure}
  {\it En Bjurstr\"om ha aff\"arn.} \\
  A Bjurstr\"om has the-store. \\
  Bjurstr\"om has a store. \\
  \caption{Indefinite Article for Proper Nouns: First Names}
  \label{indefinite-article-proper-noun}
\end{figure}

Unlike the partitive article suffix, this feature is easy to detect
with simple features. Specifically, it can be represented as the
bigram EN-PN (indefinite article-proper noun), which can be used as a
search term in the trigram feature set. The same EN-PN sequence is
expected for leaf-head paths, since the indefinite article depends on
proper noun. The phrase-structure-rule features should
look something like NP$\to$EN-PN.

Occurrences of the EN-PN bigram in the trigram feature set for
Leksand, Indal and K\"ola agree with the linguistic analysis: a rate
of 0.00007 versus 0.00006. Unfortunately, this result can hardly be
trusted because the rate of occurrence for both regions is so rare, as
well as so close between the two regions. The only conclusion that can
be drawn is that the hypothesis is not yet disproven.

\subsubsection{Possessives and the article}

In Swedish, and in the other Scandinavian countries, there is a good
deal of variation in the handling of possessives with articles. In
Swedish, normally only one is allowed in a noun phrase: either a
possessive or a determiner, but not both. However, in Danish and the
Danish-influenced areas of Sweden, both are allowed in certain
cases. When the possessive and determiner are separated from the noun
by an adjective, both are allowed. Delsing gives an example from
Danish, copied here in figure \ref{possessive-plus-article-example}.
This pattern is also standard in the southwest corner of Sweden, very
near to Denmark. This includes the interview site Bara. In addition,
it alternates with the standard on the island of Gotland, which
includes the interview sites Fole, F\.ar\"o and Sproge.

\begin{figure}
  \includegraphics[scale=0.7]{dialektboka-karta4}
  \caption{Proper-Noun Articles}
  \label{possessive-plus-article-map}
\end{figure}

\begin{figure}
  {\it naboens den stribede kat} \\
  Neighbours' the striped kat \\
  The neighbours' striped cat.
  \caption{Simultaneous possessive and determiner in noun phrase in
    Danish}
  \label{possessive-plus-article-example}
\end{figure}

This pattern can be detected by looking for the
4-grams PO-PO-AJ-NN, PR-PO-AJ-NN and NN-PO-AJ-NN. The first is the
sequence pronoun-pronoun-adjective-noun, for example {\it mitt det
  gamla huset} ``My the old house-the''. The second starts with a
proper name, such as {\it Pers} ``Per's'', and the third starts with a
noun, such as {\it naboen} ``neighbour's''. These three 4-tag sequence
can be encoded as trigrams by breaking them into two pieces.

In addition to this pattern, there is a second in the north of
Sweden. Here, it is simply that possessive personal pronouns are
allowed both before and after the noun. This pattern includes the
interview sites Indal and Anundsj\"o and is covered in the next
section.

Searching Bara, in the southwest of Sweden, for the previously
mentioned trigram patterns does not find them: the rate of occurrence
is 0.00289 inside Bara but 0.000341 outside. It should be higher in
Bara. However, Delsing also mentions that Scanians he has asked do not
recognise this form either, so it is possible that it has fallen out
of use in the 70 years or so since it was last reported.

Executing a similar search for the Gotland sites (F\.ar\"o, Fole and
Sproge), but with the addition of the standard trigrams PO-AJ-NN,
PR-AJ-NN and NN-AJ-NN, shows similar results: 0.00441 on Gotland,
0.00495 off.

The final region in map \ref{possessive-plus-article-map}, in northern
Sweden, which includes Indal and Andundsj\"o, is actually more
complicated than can be captured by the part-of-speech tags used here;
this region allows possessive proper nouns to occur with
suffix-determiner nouns. But this can occur in either order: for
example, both ``Pers huset'' and ``huset Pers'' is allowed. Although
both ``Pers hus'' and ``Pers huset'' produce identical tags (PR-NN),
trigrams encode order, so the unusual order in ``huset Pers'' can be
searched for.

Searching for the bigrams NN-PN (noun-proper noun) and NN-PO
(noun-pronoun) shows a usage rate of 0.02532 for Indal and Anundsj\"o
and a rate of 0.02438 for the rest of Sweden. This is the expected
direction, but the rate of usage is very similar between the two
regions. The comparison is really too close to make a prediction
because the difference is not likely to be significant.

% \subsubsection{Pronominal Possessives}

% In Swedish, as well all of mainland Scandinavian, another possessive
% construction is the reflexive genitive, which consists of a
% noun-reflexive-noun sequence. An example is given in figure
% \ref{genitive-reflexive-normal-example}. However, this construction
% does not allow pronouns: the sequence noun-reflexive-pronoun is not
% allowed (see figure \ref{genitive-reflexive-pronoun-example}).

% \begin{figure}
%   {\it Per sitt hus} \\
%   Per its house
% ``Per's house'' \\
% \caption{Standard Swedish genitive reflexive construction}
% \label{genitive-reflexive-normal-example}
% \end{figure}

% \begin{figure}
%   {\it han sitt hus} \\
%   his its house
% ``his house'' \\
% \caption{Pronominal genitive reflexive construction}
% \label{genitive-reflexive-pronoun-example}
% \end{figure}

% However, this construction is allowed in NORTHERN SWEDEN.
% Oops, actually I think this whole section is whole throwaway intro to
% something else. Boooooo.

% However, this is not allowed with possessive pronouns:
% *{\it han sitt hus}.

% The prepositional genitive
% behaves the same way: {\it huset till Per} ``Per's
% house'' (gloss: house-the of Per) is legal but *{\it huset till meg}
% ``my house'' (gloss: house-the of me) is not.

% There is an exception for kinship words, which I don't understand
% yet. But somehow ``far min'' is different (maybe just because it's not
% ``min far''?)

% So basically standard Swedish allows trigrams sequences like NN-PO-NN
% ({\it Per sitt hus}) but not PO-PO-NN ({\it han sitt hus}). It also
% allows sequence like NN-PR-NN ({\it huset till Per}) but not NN-PR-PO
% ({\it huset till meg}).

% Does not work (is too close to call): 0.02229 vs 0.2429

% Reversing the bigram, looking for PO-NN in the south gives
% Works (but is still super close): 0.04243 vs 0.03998

% It looks like one set just uses more nouns than the others or
% something. Conclusion: inconclusive, leaning toward no---it looks like
% they're the same.

\subsubsection{Prepropriell Possessives}

Redundent post-nominal possessive pronoun for post-nominal proper-noun
possessors. That is, {\it Huset hans Per} ``The house his Per''.

This overlaps partly with possessives and the article; the nothern
section is alternative 3 there, which was found not to work.

There is some overlap into Sweden from Norway of this pattern. Of the
interview regions, it covers K\"ola.

This predicts K\"ola to have more trigrams of the type NN-PO-PN, as
opposed to NN-PN elsewhere, or PN-NN in the north. Note that K\"ola is
pretty iffy--it's right on the border, but Delsing's map is hard to
interpret (it is distorted in a different way than most maps of
Sweden), so it's quite possible that it isn't predicted to differ anyway.

Does not work: 0 vs 0.00001

So it doesn't occur anywhere, save for some noise. Conclusion:
inconclusive, leaning toward no---it could be a lack of data or more
likely neither has this construction.

\subsubsection{Noun possessives}

Specifically s-genitives in the dative case. I'm pretty sure my wimpy
part-of-speech distinction can't catch this, and the grammar is the
same as without the dative case, so this is impossible. Oh well.

\subsubsection{Post-Adjective articles}

Aka double indefinites.

An indefinite article appears after adjectives that modify an
already-indefinite noun. For example: {\it en stor en bil} ``A large
car'' (gloss: a large a car). In a trigram feature set, this predicts
trigrams like EN-AJ-EN and AJ-EN-NN. It predicts very confused
dependencies like, uh, EN-EN-AJ-NN or double EN-AJ-NN or
EN-AJ-EN-NN. All of which are pretty weird, I'm sure you'll
agree. (Syntactically, it looks like it's duplication, like ``A large
one, car''. But what do I know, I'm not a syntactician and I don't
know Swedish. I'm just guessing at what Delsing is implying)

This occurs in the north, which predicts that Anundsj\"o and Indal
will follow this pattern.

Rousing success! 0.00054 vs 0.00012. Rare but seriously, it actually
occurs. 7 times in Indal (out of about 10000 trigrams). 4 times in
Leksand (out of under 9000 trigrams). In other regions, V\.axtorp had
3 occurrences. B\"oda had 4. Fole 3/Asby 3

Note: it would be interesting to get the icefeat ranking for this trigram.

\subsubsection{Double-definite}

Double-definite with adjectives is standard in Sweden and Norway, but
not in Denmark, where the definite suffix disappears and not in
Iceland, where the definite article is not allowed. However, in North
Sweden, there is a weirdo combined thing where the adjective is
combined with the noun instead. So, the adjective is marked, unlike
Iceland, by combining. But the dialect refuses to repurpose `det' as
an article of sorts.

Unfortunately, I have no way to differentiate this from normal usage
except to look for FEWER trigrams like PO-AJ-NN, because they would be
realised without the PO and with the AJ combined, then tagged
(hopefully) as NN because of the -et suffix and TnT's amazing suffix
tagging mechanism. So, uh...that's what I'll do. The relevant sites
are Leksand, Indal and Anundsjo.

Works: 0.00167 vs 0.00217. Not a smashing success, but it could be
significant. Who knows. It's a test, and it passed.

\subsection{Rosenkvist's Analysis of the South Swedish Apparent Cleft}

Rosenkvist's analysis applies to Sk\.ane, at least according to his
reference to ``Scanian linguists''. Oh wait, section 4 covers his
attempts to pin it down. He finds it is Svealand and G\"otaland, ie
southern and middle Sweden. This means not Arsunda, Indal and
Anundsjo.
OTOH, his survey points out only Halland,
Sm\.aland and Sk\.ane (Frillesas, Vaxtorp; Ankarsrum, Torsas; Bara,
Loderup, Norra Rorum, Ossjo).

Sequences like ``som du g\"or dig till''. Rosenkvist mentions
searching for the sequence ``det \"ar bara som'' in his novel
corpus. The subject must be a pronoun. ``som'' is either UK
(subordinating conjuction) or PO. I think here it is UK. So bigrams
like UK-PO.

Except the difference between the {\it apparent} cleft and a real
cleft (or whatever it's called) is that apparent cleft haven't got any
antecedent. And since the antecedent usually comes immediately before,
the easiest way to search for an {\it apparent} cleft instead of a
real cleft is to search for XX-UK-PO, where XX is something that can't
be an antecedent. Like AB I guess, although it would be a lot smarter
to search for all trigrams that end with the bigram UK-PO and whose
first part is not NN or or other hilarious nominal like PO, PN (or
seriously, PR would be wrong here too...). All of Rosenkvists's
examples use VV-UK-PO.

The problem with this construction is that it's hard to detect with
trigrams: there are two similar constructions that Rosenvkist mentions
that look nearly the same on the surface: true clefts and comparative
clauses. A leaf-ancestor feature set would instead look for something
with {\it som} as the subordinating conjunction. Unfortunately,
Rosenkvist does not deal with the internal structure and I don't know
syntax, much less Swedish syntax, so I am lost. I'll just do a
simplistic search like he did.

Wide area, restrictive search:

Works: 0.00512 vs 0.00542. But it's really close.

Wide area, wide search:

Works: 0.00905 vs 0.01042. But it's still too close to call.

Narrow area, restrictive search:

Doesn't work: 0.00550 vs 0.00508. Very close again.

Narrow area, wide search:

Doesn't work: 0.00962 vs 0.0151. Super close.

Conclusion: doesn't seem to be a difference, but there are a number of
factors interfering with this. (1) Hard to capture this precise
syntactic phenomenon using trigrams (2) Lack of
precise understanding of the syntactic phenonemon (3) Lack of
significance testing (or any sound method really).
If anything, there is slightly more support for the widespread
use. But surely nothing significant.

\subsection{Conclusions}

There a bunch of small differences. It's possible that all these small
differences add up to produce the different regions. (And actually
very likely.) But they are
still not very interpretable.

\section{Comparison to Phonological Dialectometry}

Come to think of it, this would be better after syntactic
dialectometry. Either way, the basic conclusion is that the
north-to-south thing is the same (and the Scanian difference), but the
cities is new. And that a quantitative study would be amazing but
neither of us had time.

\section{Comparison to Syntactic Dialectometry}

In previous work on British English, this method failed to find
agreement between syntactic distance ($R$) and phonological distance
(Levenshtein) distance---there was no
significant correlation between the two methods. Although both showed
something like a North/South distinction in Britain, its orientation was much more
obvious from phonological distance. This lack of agreement was a
preliminary answer to the question of whether multiple
ways of measuring linguistic distance give the same results.

However, there were at least five reasonable explanations for the difference
between the two distance measures.
% First, and
% least satisfying, is the possibility that one of the distances is not
% measuring what it is supposed to. Second, the corpora may not agree
% because of the 40 year difference in age and differing collection
% methodologies. Third, syntactic and phonological dialect markers may
% not share the same boundaries.

\begin{enumerate}
\item One or both of the distances does not measure what it is supposed to.
\item The two corpora may not agree on dialect boundaries because of
  their 40-year difference in age.
\item Place of birth, as recorded in the ICE, may not correlate well
  with spoken dialect, especially given variations in speaker
  education level and place of residence.
\item Dialect boundaries may appear from systematic variation in
  annotation practices rather than the speech.
\item Syntactic and phonological dialect boundaries may be different.
\end{enumerate}

Of these, this dissertation addresses the second, third and fourth
problems directly by using a single corpus, Swedia2000, annotated by a single
person. (TODO: This may not be true for the phonological annotation.)
By finding significant distances between all interview sites of
Swedia2000, it also suggests that $R$ is measuring syntax
distance. PROBABLY.

The last is the most interesting because previous work will not have
exposed this difference. Traditional dialectometry focuses on a strong
agreement among a few features from each collection site. Because
syntactic features are fewer in number than phonological ones, they
are under-represented in this type of analysis. Unfortunately, this
means that the syntactic contribution to isogloss bundles is
correspondingly reduced. In addition, because of isogloss bundles'
insensitivity to rare variations, syntactic features rarely contribute
to isogloss bundles of successful dialect boundaries.
% I really need to CITE this.

In contrast, computational analysis, such as \cite{shackleton07},
captures feature variation precisely using statistical analysis and
sophisticated algorithms. The resulting analysis displays dialects as
gradient phenomena, displaying much more complexity than the
corresponding isogloss analysis. But current specialized computational
methods only apply to phonology. Syntactic data cannot be analyzed
without a syntax-specific method.

This paper attempts to address that lack, and provide some first steps
to show whether syntax and phonology assist each other in establishing
dialects, or whether their dialect regions are unrelated. If they are not
related, and syntactic gradients can be as weak as phonological ones,
then some new dialect regions may become apparent that were not visible in
previous phonology-only analyses.

\subsection{Improvements on British Dialect Experiment}

This dissertation improves on the British experiment in a number of
ways. It addresses the obvious criticism that syntax distance on the
ICE requires so much data that the results are no more informative
syntax those of traditional dialectology---its precision lags
phonological distance methods badly. However, $R$ works with much
smaller corpora when run on Swedia2000. This shows that the problem
with the British experiment is not the distance method, but the
corpus, which fails to capture dialect differences. Most likely is
that the interviewees, mostly in a college setting, actively tried to
suppress dialect differences during the interview.

Another problem with the current study is the 40-year difference in
collection dates between the phonological corpus and the syntactic
corpus. A recent phonological corpus would likely show the same sort
of changes in the North/South divide that show up in the syntactic
corpus. The British population became more mobile during the second
half of the 20th century, and the SED survey explicitly attempted to capture
the dialects that existed before this happened \cite{orton78}.
It would also be nice to have data from the rest of the United Kingdom for
comparison as well, or at least Scotland and Wales as with the ICE.


% TODO: integrate this.
% Alternatively, I could just look at the region pairs that fail to
% achieve significance in the syntactic permutation test and check to
% see if their phonological distance is lower than the other pairs. I
% don't do this (yet).

One interesting question is
how phonological and syntactic distances correlate with geographic
distance---\namecite{gooskens04a} shows that often the correlation is
very good. This would also allow better visualization of dialect areas
than a hierarchical dendrogram.

\section{Future Work}

Try all those smarter variants of $R$.

Include the rest of Nodalida once it is done.

Better normalisation and feature ranking are needed. It appears that
the current normalisations vary either in favouring differences
only in high-frequency features OR in favouring rare features so much
that the most important appear to be those that only occur in one of
the two regions.

\section{The End}

This dissertation contributes a better understanding of syntax with
respect to dialectometry. It establishes that statistical methods can
find interesting things, and with not much more data than is expected
of previous dialectometry in other areas. Remember, the majoriy of
interviews used here were less than 1000 sentences. Previous work
pointed the way (Nerbonne \& Wiersma (2006) and Sanders (2008)) but
failed to establish the utility and reliability of these methods
either by lack of dialect application or by lack of consistent
results. This dissertation addresses these shortcomings
comprehensively.

In addition, it points the way toward future work in Swedish; while
the results here are interesting, it is difficult to corraborate them
solidly because of the lack of study on Swedish dialects, both in
dialectology and dialectometry. This gap in the literature is on its
way to being remedied with the work of Leinonen in dialectometry and
X,Y,Z in dialectology.

Like its findings, future directions based on this work are
twofold. In general dialectometry, syntactic investigations should
begin, hopefully extending to languages for which the syntactic
variation is already well-studied.

In Swedish, I hope that this investigation of syntactic dialect
variation will lead to further work in this area; there is little
enough right now, and perhaps a computer-generated overview of the
interesting features will spark some new avenues of investigation for
linguists.

%%%%%%%%% cut two (raw) %%%%%%%%%

This dissertation establishes that statistical methods for syntactic
dialectometry can be useful. The results show that significant
distances can be obtained, which was shown by earlier work and not
much else. They show that dialect distances correlate with geographic
distance for many parameter settings. Clustering dialect distances
reproduces two well-known aspects of Swedish dialects: the Scanian
border, and a north-to-south gradient, as well as a previously unremarked
aspect: differences between the cities and the
countryside. Mult-dimensional scaling produces the same
results. Finally, interesting features have been discovered from these
same clusters, but features in the dialectology literature do not
necessarily appear.

In summary, this dissertation has answered its questions with some
degree of accuracy. Although the more complete verification is still
missing, it points the way for practical studies in the future: many
parameter variations are explored and the most efficacious are pointed
out.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
