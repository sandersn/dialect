% \usepackage{tipa}
% \usepackage{graphicx}
% \newcommand{\bibcorporate}[1]{#1} % stolen from Erik Meijer's apacite.tex
\documentclass[11pt]{article}
\usepackage{setspace}

% \usepackage{acl07}
\usepackage{robbib}
\title{Comparison of Phonological and Syntactic Distance Measures}
\author{Nathan Sanders}
\begin{document}
\maketitle
%\doublespacing
\section{Introduction}

I applied existing dialectometric techniques to the English of Britain
and introduced new techniques for measuring both phonological and
syntactic distance. Comparisons between distance have become rarer
since computational linguistics has taken over dialectometry. I also
compare the results of the two, although we will see that there is a
reason comparisons are rare; they are not very informative.

The dominant phonological distance measure in computational
dialectometry is Levenshtein distance or a variant
thereof. Statistical methods are slowly being introduced, such as the
information measure of \cite{hinrichs07} and \cite{sanders08}.
I developed a statistical method that uses a naive Bayes
classifier, similar to language identification methods developed in
the early 90s.

Syntactic distance has not been measured well in computational
dialectometry. Nerbonne and Wiersma have measured syntactic distance
statistically using a permutation test, with trigram features designed
to capture at least surface syntax. I tried the same test with
leaf-ancestor paths as features.

\section{Background}
\subsection{S\'eguy}
Measurement of linguistic similarity has always been a part of
linguistics from its beginning, with various degrees of
respectability. Until 1973, when S\'eguy dubbed a new set of
approaches `dialectometry', these methods lagged behind the rest of
linguistics in formalisation. However, S\'eguy's quantitative analysis
of Gascogne French, while not aided by computer, was the predecessor
of more powerful statistical methods that essentially required the use
of computer as well as establishing the field's general dependence on
well-crafted dialect surveys that divide the incoming data along
traditional linguistic boundaries: phonology, morphology, syntax, etc.
This makes both collection and analysis easier, although it requires
much more work when trying to produce a complete picture of dialect
variation.

The ALF project, which S\'eguy directed, begin in 195? to collect data
in a dialect survey which asked speakers from the towns of Gascogne
questions informed by different areas of linguistics. For our
purposes, the most interesting are the phonological data: these were
individual words designed to capture the already-studied variation in
the area in order to characterise its extent precisely.

The atlases, as eventually published, contained not only annotated
maps, but some analysis as well. This analysis was what S\'eguy named
the dialectrometry. It was not primarily generated by an
algorithm executing on a computer, but by linguists
counting the collected data by hand. The analysis is accordingly
linguistically sophisticated and numerically naive: isoglosses were
posited from observations of type counts along with their arrangement
on the map.

\subsection{Gersic}
At or just before the same time as the Atlas was finally published in
1973, Gersic proposed a computational method for measuring distance
between to phonological segments. Segments are specified as a vector
of features---consonants have
one feature set, while vowels have another. Features are given numeric
values. For example, a [+voice] segment will have $1$ in the third
position of the feature vector, while a [-voice] segment will have
$0$. Gersic then gives a
function $d$ that gives the distance between segments:
\[ d(i,j) = \sum_{k=1} |a_{i_k} - a_{j_k}|\] Where $a_i$ is the first
segment
being compared and $a_j$ the second. However, the equation is easier
to understand if summarised by the English prose ``count the number of
features with values that differ between segments''. Gersic did not
provide a way to compare vowels with consonants, or a way to combine
the segment distances into word distances, but those techniques would
be developed later.

\subsection{Goebl}

Later, Hans Goebl emerged as a leader in the field of dialectometry,
formalising the aims and methods of dialectometry. His primary
contribution was development of various methods to combine individual
distances into global distances and from there global clusters. These
methods were more sophisticated mathematically than previous
dialectometry, and operated with any features extracted from the data. His
analyses have mostly used the Atlas Linguistique de Francais
(ALF).

In Goebl (2006), he provides a summary of his work, including
definitions of the measures Relative and Weighted Identity
Value. These measures are independent of the source, capable of
extracting distances from any set of features, which need not be
specified a priori, unlike Gersic's method. For example, Relative
Identity Value, when comparing any two segments, counts the number of
features which share the same value, and divides by the number of
shared features. This has the advantage that it is robust in the fact
of (1) unshared features and (2) missing data. The result is a single
percentage that indicates the relative similarity. Calculating this
distance between all pairs of things to be compared produces a matrix
which can be used for clustering or other purposes. The equation is
actually:

\begin{equation}
  \frac{\textrm{identical}_{jk}} {\textrm{identical}_{jk} - \textrm{unidentical}_{jk}}
\label{riv}
\end{equation}
For some items being compared $j$ and $k$. In this case
\textit{identical} is
\begin{equation}
  \textrm{identical}_{jk} = |f \in \textrm{\~N}_{jk} : f_j = f_k|
\end{equation}
where $i$ is each feature shared by $j$ and $k$ (called
$\textrm{\~N}_{jk}$). \textit{unidentical} is defined similarly, except
that it looks at all features, not just the shared features.
\begin{equation}
  \textrm{unidentical}_{jk} = |f \in \textrm{N} : f_j \neq f_k|
\end{equation}

Additionally, Goebl
describes a refinement of the method called Weighted Identity
Value. This measure stems from the idea that some differences are
more important than others, in particular that more information arises
from feature values that only happen a few times rather than those
values that characterise a large number of the items being studied.
This idea shows up later in the permutation tests used by Nerbonne and Wiersma.

The mathematical implementation of this idea is fairly simple. Goebl
is interested in feature values that occur only a few times. This is
because, if a feature has some value shared by all of the items being
compared, then it provides {\it no} useful information for
distinguishing the items---they all belong to the same group.
The situation improves if all but one item
share the same value for a feature; at least there are now two
groups, although the larger group is still not very informative.
The most information is available if each item being studied
has a different value for a feature; the items fall trivially into
singleton groups, one per item.

Equation \ref{wiv-ident} works by discounting (lowering)
the \textit{identical} count from equation \ref{riv} by
the amount of information that feature value conveys. The
amount of information, as discussed above, is based on the number of
items that share a particular value for a feature. If
the worst case, in which all items share the same value for some
feature, then that feature is useless, so \textit{identical} should be
discounted all the way to zero. The equation for \textit{identical} now becomes
\begin{equation}
  \textrm{identical} = \sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_{j}}{(Ni)w} & \textrm{if} f_j = f_k
  \end{array} \right.
\label{wiv-ident}
\end{equation}

\begin{equation} \sum_i \frac{\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
\end{array} \right.}
  {\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
    \end{array} \right. - |f \in \textrm{N} : f_j \neq f_k|}
  \label{wiv-full}
  \end{equation}

  where $\textrm{agree}f_{j}$ is the number of candidates that agree
  with item $j$ on feature $f$ and $Ni$ is the total number of
  candidates ($w$ is the weight, discussed below). Because of the
  piecewise definition of \textit{identical}, this number is always at
  least $1$ because $f_k$ agrees already with $f_j$. The effect of
  this equation is to take the count of shared features and weight
  them by the size of the sharing group. The features that are shared
  with a large number of other items get larger fraction of the normal
  count subtracted.

For example, let $j$ and $k$ be some English segments /t/ and /s/,
respectively, which share the features \textit{place} and
\textit{voice}. Coronals
and voiceless sounds are both common in English, but assume for this
example that voiceless segments occur more often. There will be more
agreeing segments in lack of voicing than those agreeing in place. So the
total weight of \textit{place} between /t/ and /s/ will be more than
that of \textit{voice}: In equation \ref{wiv-place}, we see that 1000
of the 2000 segments in the corpus agree with /s/ in place, while 1500
agree in voice.

\begin{equation}
  1 - \textrm{agree}place_{jk} = 1 - 1000/2000 = 1 - 0.5 = 0.5
  \label{wiv-place}
  \end{equation}

\begin{equation}
  1 - \textrm{agree}voice_{jk} = 1 - 1500/2000 = 1 - 0.75 = 0.25
  \label{wiv-voice}
\end{equation}

So these two features, once weighted, will no longer contribute a full
$2.0$ to the amount of agreement between the two segments /s/ and /t/,
but only $1.0$. Their combined weight has been halved because both
features are so common as to not need further explanation.
Therefore, as \textit{agree} grows larger, it subtracts more and more
from the original value of 1.
The final value, $w$, which is what gives the name ``weighted identity
value'' to this measure, provides a way to control how much is
discounted. A high $w$ will subtract more from uninteresting groups,
so the contribution of the above two features might be discounted to
$0.6 + 0.3 = 0.9$ given a smaller $w$, for example.

% William. Idsardi is an OT critic based on computability grounds

% NOTE: This is a lot like avoiding excessive neutralisation in a
% paradigm---you don't want to overdo neutralisation because otherwise
% the underlying form becomes quite distant from the surface form.

\subsection{Phonological Distance}

It is at this point that the two types of analysis, phonological and
syntactic, diverge. Although Goebl's techniques are general enough to
operate over any set of features that can be extracted, later work
uses properties specific to the type of data in order to achieve
better results. Specifically, the application of computational
linguistics to dialectometry beginning in the 1990s introduced
methods from other fields. These methods, while generally giving more
accurate results quickly, are tied to the type of data on which they
operate.

\subsubsection{Kessler}

The most significant of these methods for phonological distance was
introduced to dialectometry by \namecite{kessler95}. Kessler used
Levenshtein distance \cite{lev65} to determine distance between
individual words and then just used simple addition to combine the
distances.  Levenshtein distance is a simple idea---simply count the
number of differences between two strings. The intent is the same as
Goebl's Relative Identity Value, using single characters of a word as
features. However, since strings are an ordered sequence of characters
in this model, determining the best set of correspondences between two
strings is a non-trivial problem. This is the problem that
Levenshtein's algorithm solves. It tries all possible alignments and
remembers the best ones using a dynamic programming algorithm. The
alignment generated by the algorithm is guaranteed to have the most
possible correspondences between the two input strings. Like Goebl's
measures, Levenshtein distance is in principle applicable to any
ordered sequence and as such has been used in many fields
\cite{sankoff83}. However, its discovery of correspondence makes it
particularly suited for phonology.

The Levenshtein distance algorithm models alignment
as the series of changes necessary to convert the first sequence to
the second. This means that input to the algorithm includes the cost
of changes, in addition to the two sequences. Three changes are
typically specified: insertion, deletion, and substitution. Others have
been proposed for use in phonology,
such as metathesis (see \cite{kondrak02} for an
example), but since metathesis can be modelled as an insertion and a
deletion, there is not much reason unless it is deemed to be very
common for some corpus.
Given these inputs, the algorithm calculates the number of each
operation necessary to convert the first sequence to the
second. Finding the lowest total operation cost produces the highest number of
correspondences.

In a character-based model, insertion and deletion are the primitive operations,
with a cost of one each. Substitution is one insertion and one
deletion, giving it a cost of two. However, substitution of a character
for itself changes nothing and thus has zero cost.
%Given these functions, the
%Levenshtein algorithm will return the minimum number of insertions and
%deletions necessary for transforming the source to the target.

The formal definition of the functions $ins$, $del$, and $sub$
for characters is
\[ \begin{array}{l}
   ins(t_j) = 1 \\
   del(s_i) = 1 \\
   sub(s_i,t_j) = \left\{
     \begin{array}{ll}
       0 & \textrm{if $s_i=t_j$} \\
       2 & \textrm{otherwise}
     \end{array} \right.

   \end{array}
\]

For example, with these costs, the distance between \textit{sick} and
\textit{tick} is two---a substitution of t for s. The distance between
\textit{dog} and \textit{dog} is zero. The distance between
\textit{dog} and \textit{cat} is six because none of the characters
correspond, so three insertions and three deletions are needed to
convert \textit{dog} to \textit{cat}. Not all calculations are so
easy. The distance between \textit{realty} and \textit{reality} is a
single insertion. The distance between \textit{nonrelated} and
\textit{words} is 9, not 15 ($10 + 5$), because the characters `o-r-d'
are in correspondence. While a linguist can easily learn to calculate Levenshtein
distance, an algorithm does not learn and must have a systematic way
to choose the best alignment.

To do this, Levenshtein's algorithm must provide a way to build up
the distance between two strings based on smaller substrings.
This leads to a recursive algorithm in which Levenshtein distance
between two strings is defined in terms of the cheapest of three
possible combinations of source/target substrings plus an operation to
produce the two strings. Let us take as an example the distance
between the source ``cab'' and the target ``cob''. The three cases are
as follows:

\begin{enumerate}
\item Insertion of `b' to ``ca'' to produce
``cob''. Insertion costs 1, and the distance between ``ca'' and
``cob'' is 3, so the cost of this case is 4.
\item Deletion of `b' from ``cabb'' to produce ``cob''. Deletion costs
  1, and the distance between ``cabb'' and ``cob'' is 3, so the cost
  of this case is 4.
\item Substitution of `b' to `b' in ``ca'' to produce
  ``co''. Substitution of identical characters costs 0, and the distance
  between ``ca'' and ``co'' is 2 because the substitution of `o' for
  `a' already cost 2. The resulting cost of this case is 2.
\end{enumerate}

For this example, the substitution case is the cheapest. It is notable
that the substrings in each of the three cases can be derived for any
pair of source/target strings. There is one case for each of the
operations specified above. This leads to the following formal
definition of Levenshtein's distance algorithm.

For each character $s_i \in S$ and $t_j \in T$ for any word $S$ and $T$,
\[ \begin{array}{rl}
 levenshtein(s_i,t_j) = minimum( & ins(s_i)+levenshtein(s_{i-1},t_j),\\
& del(t_j)+levenshtein(s_i,t_{j-1}, \\
& sub(s_i,t_j)+levenshtein(s_{i-1},t_{j-1}))
   \end{array}
\]
The total distance is $levenshtein(S_{|S|},T_{|T|})$.

This recursive definition of distance is most efficiently realised by
storing previous distances in a table. In addition, the table make a
nice visualisation of the algorithm's operation.
For example, finding
the Levenshtein distance from ``ART''
to ``CAT'' creates the table in figure \ref{art2cattable}. In this
table, the substring cases for ``ART'' and ``CAT'' can be found in the
square above for insertion, the square to the left for deletion and
the diagonal above-left square for substitution.
In this
table, insertion corresponds to a downward move, deletion to a
rightward move, and substitution a diagonal move. The total
Levenshtein distance is found at the bottom-righthand corner, but the
distances to all intermediate forms are stored in the table as
well. The intermediate form ``CATART'', for example, is obtained by
inserting three times without any deletions or substitutions. Its
cost of 3 is found at the bottom of the first column.

\begin{figure}
\caption{The distance table for ``ART'' to ``CAT''}

\begin{center}
\begin{tabular}{c|c|c|c|c}
%\hline
  &   & A & R & T \\
\hline
  & $\mathbf{0}$ & 1 & 2 & 3 \\
\hline
C & $\mathbf{1}$ & 2 & 3 & 4 \\
\hline
A & 2 & $\mathbf{1}$ & $\mathbf{2}$ & 3 \\
\hline
T & 3 & 2 & 3 & $\mathbf{2}$
% \hline
\end{tabular}

\end{center}

\label{art2cattable}
\end{figure}

The optimal path is shown in bold. Notice that it
follows the diagonal for free substitutions and propagates either down or right
in their absence. The final distance is two, indicating that two
primitive operators are required; that is, two insertions or deletions.
In fact, the table gives
them: insert `C' to obtain ``CART'', moving down in the table; then
delete `R' to obtain ``CAT'', moving left in the table. `A' and `T'
are common to both words and both produce a diagonal move.

% example here
% then mention Gersic again (remember him?)
% then jump over to Dunning and all the language identification work
% that happened in the early 90s
\subsubsection{Heeringa}

Variations on this theme have been explored, and best
described in Heeringa 2004. Starting with \cite{nerbonne97}, he
augmented the distance definition so that substitution cost was based
on the number of features that would have to change. This provides
more precision, and is based on phonological theory. Later, he also
tried weighting features by information gain (Nerbonne and Heeringa
1998?). In his dissertation, \namecite{heeringa04} tried an even more
phonetic method when he defined the substitution cost between segments
as the total distance between the first two formants, as measured in
barks. However, because he did not have access to the original speech
of the dialect data he was measuring, this substitution cost was
uniform across all instances of a particular segment.

This section is still a summary. Some detail in the methods needs to
be added still. The following paragraphs were moved from the method
section were they are out of place.

The most direct way to refine Levenshtein distance to take advantage
of linguistic knowledge is by
changing the definitions of $ins$, $del$, and $sub$ to take into
account phonetic
and phonological properties of segments. When segments are treated as
feature bundles instead of merely being atomic, \cite{nerbonne97}
propose that the substitution distance between two segments simply be
the number of features that are different. Two identical segments will
therefore have a substitution distance of zero; segments phonetically
similar will have a small distance. For example, [k] and [g] would
have a distance of one in this system.

Although it increases precision, feature-based substitution causes a
number of complications. The first is that substitution distance
becomes complicated when not all features are specified for every segment. This is
the case, for example, between the vowels and consonants.
The minimum difference must be at least the number of unshared
features, such as \emph{Advanced Tongue Root} for vowels or
\emph{obstruent} for consonants. In other words, the minimum segment
distance will always be at least the sum of the non-shared
features. The distance of the shared features can then be added on to
this baseline. %This is known as the Hamming, or Manhattan distance
%(cite Kondrak and Nerbonne or Heeringa on this after verifying it).
For example, if a consonant with seven features shares only two
features with a five-feature vowel, the minimum distance will be eight:
\[ (7 - 2) + (5 - 2) = 5 + 3 = 8 \] As a result, the range of
distances possible will be have a minimum of 8 if all shared features
match and a maximum of 10 if none do.

The second complication is obtaining definitions for $ins$ and $del$
once $sub$ is defined. It would be best to retain the original
proportions---substitution should cost twice as much as insertion and
deletion. To deal with substitution's variable cost, then, insertion and
deletion should be averages. To find the average substitution cost, one can
 take the average cost of substituting every character
for every other character. Then $ins$ and $del$ return half of this
average. With these three functions defined, the table-based algorithm
given above can combine feature distances to find the minimum word distance.

\subsubsection{Statistical approaches}
Recent approaches to dialect distance have gone back almost to Goebl's
definitions, but with more emphasis on information theoretic measures
to help extract information. For example, \namecite{sanders08} gives a
method called Maximum Likelihood distance that uses a naive Bayes
classifier trained on bigrams. \namecite{hinrichs07} uses vector math or
something to do nearly the same thing. Maximum Likelihood distance is
described here.

\label{dunningalgorithm}

\cite{dunning94} gives a method for language classification that uses
a simple Markov-based bigram language model. Essentially, it addresses
the question ``How likely was the training corpus to have generated
the test corpus?''. The method starts with Bayes' Law in equation
\ref{bayes} and makes the maximum likelihood assumption to obtain the
na\"ive Bayes classifier in equation \ref{naivebayes}.  The na\"ive
Bayes classifier assumes a uniform prior, and assumes that the test
data have a probability of one.
                                % or is it the training data? hmm
With the bigram Markov model, the maximum likelihood estimation (MLE) is
easily computed as in equation \ref{bigramprod}.

\begin{equation}
\label{bayes}
P(training|test) = \frac{P(test|training) P(training)}{P(test)}
\end{equation}

\begin{equation}
\label{naivebayes}
P(training|test) = P(test|training)
\end{equation}

\begin{equation}
\label{bigramprod}
MLE = \prod_{i=1}^n P(bigram_i)
% ought to come up with a better name for the lhs. steal from Dunning k
\end{equation}

Dunning then chooses the training language that maximizes the
likelihood for
the test word list. This modification alters only which side of
the comparison is being varied: Dunning has only one word list, which
is then compared to many possible languages. This method has only one
training language, the target, but estimates the distance from
multiple test word lists.

% Essentially, this reverses the
% question by keeping the training corpus constant and testing against
% multiple sets of data. The likelihoods produced by this can be used as
% a distance measure.

For example, assume the following
artificial training language ``ababcaaaaaa''. The bigrams of this
language are ``ab'', ``ba'', ``ab'', ``bc'', \ldots Then the frequency
of each bigram can be calculated:

\begin{tabular}{ccc}
P(``aa'') = & $\frac{5\ occurrences}{10\ bigrams}$ & = 0.5 \\
P(``ab'') = & $\frac{2}{10}$ & = 0.2 \\
P(``ba'') = & $\frac{1}{10}$ & = 0.1 \\
P(``bc'') = & $\frac{1}{10}$ & = 0.1 \\
P(``ca'') = & $\frac{1}{10}$ & = 0.1 \\
\end{tabular}

If the estimator is asked to classify the test word ``abc'', it would
be broken into the bigrams ``ab'' and ``bc'', giving the likelihood
P(``ab'') $\cdot$ P(``bc'') $= 0.2 \cdot 0.1 = 0.02$. In comparison,
the string ``aab'' would have the likelihood P(``aa'') $\cdot$
P(``ab''') $= 0.5 \cdot 0.2 = 0.1 $. ``aab'' is therefore closer to
the language that generated the training corpus, despite the fact that
``abc'' actually occurs in the training.

Unfortunately, this estimator needs refinement in a several areas. The
worst problem is that it gives zero probability for any bigram that
does not occur in the training, such as ``ac'' in the example
above. This single factor, P(``ac'')$=0$, causes the estimated
probability of the entire test string to fall to zero---in other
words, saying that the distance between the two languages is
infinite. This is a real problem when the test corpora consist of
phonetically disordered speech, as in our experiments.

The solution is to smooth the input, preventing the appearance of
zero probabilities. Smoothing allocates some of the probability space
to unseen bigrams. The smoothing method used here is the
Good-Turing method, first presented by \cite{good53}.
%and explained in \namecite{jurafskymartin}.
Good-Turing smoothing estimates the counts of bigrams seen N times
from the counts of bigrams seen N+1 times. The precise equation used to
determine the expected value for a bigram
seen a certain number of times is
$r^* = (r+1)(n_{r+1} / n_r)$.

For the most interesting case, previously unseen bigrams, $r=0$
because that is the number of times these bigrams have
appeared. Then $r+1=1$ and $n_1$ is the number of different
bigrams that have occurred only once. Finally, $n_0$ is the number of
bigrams that have never occurred, which can be found by subtracting
all bigrams seen any number of times from the total number of possible
bigrams. Of course, Good-Turing smoothing is applied to
higher numbers as well: for example, the number of bigrams seen six
times is estimated from the number of bigrams seen seven times by
using the appropriate values of $r, n_6, \textrm{and } n_7$.

Two minor problems remain. First, longer words give lower likelihoods. To ensure
that likelihoods of different test corpora are
comparable, we scale the results of each test corpus by the length of
the corpus.
Second, it is more convenient for both human and computer to use
some other measure than raw likelihoods. Since likelihoods are just
probabilities, 1.0 means ``identical to training language'' and 0.0 means
``infinite distance from training language''.
To improve intuitive understanding of the results, we
would like a distance measure that is greater
for weaker matches between training and test. In addition, storing very small
probabilities can be problematic when using hardware-native floating
point numbers.
Taking the negative logarithm of the likelihoods solves both of these
problems: the logarithm converts a 0--1 range to a 0--$-\infty$ range,
which, when negated, produces a number that increases in a way that
corresponds nicely with intuitions of distance.

\subsection{Syntactic  Distance}
Recently some work in syntax has begun as well. The first steps in
this were Nerbonne and Wiersma's 2006 analysis of Finnish L2 learners
of English, followed by Sanders' 2007 analysis of British dialect
areas. Syntax distance must be approached in quite a different way than
phonological distance. First, syntactic data is extractable from raw
text, so there is usually much more available. But this has a
concomitant (or an associated) drop in reliability of the
data.

% this paragraph explains the difference between syntactic corpora and
% phonological corpora, badly.
Second, the phonological data is pre-processed by the
transcriber. This important step reduces the dimensionality of a
complex phonetic waveform to a few binary features. While making
phonological algorithms easier to work with, it may introduce bias
from the transcriber---some dialect surveys have been known to exhibit
``field worker dialects'' (cite this, probably from Trudgill). A
similar bias can occur when text is hand-parsed, but with a
large enough corpus, one can use an automatic parse that should have
fewer biases, albeit at the cost of more random error.

% This paragraph need an introducing paragraph
Finally, if computational methods are duplicating the results of
previous work, that's great, but it would be even better to have a
way of finding relationships between
the results of the various methods; this would tell us how well the
areas cohere at various linguistic levels, which is typically done by
hand or not at all by traditional dialectologists.

\subsubsection{Nerbonne and Wiersma}
This section is too short (still a summary) and written
incorrectly--not as a summary of previous work.

Syntax distance has been much less explored. At present, there is
really only one method used. (Although I need to try some others ... )
Due to the larger corpora available for syntactic analysis and the
less careful selection of content of said corpora, a statistical
method that is resistant to error is more appropriate than the typical
low-data symbolic approach used in phonology. Beyond this, the
syntactically annotated corpora are not word- or sentence-aligned in
the same way that the available phonological data are. Hence a
syntactic distance measure will have to have counting as its basis by
default.

One of the first methods was proposed by \namecite{nerbonne06}.
It models syntax by part-of-speech (POS) trigrams and
uses the trigram types in a permutation test of significance. This was
extended by \namecite{sanders07}, who used \quotecite{sampson00} leaf-ancestor
paths as the basis for building the model instead. Both approaches
essentially treated the representation of syntax as symbols for the
actual permutation test, however.

The problem with this method is that it is just a statistical test for
significance; although it includes a simple distance measure, it is
impossible to normalise completely to account for effects of size
because the corpora are not aligned. A permutation test is needed to
determine if this distance is significant. But the permutation test
can only tell us whether the two corpora are unlike.

Other ideas include training a
model on one area and comparing the entropy (compression) of other
areas. At this point it's unclear whether this would provide a
comparable measure, however.

\subsubsection{Language models}
Part-of-speech trigrams are quite easy to obtain from a syntactically
annotated corpus. Nerbonne and Wiersma (2006) argue that POS trigrams
can accurately represent at least the important parts of syntax,
similar to the way chunk parsing can capture the most important
information about a sentence. POS
trigrams can be collected from a tree by
\verb+(window 3 (leaves tree))+

On the other hand, it would be nice to directly represent the upper
structure of trees. Sampson's leaf-ancestor paths provide one way to
do this: they simply append the path to the root to each POS in the
sentence.

Wow. There is a lot more I could say about this. Probably another
paragraph of background before starting to talk about leaf-ancestor
paths, and then all the gory details from my ACL paper. That was at
least a page.

Another idea is supertags rather than leaf-ancestor paths. This is
quite similar but might work better.

\subsubsection{Permutation test}

A permutation test works by determining if the distance between two
corpora is significant or just statistical error (this is the wrong
sense of statistical error). The first step is to calculate the distance between
two corpora. Then the corpora are combined and two subcorpora are
extracted from the combined corpus. This is repeated many times: if
the distance between the subcorpora is less than the distance between
the original corpora, we can see that combining the two corpora erased
the differences. Repeating this test enough times will show if the
difference is significant. Twenty times is the minimum for
significance, but usually the test is repeated one (ten?) thousand times or more.

Of course, as a basis for the test, we first need a way to calculate
distance. The measure used here, called Recurrence, is due to Kessler
(2001). It is fairly simple: all POS tokens are sorted by type and
counted. Then for each POS type the count in both corpora is averaged
and the count of one is subtracted from the average. The measure is
designed to represent the amount of variation from each other is
exhibited by the two corpora.

Afterward, the distance must be normalised to account for two things:
the length of sentences in the corpus and the amount of variety in the
corpus. If sentence length differs too much between corpora, there
will be consistently lower token counts in one corpus. So the counts
are re-allocated by relative sizes of two corpora. (More explanation
and an equation is needed here or perhaps later). If the number of
types differs too much between the two corpora, the corpus with more
tokens will consistently have lower token counts as well. To avoid
this all tokens are scaled by the average number of types per token
across both corpora: $2n/N$ where $n$ is the type count and $N$ is
the token count. The additional factor $2$ is necessary because we are
recombining the tokens from the two corpora.

Once the distances have been normalised, the measures of the original
corpora and the sampled subcorpora should be comparable. It is a
simple matter to repeat this process enough times to ascertain whether
the two original corpora are statistically different.

\subsubsection{Sanders (2007)}
A proper overview of leaf-ancestor paths. Something decent ok.

\section{Methods}
\begin{enumerate}
\item \quotecite{nerbonne97} refinement, without information gain weighting.
\item \quotecite{nerbonne06} syntax distance.
\item Combined on the data gathered and analysed in \namecite{shackleton07}.
\item And the ICE data (with leaf-ancestor paths) of \namecite{sanders07}.
\item And dividing both sets of data according to the same Government
  Office regions.
\end{enumerate}
\subsection{Levenshtein Distance Experiment}
\begin{enumerate}
\item Group the data of \cite{shackleton07} into government office
  regions.
\item (TODO:Summarise the data set here.)
\item It's from the 50s, 50 single (double?) syllable words designed
  for maximal dialect variation.
\item Compare each site in each region to each site in every other region.
\item Average the distances.
\item This forms a graph connecting the 11 regions by phonological distance.
\end{enumerate}
\subsection{Syntax Distance Experiment}
\begin{enumerate}
\item Group the ICE corpus into government office regions.
\item The ICE corpus consists of transcribed conversations and
  monologues, parsed according to some syntactic standard.
\item The parsed sentences were split into leaf-ancestor paths and
  part-of-speech trigrams.
\item The experiments were run with a variety of parameters.
\item Sample sizes of 1000 and 500, $r$ and $r^2$.
\item First, for each parameter setting, a preliminary test comparing
  London and Scotland was carried out.
\item First, London and Scotland were compared.
\item Then they were shuffled together and split into two halves the
  same size as London and Scotland themselves.
\item If the method is discriminating successfully, London and
  Scotland should be significantly different and the shuffled London
  and Scotland should {\em not} be significantly different.
\item This was true for only one parameter setting: distance measure
  $r$ and sample size 1000, using leaf-ancestor paths.
\item For the rest, London and Scotland were not significantly
  different at $p < 0.05$.
\item Although some parameter settings resulted in $p < 0.10$, in
  particular $r$, 1000, trigrams.
\item I suspect this is a problem of corpus size. More on that below.
\item Finally, all government office regions were compared to each
  other using the successful parameter settings of 1000, $r$, 1000,
  paths.
\item The results are \ldots below?
\end{enumerate}
\subsection{Correlation}
\begin{enumerate}
\item The output of the syntax distance is too simple for anything but
  coarse comparison to phonological distance.
\item It's just a significance test.
\item So we can look at the regions that were not significantly
  different to see whether their distances were considerably less than
  other regions.
\item There is no hard limit for this.
\item See the results below.
\end{enumerate}
The second question of this paper can now be answered: whether
multiple ways of measuring linguistic distance give the same
answer. As the answers provided by syntax distance are somewhat crude
at this point, the correlation will necessarily be crude as well, but
it should be possible to see if the syntax distance measure at least
agrees with the phonological measure.

Ideally, this would be a simple matter of aligning distances and
running a correlation on them (see for example papers by Heeringa
2003-2005 or so.) However, given the yes/no data that is in addition
obscured by the requirement of corpus size, the best comparison is
simply that of agreement. Is the phonological distance between two corpora that
were significantly different according to the syntactic measure
significantly (not sure how to calculate this) higher than the average
distance?

% experiment..
In order to carry this out, the phonological distances of Shackleton
(2007) were grouped into the same areas as the corpora of Sanders
(2007). The group distances were the average of the constituent area distances.
\section{Results}
\begin{enumerate}
\item Phonogical distance was not surprising. It's also not done.
\item Syntax distance was mostly significant except between NW, SE,
  London and EM. (that's a chain there)
\item The two correlated how? This isn't done.
\end{enumerate}
\section{Discussion}

Discussion
  \begin{enumerate}
  \item What do these tell us?
  \item Do the two distances correlate?
  \item Do the distances correlate with any other distances,
    particularly geographic distance?
  \item The End.
  \end{enumerate}


% \begin{enumerate}
% \item Introduction
%   \begin{enumerate}
%   \item talking about Seguy again.
%   \item some similar work has been done with English
%   \item Blah blah computers are great but stubborn
%   \item Why not at least try though
%   \item Hence: phonological distance and syntax distance
%   \end{enumerate}
% \item Phonological distance
%   \begin{enumerate}
%   \item Cochlear data
%     \begin{enumerate}
%     \item 107 word list
%     \item Pediatric cochlear implant users
%     \item Collected for Steven B. Chin's research project
%     \end{enumerate}
%   \item British data
%     \begin{enumerate}
%     \item 1001 (just Long) word list
%     \item Collected from NORM in the 1950s to capture British dialects
%       before they were destroyed by mobility and mess media.
%     \item This has happened only to some extent, however. There are
%       still regional differences, but the regions are different and
%       the differences have changed since the 1950s.
%     \item Analysed by Shackleton (2007) in numerous ways.
%     \item Correlation with my syntax results will follow.
%     \end{enumerate}
%   \item Some other stuff
%   \end{enumerate}
% \item Syntax distance
%   \begin{enumerate}
%   \item No alignment, but massive corpora.
%   \item Ergo, different methods : statistics.
%   \item Current method only tells whether region is significantly
%     different or not. That's not so great.
%   \item But with a statistical method, the most you can hope for is
%     distance---details of any differences will have to be extracted
%     separately.
%   \item You might be able to extract info from the data structures created along
%     the way; naive bayes maximum entropy
%     classifier gives quite good results for what bigrams are the most
%     important to it.
%   \item Some ideas include using supertags instead of leaf-ancestor
%     paths, perhaps automatically extracted from the manual parse.
%   \item Another idea is comparing entropy by compression. This seems
%     pretty promising to me.
%   \end{enumerate}

% \item Discussion
%   \begin{enumerate}
%   \item What do these tell us?
%   \item Do the two distances correlate?
%   \item Do the distances correlate with any other distances,
%     particularly geographic distance?
%   \item The End.
%   \end{enumerate}

% Notes:
% Apparently one Clive Upton has something resembling a British dialect
% atlas that could be used the same way that Nerbonne and Heeringa
% did. But I wonder why they didn't already do it---Norwegian seems much
% less likely than English simply because one {\em expects} it to have
% larger corpora.

% Actually the wikipedia has a nice overview of the survey project and links to
% the resulting
% works.

% \verb+http://en.wikipedia.org/wiki/Survey_of_English_Dialects+ ; It is
% problematic in that the data were collected in the 50s from NORMs
% because they were interested in the traditional dialects, not the new
% ones that might arise.

% Voices is in progress and will eventually provide a modern dialect survey.
% 1978. The linguistic atlas of England, edited by Orton, John
% Widdowson and Clive Upton. (probably the best) : PE1705.L56

% 1993. Survey of English Dialect: The Dictionary and Grammar :
% PE1704.U65 1994
% 1996. An Atlas of English Dialects. 2nd edn. London: Routledge [with
% J.D.A.Widdowson]. : PE1705.U68 1996
% 1994: PE1704.U65 1994

%\end{enumerate}
\bibliographystyle{robbib} % ACL is \bibliographystyle{acl}
\bibliography{central}
\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
