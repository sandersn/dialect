% \usepackage{tipa}
% \newcommand{\bibcorporate}[1]{#1} % stolen from Erik Meijer's apacite.tex
\documentclass[11pt]{article}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage[all]{xy}

% \usepackage{acl07}
\usepackage{robbib}
\title{Comparison of Phonological and Syntactic Distance Measures}
\author{Nathan Sanders}
\begin{document}
\maketitle
\doublespacing
\section{Introduction}

I applied existing dialectometric techniques to the English of Britain
and introduced new techniques for measuring both phonological and
syntactic distance. Comparisons between distance have become rarer
since computational linguistics has taken over dialectometry. I also
compare the results of the two, although we will see that there is a
reason comparisons are rare; they are not very informative.

The dominant phonological distance measure in computational
dialectometry is Levenshtein distance or a variant
thereof. Statistical methods are slowly being introduced, such as the
information measure of \cite{hinrichs07} and \cite{sanders08}.
I developed a statistical method that uses a naive Bayes
classifier, similar to language identification methods developed in
the early 90s.

Syntactic distance has not been measured well in computational
dialectometry. Nerbonne and Wiersma have measured syntactic distance
statistically using a permutation test, with trigram features designed
to capture at least surface syntax. I tried the same test with
leaf-ancestor paths as features.

\section{Previous Work}
\subsection{S\'eguy}
Measurement of linguistic similarity has always been a part of
linguistics from its beginning, with various degrees of
respectability. Until 1973, when S\'eguy dubbed a new set of
approaches `dialectometry', these methods lagged behind the rest of
linguistics in formalisation. However, S\'eguy's quantitative analysis
of Gascogne French, while not aided by computer, was the predecessor
of more powerful statistical methods that essentially required the use
of computer as well as establishing the field's general dependence on
well-crafted dialect surveys that divide the incoming data along
traditional linguistic boundaries: phonology, morphology, syntax, etc.
This makes both collection and analysis easier, although it requires
much more work when trying to produce a complete picture of dialect
variation.

The ALF project, which S\'eguy directed, begin in 195? to collect data
in a dialect survey which asked speakers from the towns of Gascogne
questions informed by different areas of linguistics. For our
purposes, the most interesting are the phonological data: these were
individual words designed to capture the already-studied variation in
the area in order to characterise its extent precisely.

The atlases, as eventually published, contained not only annotated
maps, but some analysis as well. This analysis was what S\'eguy named
the dialectrometry. It was not primarily generated by an
algorithm executing on a computer, but by linguists
counting the collected data by hand. The analysis is accordingly
linguistically sophisticated and numerically naive: isoglosses were
posited from observations of type counts along with their arrangement
on the map.

\subsection{Gersic}
At or just before the same time as the Atlas was finally published in
1973, Gersic proposed a computational method for measuring distance
between to phonological segments. Segments are specified as a vector
of features---consonants have
one feature set, while vowels have another. Features are given numeric
values. For example, a [+voice] segment will have $1$ in the third
position of the feature vector, while a [-voice] segment will have
$0$. Gersic then gives a
function $d$ that gives the distance between segments:
\[ d(i,j) = \sum_{k=1} |a_{i_k} - a_{j_k}|\] Where $a_i$ is the first
segment
being compared and $a_j$ the second. However, the equation is easier
to understand if summarised by the English prose ``count the number of
features with values that differ between segments''. Gersic did not
provide a way to compare vowels with consonants, or a way to combine
the segment distances into word distances, but those techniques would
be developed later.

\subsection{Goebl}

Later, Hans Goebl emerged as a leader in the field of dialectometry,
formalising the aims and methods of dialectometry. His primary
contribution was development of various methods to combine individual
distances into global distances and from there global clusters. These
methods were more sophisticated mathematically than previous
dialectometry, and operated with any features extracted from the data. His
analyses have mostly used the Atlas Linguistique de Francais
(ALF).

In Goebl (2006), he provides a summary of his work, including
definitions of the measures Relative and Weighted Identity
Value. These measures are independent of the source, capable of
extracting distances from any set of features, which need not be
specified a priori, unlike Gersic's method. For example, Relative
Identity Value, when comparing any two segments, counts the number of
features which share the same value, and divides by the number of
shared features. This has the advantage that it is robust in the fact
of (1) unshared features and (2) missing data. The result is a single
percentage that indicates the relative similarity. Calculating this
distance between all pairs of things to be compared produces a matrix
which can be used for clustering or other purposes. The equation is
actually:

\begin{equation}
  \frac{\textrm{identical}_{jk}} {\textrm{identical}_{jk} - \textrm{unidentical}_{jk}}
\label{riv}
\end{equation}
For some items being compared $j$ and $k$. In this case
\textit{identical} is
\begin{equation}
  \textrm{identical}_{jk} = |f \in \textrm{\~N}_{jk} : f_j = f_k|
\end{equation}
where $i$ is each feature shared by $j$ and $k$ (called
$\textrm{\~N}_{jk}$). \textit{unidentical} is defined similarly, except
that it looks at all features, not just the shared features.
\begin{equation}
  \textrm{unidentical}_{jk} = |f \in \textrm{N} : f_j \neq f_k|
\end{equation}

Additionally, Goebl
describes a refinement of the method called Weighted Identity
Value. This measure stems from the idea that some differences are
more important than others, in particular that more information arises
from feature values that only happen a few times rather than those
values that characterise a large number of the items being studied.
This idea shows up later in the permutation tests used by Nerbonne and Wiersma.

The mathematical implementation of this idea is fairly simple. Goebl
is interested in feature values that occur only a few times. This is
because, if a feature has some value shared by all of the items being
compared, then it provides {\it no} useful information for
distinguishing the items---they all belong to the same group.
The situation improves if all but one item
share the same value for a feature; at least there are now two
groups, although the larger group is still not very informative.
The most information is available if each item being studied
has a different value for a feature; the items fall trivially into
singleton groups, one per item.

Equation \ref{wiv-ident} works by discounting (lowering)
the \textit{identical} count from equation \ref{riv} by
the amount of information that feature value conveys. The
amount of information, as discussed above, is based on the number of
items that share a particular value for a feature. If
the worst case, in which all items share the same value for some
feature, then that feature is useless, so \textit{identical} should be
discounted all the way to zero. The equation for \textit{identical} now becomes
\begin{equation}
  \textrm{identical} = \sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_{j}}{(Ni)w} & \textrm{if} f_j = f_k
  \end{array} \right.
\label{wiv-ident}
\end{equation}

\begin{equation} \sum_i \frac{\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
\end{array} \right.}
  {\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
    \end{array} \right. - |f \in \textrm{N} : f_j \neq f_k|}
  \label{wiv-full}
  \end{equation}

  where $\textrm{agree}f_{j}$ is the number of candidates that agree
  with item $j$ on feature $f$ and $Ni$ is the total number of
  candidates ($w$ is the weight, discussed below). Because of the
  piecewise definition of \textit{identical}, this number is always at
  least $1$ because $f_k$ agrees already with $f_j$. The effect of
  this equation is to take the count of shared features and weight
  them by the size of the sharing group. The features that are shared
  with a large number of other items get larger fraction of the normal
  count subtracted.

For example, let $j$ and $k$ be some English segments /t/ and /s/,
respectively, which share the features \textit{place} and
\textit{voice}. Coronals
and voiceless sounds are both common in English, but assume for this
example that voiceless segments occur more often. There will be more
agreeing segments in lack of voicing than those agreeing in place. So the
total weight of \textit{place} between /t/ and /s/ will be more than
that of \textit{voice}: In equation \ref{wiv-place}, we see that 1000
of the 2000 segments in the corpus agree with /s/ in place, while 1500
agree in voice.

\begin{equation}
  1 - \textrm{agree}place_{jk} = 1 - 1000/2000 = 1 - 0.5 = 0.5
  \label{wiv-place}
  \end{equation}

\begin{equation}
  1 - \textrm{agree}voice_{jk} = 1 - 1500/2000 = 1 - 0.75 = 0.25
  \label{wiv-voice}
\end{equation}

So these two features, once weighted, will no longer contribute a full
$2.0$ to the amount of agreement between the two segments /s/ and /t/,
but only $1.0$. Their combined weight has been halved because both
features are so common as to not need further explanation.
Therefore, as \textit{agree} grows larger, it subtracts more and more
from the original value of 1.
The final value, $w$, which is what gives the name ``weighted identity
value'' to this measure, provides a way to control how much is
discounted. A high $w$ will subtract more from uninteresting groups,
so the contribution of the above two features might be discounted to
$0.6 + 0.3 = 0.9$ given a smaller $w$, for example.

% William. Idsardi is an OT critic based on computability grounds

% NOTE: This is a lot like avoiding excessive neutralisation in a
% paradigm---you don't want to overdo neutralisation because otherwise
% the underlying form becomes quite distant from the surface form.

\section{Methods}

It is at this point that the two types of analysis, phonological and
syntactic, diverge. Although Goebl's techniques are general enough to
operate over any set of features that can be extracted, later work
uses properties specific to the type of data in order to achieve
better results. Specifically, the application of computational
linguistics to dialectometry beginning in the 1990s introduced
methods from other fields. These methods, while generally giving more
accurate results quickly, are tied to the type of data on which they
operate.

\subsection{Phonological Distance}

The most significant of these methods for phonological distance was
introduced to dialectometry by \namecite{kessler95}. Kessler used
Levenshtein distance \cite{lev65} to determine distance between
individual words and then just used simple addition to combine the
distances.  Levenshtein distance is a simple idea---simply count the
number of differences between two strings. The intent is the same as
Goebl's Relative Identity Value, using single characters of a word as
features. However, since strings are an ordered sequence of characters
in this model, determining the best set of correspondences between two
strings is a non-trivial problem. This is the problem that
Levenshtein's algorithm solves. It tries all possible alignments and
remembers the best ones using a dynamic programming algorithm. The
alignment generated by the algorithm is guaranteed to have the most
possible correspondences between the two input strings. Like Goebl's
measures, Levenshtein distance is in principle applicable to any
ordered sequence and as such has been used in many fields
\cite{sankoff83}. However, its discovery of correspondence makes it
particularly suited for phonology.

The Levenshtein distance algorithm models alignment
as the series of changes necessary to convert the first sequence to
the second. This means that input to the algorithm includes the cost
of changes, in addition to the two sequences. Three changes are
typically specified: insertion, deletion, and substitution. Others have
been proposed for use in phonology,
such as metathesis (see \cite{kondrak02} for an
example), but since metathesis can be modelled as an insertion and a
deletion, there is not much reason unless it is deemed to be very
common for some corpus.
Given these inputs, the algorithm calculates the number of each
operation necessary to convert the first sequence to the
second. Finding the lowest total operation cost produces the highest number of
correspondences.

In a character-based model, insertion and deletion are the primitive operations,
with a cost of one each. Substitution is one insertion and one
deletion, giving it a cost of two. However, substitution of a character
for itself changes nothing and thus has zero cost.
%Given these functions, the
%Levenshtein algorithm will return the minimum number of insertions and
%deletions necessary for transforming the source to the target.

The formal definition of the functions $ins$, $del$, and $sub$
for characters is
\[ \begin{array}{l}
   ins(t_j) = 1 \\
   del(s_i) = 1 \\
   sub(s_i,t_j) = \left\{
     \begin{array}{ll}
       0 & \textrm{if $s_i=t_j$} \\
       2 & \textrm{otherwise}
     \end{array} \right.

   \end{array}
\]

For example, with these costs, the distance between \textit{sick} and
\textit{tick} is two---a substitution of t for s. The distance between
\textit{dog} and \textit{dog} is zero. The distance between
\textit{dog} and \textit{cat} is six because none of the characters
correspond, so three insertions and three deletions are needed to
convert \textit{dog} to \textit{cat}. Not all calculations are so
easy. The distance between \textit{realty} and \textit{reality} is a
single insertion. The distance between \textit{nonrelated} and
\textit{words} is 9, not 15 ($10 + 5$), because the characters `o-r-d'
are in correspondence. While a linguist can easily learn to calculate Levenshtein
distance, an algorithm does not learn and must have a systematic way
to choose the best alignment.

To do this, Levenshtein's algorithm must provide a way to build up
the distance between two strings based on smaller substrings.
This leads to a recursive algorithm in which Levenshtein distance
between two strings is defined in terms of the cheapest of three
possible combinations of source/target substrings plus an operation to
produce the two strings. Let us take as an example the distance
between the source ``cab'' and the target ``cob''. The three cases are
as follows:

\begin{enumerate}
\item Insertion of `b' to ``ca'' to produce
``cob''. Insertion costs 1, and the distance between ``ca'' and
``cob'' is 3, so the cost of this case is 4.
\item Deletion of `b' from ``cabb'' to produce ``cob''. Deletion costs
  1, and the distance between ``cabb'' and ``cob'' is 3, so the cost
  of this case is 4.
\item Substitution of `b' to `b' in ``ca'' to produce
  ``co''. Substitution of identical characters costs 0, and the distance
  between ``ca'' and ``co'' is 2 because the substitution of `o' for
  `a' already cost 2. The resulting cost of this case is 2.
\end{enumerate}

For this example, the substitution case is the cheapest. It is notable
that the substrings in each of the three cases can be derived for any
pair of source/target strings. There is one case for each of the
operations specified above. This leads to the following formal
definition of Levenshtein's distance algorithm.

For each character $s_i \in S$ and $t_j \in T$ for any word $S$ and $T$,
\[ \begin{array}{rl}
 levenshtein(s_i,t_j) = minimum( & ins(s_i)+levenshtein(s_{i-1},t_j),\\
& del(t_j)+levenshtein(s_i,t_{j-1}, \\
& sub(s_i,t_j)+levenshtein(s_{i-1},t_{j-1}))
   \end{array}
\]
The total distance is $levenshtein(S_{|S|},T_{|T|})$.

This recursive definition of distance is most efficiently realised by
storing previous distances in a table. In addition, the table make a
nice visualisation of the algorithm's operation.
For example, finding
the Levenshtein distance from ``ART''
to ``CAT'' creates the table in figure \ref{art2cattable}. In this
table, the substring cases for ``ART'' and ``CAT'' can be found in the
square above for insertion, the square to the left for deletion and
the diagonal above-left square for substitution.
In this
table, insertion corresponds to a downward move, deletion to a
rightward move, and substitution a diagonal move. The total
Levenshtein distance is found at the bottom-righthand corner, but the
distances to all intermediate forms are stored in the table as
well. The intermediate form ``CATART'', for example, is obtained by
inserting three times without any deletions or substitutions. Its
cost of 3 is found at the bottom of the first column.

\begin{figure}
\caption{The distance table for ``ART'' to ``CAT''}

\begin{center}
\begin{tabular}{c|c|c|c|c}
%\hline
  &   & A & R & T \\
\hline
  & $\mathbf{0}$ & 1 & 2 & 3 \\
\hline
C & $\mathbf{1}$ & 2 & 3 & 4 \\
\hline
A & 2 & $\mathbf{1}$ & $\mathbf{2}$ & 3 \\
\hline
T & 3 & 2 & 3 & $\mathbf{2}$
% \hline
\end{tabular}

\end{center}

\label{art2cattable}
\end{figure}

The optimal path is shown in bold. Notice that it
follows the diagonal for free substitutions and propagates either down or right
in their absence. The final distance is two, indicating that two
primitive operators are required; that is, two insertions or deletions.
In fact, the table gives
them: insert `C' to obtain ``CART'', moving down in the table; then
delete `R' to obtain ``CAT'', moving left in the table. `A' and `T'
are common to both words and both produce a diagonal move.

% example here
% then mention Gersic again (remember him?)
% then jump over to Dunning and all the language identification work
% that happened in the early 90s
\subsubsection{Heeringa}
\label{levmethod}
Variations on this theme have been explored, and best described by
\namecite{heeringa04}. Starting with \cite{nerbonne97}, he augmented
the distance definition so that substitution cost was based on the
number of phonological features that differ between segments. This provides more
precision and is based on phonological theory. He also tried
weighting features by information gain.  In his dissertation,
\cite{heeringa04}, he tried a more phonetic method when he defined the
substitution cost between segments as the total distance between the
first two formants, as measured in barks. However, because he did not
have access to the original speech of the dialect data he was
measuring, this substitution cost was uniform across all instances of
a particular segment.

The most direct way to refine Levenshtein distance to take advantage
of linguistic knowledge is by changing the definitions of $ins$,
$del$, and $sub$ to take into account phonetic and phonological
properties of segments. When segments are treated as feature bundles
instead of merely being atomic, \namecite{nerbonne97} propose that the
substitution distance between two segments simply be the number of
features that are different. Two identical segments will therefore
have a substitution distance of zero; segments phonetically similar
will have a small distance. For example, [k] and [g] would have a
distance of one in this system. \namecite{shackleton07} further uses
numeric rather than binary features, which allows for relative
weighting of features, so that [k] and [t] are more similar than [k]
and [p].

The features used in this paper are those used by Shackleton. The
feature set defines different features for vowels, consonants and
vowel-following rhotics. The features are given in table
\ref{featureset}.

\begin{table}
\begin{tabular}{c|lr}
Vowel & Height & 1.0 -- 7.0 \\
  & Backing & 1.0 -- 3.0 \\
  & Rounding & 1.0 -- 2.0 \\
  & Length & 0.5 -- 2.0 \\ \hline
Consonant & Fricative & 0.0 -- 1.0 \\
  & h/wh & 0.0 -- 1.0 \\
  & Glottal  Stop &0.0 -- 1.0 \\
  & Velar & 0.0 -- 2.0 \\
  & Other & 0.0 -- 1.0 \\ \hline
Rhotic & Place & 1.0 -- 3.0 \\
  & Manner & 0.0 -- 4.0 \\
\end{tabular}
\caption{Feature Set used by Shackleton (2007)}
\label{featureset}
\end{table}

Although it increases precision, feature-based substitution causes a
number of complications. The first is that substitution distance
($sub$) becomes complicated since vowels and consonants do not share
features.  Here, the difference must be the number of unshared
features, so vowels and consonants always have a distance of 9, given
the feature set in table \ref{featureset} give consonants 5 features
and vowels 4.  If both segments are consonantal or vocalic, the
individual feature differences are summed.  For example, [a] and [e]
has the following value: \[ |3-3| + |3-2| + |0-0| + |0-0| = 1 \]
TODO:Check this in interactive mode.

The second complication is obtaining definitions for $ins$ and $del$
once $sub$ is defined. It would be best to retain the original
proportions---substitution should cost twice as much as insertion and
deletion. To deal with substitution's variable cost, then, insertion and
deletion should be averages. To find the average substitution cost, one can
 take the average cost of substituting every character
for every other character. Then $ins$ and $del$ return half of this
average. With these three functions defined as below, the table-based algorithm
given above can combine feature distances to find the minimum word distance.

%%% NEW ok (and hence buggy) %%%
\[ \begin{array}{l}
   ins(t_j) = \overline{sub} / 2 \\
   del(s_i) = \overline{sub} / 2 \\
   sub(s_i,t_j) = \left\{
     \begin{array}{ll}
       |s_i|+|t_j| & \textrm{if $C(s_i) \ne C(t_j)$} \\
       \sum_{f_s,f_t \in s_i \times t_j} |f_s - f_t| & \textrm{otherwise}
     \end{array} \right.

   \end{array}
\]

Here, $f_s$ and $f_t$ are the features of the segments $s_i$ and $t_j$
being compared, $C$ is the consonantal property of a segment, and
$\overline{sub}$ is the average substitution cost, defined below.

\[ \overline{s_{ij}} = \frac{\sum_{i \in S}\sum_{j \in S}{levenshtein(s_i,s_j)}}{|s_i||s_j|}\]

\[ \overline{sub} = \sum_{i \in S}\sum_{j \in S}levenshtein(s_i,s_j) / \overline{s_{ij}} \]

Finally, multiple speakers per region can be compared by comparing
every speaker from the first region to every speaker from the second
region and averaging the distances.
TODO:Not finished, and equations are extraordinarily vague.


\subsection{Syntactic  Distance}
% 1st sentence is awkward (and 2nd too a little)
Recently some work in syntax has begun as well. The first steps in
this were Nerbonne and Wiersma's 2006 analysis of Finnish L2 learners
of English, followed by Sanders' 2007 analysis of British dialect
areas. Syntax distance must be approached in quite a different way than
phonological distance. First, syntactic data is extractable from raw
text, so there is usually much more available. But this implies an
associated drop in manual linguistic processing of the
data.

% this paragraph explains the difference between syntactic corpora and
% phonological corpora, badly.
Phonological data is pre-processed, unavoidably, by the perceptual
system of the transcriber. This important step reduces the information
in a complex phonetic waveform to a few binary features. While
phonological transcriptions are easier to work with, they may exhibit bias
from the transcriber---some dialect surveys have been found to exhibit
``field worker dialects'' (cite this, probably from Trudgill, and give
an example) in which
boundaries between regions occur based on the transcriber. A
similar bias can occur when text is hand-parsed, but with a
large enough corpus, one can use an automatic parser that should have
fewer biases, albeit at the cost of more error.

% This paragraph need an introducing paragraph
Finally, if computational methods are duplicating the results of
previous work, that's great, but it would be even better to have a
way of finding relationships between
the results of the various methods; this would tell us how well the
areas cohere at various linguistic levels, which is typically done by
hand or not at all by traditional dialectologists.

\subsubsection{Nerbonne and Wiersma}
\label{nerbonne06}
TODO:These next few pages are divided into way too many sections.

Syntax distance has been much less explored. At present, there is
really only one method used. Due to the lack of alignment between the
larger corpora available for syntactic analysis, a statistical
comparison of differences in more appriopriate than the simple
symbolic approach possible with the word-aligned corpora used in
phonology. Because of the lack of alignment, a syntactic distance
measure will have to use counting as its basis by default.

\namecite{nerbonne06} was an early method proposed for syntactic
distance.  It models syntax by part-of-speech (POS) trigrams and uses
the trigram types in a permutation test of significance. This method was
extended by \namecite{sanders07}, who used \quotecite{sampson00}
leaf-ancestor paths as the basis for building the model instead.

The heart of the measure is simple: the difference in type counts
between the combined types of two corpora. \namecite{kessler01}
originally proposed this measure, the {\sc Recurrence}
metric ($R$) given in equation \ref{rmeasure}. To account for differences
in corpus size, repeated sampling is used. In addition, the samples
are normalised to account for differences in sentence length.
Unfortunately, even normalised, the measure doesn't indicate whether
its results are significant; a permutation test is needed for that.

\begin{equation}
R = \Sigma_i |c_{ai} - c_{bi}|
\label{rmeasure}
\end{equation}

Given two corpora $a$ and $b$, $c_a$ and $c_b$ are the type counts.
$R$ is designed to represent the amount of variation exhibited by the two corpora.
TODO:This sounds slightly wrong. Steal from acl07.tex


% Other ideas include training a
% model on one area and comparing the entropy (compression) of other
% areas. At this point it's unclear whether this would provide a
% comparable measure, however.

\subsubsection{Language models}
Part-of-speech (POS) trigrams are quite easy to obtain from a syntactically
annotated corpus. \namecite{nerbonne06} argue that POS trigrams
can accurately represent at least the important parts of syntax,
similar to the way chunk parsing can capture the most important
information about a sentence. POS trigrams can either be generated by
a tagger as Nerbonne and Wiersa did, or taken from the leaves of
the trees of a parsed corpus.

On the other hand, it would be nice to directly represent the upper
structure of trees. \quotecite{sampson00} leaf-ancestor paths provide one way to
do this: leaf-ancestor paths produce for each leaf in the tree the
path from that leaf back to the root. Sampson originally developed
leaf-ancestor paths as an improved measure of similarity between
gold-standard and machine-parsed trees, to be used in evaluating
parsers. The basic idea of a collection of features that capture
distance between trees transfers quite nicely to our
work. \namecite{sanders07} replaced POS trigrams with leaf-ancestor
paths for the ICE corpus and found improved results on larger
corpora. However, smaller corpora are less likely to attain significance
compared to POS trigram features.

Leaf-ancestor paths are simple as long as every sibling is unique. For
example, the parse tree
\[\xymatrix{
  &&\textrm{S} \ar@{-}[dl] \ar@{-}[dr] &&\\
  &\textrm{NP} \ar@{-}[d] \ar@{-}[dl] &&\textrm{VP} \ar@{-}[d]\\
  \textrm{Det} \ar@{-}[d] & \textrm{N} \ar@{-}[d] && \textrm{V} \ar@{-}[d] \\
\textrm{the}& \textrm{dog} && \textrm{barks}\\}
\]
creates the following leaf-ancestor paths:

\begin{itemize}
\item S-NP-Det-The
\item S-NP-N-dog
\item S-VP-V-barks
\end{itemize}

For identical siblings, brackets must be inserted in the path to
disambiguate the first sibling from the second. The details are
incidental to the main idea however, since identical siblings are a
rare occurrence in any case. (Note: I haven't checked :)
% Another idea is supertags rather than leaf-ancestor paths. This is
% quite similar but might work better.

\subsubsection{Permutation test}
\label{permutationtest}

A permutation test detects whether two corpora are significantly
different. It does this on the basis of the R measure described in
section \ref{nerbonne06}. The test first calculutes $R$ between
samples of the two corpora. Then the corpora are mixed together and
$R$ is calculated between two samples are drawn from the mixed
corpus. If the two corpora are different, $R$ should be larger between
the samples of the original corpora than $R$ from the mixed
corpus. Any real differences will be randomly redistributed by the
mixing process, lowering $R$ of samples. Repeating this comparison
enough times will show if the difference is significant. Twenty times
is the minimum needed to detect significance for $p < 0.5$
significance, but I repeat the test one thousand times in my
experiments.

For example, assume that $R$ detects real differences between London
and Scotland such that $R(\textrm{London},\textrm{Scotland}) =
100$. The permutation test then mixes the London and Scotland to
create LSMixed. Since the real differences are now mixed randomly, we
would expect $R(\textrm{LSMixed}, \textrm{LSMixed}) < 100$, something
like 90 or 95. This should be true at least 95\% of the time if the
differences are significant.

\subsection{Normalisation}
Afterward, the distance must be normalised to account for two things:
the length of sentences in the corpus and the amount of variety in the
corpus. If sentence length differs too much between corpora, there
will be consistently lower token counts in one corpus, which would
cause a spuriously large $R$. In addition, if one corpus has less
variety than the other, it will have inflated type counts, because
more tokens will be allocated to fewer types. To avoid
this all tokens are scaled by the average number of types per token
across both corpora: $2n/N$ where $n$ is the type count and $N$ is
the token count. The additional factor $2$ is necessary because we are
recombining the tokens from the two corpora.

% this is the technical explanation which I think Sandra wants me to
% leave out. Probably. Maaybe it was just the past history.
% So the counts
% are re-allocated by relative sizes of two corpora. (More explanation
% and an equation is needed here or perhaps later).

\section{Experiment}
The two methods described in the previous section, Levenshtein
distance for phonology and a permutation test based on {\sc
  Recurrence} for syntax, were used to analyse British dialect data.

These data come from two sources. The definitive phonological data
source is the Survey of English Dialects (SED) \cite{orton63}, dialect
data collected in the 1950s. This corpus consists mostly of
phonological features of older rural speakers, aiming to capture
dialect boundaries that would erode and mutate rapidly in the last
half of the 20th century. Recently, \namecite{shackleton07} carried
out a comprehensive computational study of the SED data. I used his
data set organised by Government Office Region rather than county.

For syntax, I used the International Corpus of English (ICE)
\cite{nelson02}. The ICE is a corpus of syntactically annotated
conversations and speeches collected in the 1990s. The speakers with a
place of birth recorded were used to form the data set. In order to
have enough speakers in each corpus, Government Office Regions were
used to group the speakers instead of counties. Nine Government Office
Regions form England; Scotland and Wales each count as a single region
in this grouping. Only English data are available in the SED, so the
final comparison does not include Scotland and Wales. However, they
are shown in the intermediate syntactic results.

% \begin{enumerate}
% \item \quotecite{nerbonne97} refinement, without information gain weighting.
% \item \quotecite{nerbonne06} syntax distance.
% \item Combined on the data gathered and analysed in \namecite{shackleton07}.
% \item And the ICE data (with leaf-ancestor paths) of \namecite{sanders07}.
% \item And dividing both sets of data according to the same Government
%   Office regions.
% \end{enumerate}

\subsection{Levenshtein Distance Experiment}

\namecite{shackleton07} chose a 55-word subset of the words elicited
by the SED survey. The subset contains only vowels and consonants
known to vary in England. Using this data set, I grouped the speakers
in the survey into their respective Government Office Regions (GORs)
and compared all speakers of a region to all speakers of every other
region. The distances were averaged as described in section
\ref{levmethod}. This produced a fully connected graph of the nine
English GORs, with the edges weighted by phonological
distance. TODO: A small illegible figure would be nice here.

\subsection{Syntax Distance Experiment}

Starting from the GOR-grouped ICE corpus, I produced trigrams and
leaf-ancestor paths from the trees. Then I chose varying parameter
settings, varying sample size between 500 and 1000, $R$ measure
between $r$ and $r^2$, and input data type between trigram and
leaf-ancestor path. To test that the results from a particular
parameter settings were valid, I ran a preliminary test of each one on London and
Scotland. First, I compared London and Scotland, which, according to
existing linguistic knowledge, should be quite different.
(TODO:Cite this, maybe starting from http://www.abdn.ac.uk/~enl038/grammar.htm)
Then I mixed them
together and split the mixed corpus into two the size of the
original. If the two locations are properly identified as different,
London and Scotland should be significantly different with $p < 0.05$, while the
shuffled doppelgangers should not be.

This was true for only one parameter setting: distance measure $r$,
sample size of 1000 and input data of leaf-ancestor paths. For the
rest London and Scotland were not significantly different, indicating
that the corpora were too small. A few parameter settings were almost
significant, with $p < 0.10$, however. See the discussion for more details.
As a result, I ran the rest of the experiment only for the parameter
setting that produced significant results. Here, all but 7 of the 55
connections between the 11 GORs were significant.
TODO: Check this and prepare the graph for inclusion in the results
section.

\subsection{Correlation}
Correlation of phonological distance and syntactic distance is
difficult because the significance test for syntactic distance is a
binary value. Its coarseness does not provide enough information
compared to the numeric value produced by Levenshtein distance.
It is possible to use the $R$ value for comparison between regions
that were found to be significantly different by the permutation
test. I'm not sure this is a valid comparison.

Alternatively, I could just look at the region pairs that fail to
achieve significance in the syntactic permutation test and check to
see if their phonogical distance is lower than the other pairs. I
don't do this (yet).

\section{Results}

\begin{figure}
  \includegraphics{sed_dendrogram}
\caption{Phonological distance cluster}
\label{phonology-dendrogram}
\end{figure}

Levenshtein distance nicely clusters the GORs into two main branches,
North and South, shown in figure \ref{phonology-dendrogram}. In the
North, Yorkshire and Northwest clustered fairly tightly, followed the
East Midlands and the Northeast. In the South, East England, London
and the Southeast formed a cluster that likely reflects the greater
London area, followed by the Southwest and West Midlands.

The permutation test showed that distances between all regions were
significant except between the regions in figure
\ref{syntax-nonsig}. Corpus size correlates weakly negatively with
significance ($r = -0.72$). Size is obviously a factor, though not the
only one. Perhaps sampling with replacement on smaller corpora
exaggerate differences through repetition. Larger corpora contain more
variety and so will be less likely to appear significantly different.

\begin{table}
\begin{tabular}{cc}
  Yorkshire & East Midlands \\ \hline
  East Midlands & London \\\hline
  London & Southeast \\\hline
  Southeast & Southwest \\\hline
  Northwest & London\\\hline
  Northwest & Southeast \\\hline
\end{tabular}
\caption{Regions with no significant $R$ difference}
\label{syntax-nonsig}
\end{table}

\begin{figure}
  \includegraphics{ice_dendrogram}
\caption{Syntactic distance cluster}
\label{syntax-dendrogram}
\end{figure}

The dendrogram produced by clustering R distance (figure
\ref{syntax-dendrogram} is harder to
interpret. There are two clear groups, but several of the distances
are not significant, such as those forming the tight East
Midlands/London cluster. However, in other cases the grouping will still hold
since there are other distances that are significant being taken into
account. The members of the group are less obvious than those from
phonological distance, too. This could be caused by the 40-year
difference in corpus collection date or by noise from the measurement
method.
TODO: I should really do something like ANOVA here too I think.
TODO: I need to explain hierarchical clustering quickly in the methods
section.

There is generally a North/South distinction in the syntactic
clustering as well, but there are some odd members of each group. In
particular, East England groups with the North and the Northwest
groups with the South. The second is probably erroneous because the
distances between the Northwest and Southeast/London are not
significant, so it's not possible to pinpoint where the Northwest
should, in fact, be grouped.

On the other hand, the correlation between Levenshtein distance and
$R$ is low: -0.096. For the correlation, only the significant
distances were considered.

% OLD
The second (?) question of this paper can now be answered: whether
multiple ways of measuring linguistic distance give the same
answer. As the answers provided by syntax distance are somewhat crude
at this point, the correlation will necessarily be crude as well, but
it should be possible to see if the syntax distance measure at least
agrees with the phonological measure.
%END OLD


\section{Discussion}

On the whole, agreement between phonological and syntactic distance is
weak. Different ways of comparing the distances show differing degrees
of similarity. Both methods show a North/South distinction, but it is
much stronger in phonological distance. The details, however, vary
quite a bit. There are three good explanations for this. First, and
least satisfying, is the possibility that one of the distances is not
measuring what it is supposed to. Second, the corpora may not agree
because of the 40 year difference in age and differing collection
methodologies. Third, syntactic and phonological dialect markers may
not share the same boundaries.

The third possibility is the most interesting because previous work
will not have exposed this difference. Traditional dialectology
focusses on a few features from each collection site. The features
are supposed to be representative; sharp differences between dialects
are supposed to appear in many features and collect into isogloss
bundles.

There are two problems with this. First, phonological data is easier
to analyse, leaving syntactic data as a second-class citizen. For
example, syntax features often has many more occurring variants than
phonological features. Worse, isogloss boundaries only work at a
high-level, such as the North/South boundary in England or the
North/South boundary in German (TODO:might not want to mention this if
I have to find a citation, though Trudgill would probably do). At a
fine scale, the variation is only partially regular and obscured with
pervasive random traits. The complexity is obvious in atlasses based
on the SED data (TODO:Cite Atlas of English Dialects)---many features
have a regular distribution, but one which correlates only weakly with
other features' distributions. Isogloss bundles, needed to draw
dialect boundaries, only appear in a few places.

Computational analysis, such as
\cite{shackleton07}, captures feature variation precisely using
statistical analysis. The resulting complexity clearly shows the
inaptitude of isogloss bundles to capture it. Gradients are a better
way to find dialect areas--often only one or two features will serve
as a boundary between two dialects (see Shackleton, or Nerbonne or
basically anybody). Only particularly steep gradients will show
up as bundles of isoglosses.

Unfortunately, computational research on syntax has been
lacking. Unlike traditional methods, where syntax was a second-class
citizen because of difficulty in analysing using the same methods as
phonology, syntax is completely missing in computational
research. The more complex analytic methods for phonology cannot be applied at all
to syntactic data.

This paper attempts to address that lack, and provide some first steps
to show whether syntax and phonology assist each other in establishing
dialects, or whether they are not related at all. If they are not
related, and syntactic gradients can be as weak as phonological ones,
then some new dialect areas may be apparent that were not visible in
previous phonology-onlyl analyses.
TODO: Mention this at the beginning of the paper too. I don't think
it's clear there.

\subsection{Future Work}

This paper is, however, only a beginning. There are a number of way to
improve upon the work presented here. The obvious criticism of the
syntax distance used here is that it requires so much data that the
results are no more detailed than those available from traditional
methods. Its precision lags phonological distance methods badly. The
first priority should be a syntax distance method that works with
smaller corpora. This would allow me to re-run the current experiment
on counties rather than Government Office Regions, resulting in more
precise induction of dialect areas.

Another problem with the current study is the 40-year difference in
collection dates between the phonological corpus and the syntactic
corpus. A recent phonological corpus would likely show the same sort
of changes in the North/South divide that show up in the syntactic
corpus. The British population became more mobile during the second
half of the 20th century, and the SED survey explicitly attempted to capture
the dialects that existed before this happened. Also it would be nice to have at
least Scotland and Wales for comparison too.

One interesting question is
how phonological and syntactic distances correlate with geographic
distance---\namecite{gooskens04a} show that often the correlation is
very good. This would also allow better visualisation of dialect areas
than a hierarchical dendrogram.


%   \item Some ideas include using supertags instead of leaf-ancestor
%     paths, perhaps automatically extracted from the manual parse.
%   \item Another idea is comparing entropy by compression. This seems
%     pretty promising to me.

% Notes:
% Apparently one Clive Upton has something resembling a British dialect
% atlas that could be used the same way that Nerbonne and Heeringa
% did. But I wonder why they didn't already do it---Norwegian seems much
% less likely than English simply because one {\em expects} it to have
% larger corpora.

% Actually the wikipedia has a nice overview of the survey project and links to
% the resulting
% works.

% \verb+http://en.wikipedia.org/wiki/Survey_of_English_Dialects+ ; It is
% problematic in that the data were collected in the 50s from NORMs
% because they were interested in the traditional dialects, not the new
% ones that might arise.

% Voices is in progress and will eventually provide a modern dialect survey.
% 1978. The linguistic atlas of England, edited by Orton, John
% Widdowson and Clive Upton. (probably the best) : PE1705.L56

% 1993. Survey of English Dialect: The Dictionary and Grammar :
% PE1704.U65 1994
% 1996. An Atlas of English Dialects. 2nd edn. London: Routledge [with
% J.D.A.Widdowson]. : PE1705.U68 1996
% 1994: PE1704.U65 1994

\bibliographystyle{robbib} % ACL is \bibliographystyle{acl}
\bibliography{central}
\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
