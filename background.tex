\chapter{Introduction}
% TODO: Probably just copy the abstract+conclusion in here and merge
% the two...
This dissertation examines syntax distance in dialectometry using
computational methods as a basis. It is a continuation of my previous
work \cite{sanders07}, \cite{sanders08b} and earlier work by
\namecite{nerbonne06}, the first computational measure of syntax
distance. Dialectometry has existed as a field since
\namecite{seguy73} and is a sub-field of dialectology
\cite{chambers98}; recently, computational methods have come to
dominate dialectometry, but they are limited in focus compared to
previous work; most have explored phonological distance only, while
earlier methods integrated phonological, lexical, and syntactic data.

Dialectology is the study of linguistic variation. % in space / over
% distance / other variables.
Its goal is to characterize the linguistic features that
separate two language varieties. Dialectometry is a subfield of
dialectology that uses mathematically sophisticated methods to extract
and combine linguistic features. In recent years it has
been associated with computational linguistic work, most of which
has focused on phonology, starting with
\namecite{kessler95}, followed by \namecite{nerbonne97} and
\namecite{nerbonne01}. \namecite{heeringa04} provides a comprehensive
review of phonological distance in dialectometry as well as some new
methods.

In dialectometry, a distance measure can be defined in two parts:
first, a method of decomposing the linguistic data into minimal,
linguistically meaningful features, and second, a method of combining
the features in a mathematically and linguistically sound way. Figure
\ref{abstract-distance-measure-model} gives an overview of how the
model works. Input consists of two corpora; each item in each corpus
is decomposed into a set of features extracted by $f(s)$. The
resulting corpora are then compared by $d(S,T)$, which combines the
corpora into a single number: the distance.

\begin{figure}
\[\xymatrix@C=1pc{
 \textrm{Corpus} \ar@{>}[d]|{f(s)} &
  S = s_o,s_1,\ldots
  \ar@{>}[d] % \ar@<2ex>[d] \ar@<-2ex>[d]
  &&
  T = t_o,t_1,\ldots
  \ar@{>}[d] % \ar@<2ex>[d] \ar@<-2ex>[d]
  \\
 *\txt{Decomposition} \ar@{>}[d]|{d(S,T)} &
 *{\begin{array}{c}
     \left[ + f_o, +f_1 \ldots \right], \\
     \left[ - f_o, +f_1 \ldots \right], \\
     \ldots \\ \end{array}}
 \ar@{>}[dr]
 &&
 *{\begin{array}{c}
     \left[ + f_o, -f_1 \ldots \right], \\
     \left[ + f_o, -f_1 \ldots \right], \\
     \ldots \\ \end{array}}
 \ar@{>}[dl]  \\
 \textrm{Combination} &
 & \textrm{Distance} & \\
} \]
\label{abstract-distance-measure-model}
\caption{Abstract Distance Measure Model : $f \circ d$}
\end{figure}

Dialectometry has focused on phonological distance measures, while
syntactic measures have remained undeveloped. The most important
reason for this focus is that it is easier to define a distance
measure on phonology. In phonology, it is easy to collect corpora
consisting of identical word sets. Then these words decompose to segments and,
if necessary, segments further decompose to phonological
features. This decomposition is straightforward and based on
\namecite{chomsky68}. For combination, string alignment, or Levenshtein
distance \cite{lev65}, is a well-understood algorithm used for
measuring changes between any two sequences of characters taken from a
common alphabet. Levenshtein distance is simple mathematically, and
has the additional advantage that its intermediate data structures are
easy to interpret as the linguistic processes of epenthesis, deletion and
metathesis. These things are not possible
with syntactic distance: neither matched sentence collections nor
straightfoward functions for decomposition and combinations.

A secondary reason for dialectometry's focus on phonology is that it
is inherited from dialectology's focus on phonology.
% (TODO:Cite?)
This might be solely due to the history of dialectology as a field, but it is
likely that more phonological than syntactic differences exist between
dialects, due to historically greater standardization
of syntax via the written form of language. Phonological
dialect features are less likely to be stigmatized and suppressed by a
standard dialect than syntactic ones.
% (TODO:Cite, probably
% Trudgill and Chambers something like '98, maybe where they talk about
% what aspects of dialects are noticed and stigmatized).
Whatever the reason, much less dialectology work on syntax is
available for comparison with new dialectometry results.

\subsection{Problems}

Because of the preceding two reasons, syntax is a relatively
undeveloped area in dialectometry. Currently, the literature lacks a
generally accepted syntax measure. Unfortunately, approaching the
problem by copying phonology is not a good solution; there are real
differences between syntax and phonology that mean phonological
approaches do not apply. For example, there are fewer differences to be
found in syntax, and they occur more sparsely.
% (TODO: Back this up either with reasoning or citation).
However, dialectology has traditionally worked with fairly small
corpora. This suffices for phonology, because
it is easy to extract good features and there are many
consistent differences between corpora. For syntax, though, it is not possible
% (TODO: Weasel a bit)
to identify reliable features in small corpora.

There are two approaches that have been proposed to remedy this. The
first, proposed by \namecite{spruit08} for analyzing the Syntactic
Atlas of the Dutch Dialects \cite{barbiers05}, is to continue using
small dialectology corpora and manually extract features so that only
the most salient features are used. Then a sophisticated method of
combination such as Goebl's Weighted Identity Value (WIV), described
below and by \namecite{goebl06}, can be used to produce a
distance. WIV is more complex mathematically than Levenshtein
distance, and operates on any type of linguistic feature. However, manual feature
extraction is not feasible in knowledge-poor or time-constrained
environments. It is also subject to bias from the
dialectologist. Since the best manually extracted features are those that capture
the difference between two dialects, the best-known features are most
likely to become the best manual features, passing over the rarely
occurring and obscure features that might actually be the best
indicators of a particular dialect.

This approach ignores the specific properties of the syntax distance
problem. It is easy to define features for syntactic structure. This
proposal covers part-of-speech trigrams, leaf-ancestor paths, and
dependency paths over nodes, but many variations on these features are
possible, such as lexical trigrams, lexicalized leaf-ancestor paths,
or dependency paths over dependency arc labels. Methods from other
syntactic work in computational linguistics could apply too: supertags
\cite{joshi94}, convolution kernels \cite{collins01} or any number of
simpler features such as tree height, number of nodes, or number of
words. The problem is not finding a feature set. The problem is
finding a good feature set. Small corpora hamper this search by making
statistical significance difficult to achieve, especially since
syntactic dialect differences are expected to be less frequent than
phonological ones. Fortunately, syntactic corpora are typically larger
than phonological corpora because the annotation work is easier; much
of the syntactic annotation can be generated automatically and then
corrected manually.

Even with a feature set defined, a distance measure still requires a
method of combining features. One such method, a simple statistical
measure called $R$, has been proposed by \namecite{nerbonne06} based
on work by \namecite{kessler01}. At present, however, $R$ has not been
adequately shown to detect dialect differences. A small body of work
suggests that it does, but as yet there has not been a satisfying
correlation of its results with phonology or, as with phonological
distance, with existing results from the dialectology literature on
syntax.

Nerbonne \& Wiersma's first paper used part-of-speech trigram features
as a proxy for syntactic information and $R$ for syntax distance
together with a test for statistical significance\cite{nerbonne06}.
Their experiment compared two generations of Norwegian L2 speakers of
English.  They found that the two generations were significantly
different, although they had to normalize the trigram counts to
account for differences in sentence length and complexity. However,
showing that two generations of speakers are significantly different
with respect to $R$ does not necessarily imply that the same will be
true for other types of language varieties. Specifically, for this
dissertation, the success of $R$ on generational differences does not
imply success on dialect differences.

I addressed this problem \cite{sanders08b} by measuring $R$ between
the nine Government Office Regions of England, using the International
Corpus of English Great Britain \cite{nelson02}. Speakers were classified by
birthplace. I also introduced Sampson's leaf-ancestor paths as
features \cite{sampson00}. I found statistically
significant differences between most corpora, using both trigrams and
leaf-ancestor paths as features. However, $R$'s distances were not
significantly correlated with Levenshtein distances. Nor did I
show any qualitative similarities between known syntactic dialect
features and the high-ranked features used by $R$ in producing its
distance. As a result, it is not clear whether the significant $R$ distances
correlate with dialectometric phonological distance or with known
features found by dialectologists.

% NOTE: 2-d stuff is not the primary problem, since we can't compare
% trees to trees anyway. The primary problem is comparing two corpora
% full of differing sentences. A secondary problem arises to make sure
% that the 2-d-extracted features aren't skewed one way or another. I
% guess I need to come up with a general justification for the
% normalizing and smoothing code from Nerbonne & Wiersma

% Additional problems: phonology is 1-dimensional, with one obvious way
% to decompose words into segments and segments into features. Syntax is
% 2-dimensional, so the decomposition must take several more factors
% into account so that the features it produces are
% useful and comparable to each other. And those features are \ldots

% % TODO Henrik Rosenkvist seems to
% % be the main guy interested in syntactic analysis of dialect distance

% Overview : Goal, Variables, Method
%   Contribution
% Literature Review
%   : (including theoretical background)
%   Draw hypotheses from earlier studies
% Method
%   :
%   Experiment section as 'Corpus' section

% Goal: To extend existing measurement methods. To measure them
% better. To measure them on more complete data.

\section{Dissertation Overview}

How I will solve these problems and test these hypotheses.

Blah blah a few sentences on methods: $R$ and feature extraction and
analysis blah blah.

As regards to Swedish: I use the Swediasyn, Swedish section, to test
the hypotheses on Swedish. [[Describes Swediasyn]]
There has been some syntactic dialectology on
Swedish. [[So far I know of a couple of papers by Rosenkvist.]] It is
not clear whether syntactic features follow the same boundaries as
other features. [[There isn't a whole lot of literature on any
boundaries actually.]]

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
