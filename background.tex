\chapter{Introduction}
This dissertation examines syntax distance in dialectometry using
computational methods as a basis. It is a continuation of my previous
work \cite{sanders07}, \cite{sanders08b} and earlier work by
\namecite{nerbonne06}, the first computational measure of syntax
distance. Dialectometry has existed as a field since
\namecite{seguy73} and is a sub-field of dialectology
\cite{chambers98}; recently, computational methods have come to
dominate dialectometry, but they are limited in focus compared to
previous work; most have explored phonological distance only, while
earlier methods integrated phonological, lexical, and syntactic data.

Dialectology is the study of linguistic variation. % in space / over
% distance / other variables.
Its goal is to characterize the linguistic features that
separate two language varieties. Dialectometry is a subfield of
dialectology that uses mathematically sophisticated methods to extract
and combine linguistic features. In recent years it has
been associated with computational linguistic work, most of which
has focused on phonology, starting with
\namecite{kessler95}, followed by \namecite{nerbonne97} and
\namecite{nerbonne01}. \namecite{heeringa04} provides a comprehensive
review of phonological distance in dialectometry as well as some new
methods.

In dialectometry, a distance measure can be defined in two parts:
first, a method of decomposing the linguistic data into minimal,
linguistically meaningful features, and second, a method of combining
the features in a mathematically and linguistically sound way. Figure
\ref{abstract-distance-measure-model} gives an overview of how the
model works. Input consists of two corpora; each item in each corpus
is decomposed into a set of features extracted by $f(s)$. The
resulting corpora are then compared by $d(S,T)$, which combines the
corpora into a single number: the distance.

\begin{figure}
\[\xymatrix@C=1pc{
 \textrm{Corpus} \ar@{>}[d]|{f(s)} &
  S = s_o,s_1,\ldots
  \ar@{>}[d] % \ar@<2ex>[d] \ar@<-2ex>[d]
  &&
  T = t_o,t_1,\ldots
  \ar@{>}[d] % \ar@<2ex>[d] \ar@<-2ex>[d]
  \\
 *\txt{Decomposition} \ar@{>}[d]|{d(S,T)} &
 *{\begin{array}{c}
     \left[ + f_o, +f_1 \ldots \right], \\
     \left[ - f_o, +f_1 \ldots \right], \\
     \ldots \\ \end{array}}
 \ar@{>}[dr]
 &&
 *{\begin{array}{c}
     \left[ + f_o, -f_1 \ldots \right], \\
     \left[ + f_o, -f_1 \ldots \right], \\
     \ldots \\ \end{array}}
 \ar@{>}[dl]  \\
 \textrm{Combination} &
 & \textrm{Distance} & \\
} \]
\label{abstract-distance-measure-model}
\caption{Abstract Distance Measure Model : $f + d$}
\end{figure}

Dialectometry has focused on phonological distance measures, while
syntactic measures have remained undeveloped. The most important
reason for this focus is that it is easier to define a distance
measure on phonology. In phonology, it is easy to collect corpora
consisting of identical word sets. Then these words decompose to segments and,
if necessary, segments further decompose to phonological
features. This decomposition is straightforward and based on
\namecite{chomsky68}. For combination, string alignment, or Levenshtein
distance \cite{lev65}, is a well-understood algorithm used for
measuring changes between any two sequences of characters taken from a
common alphabet. Levenshtein distance is simple mathematically, and
has the additional advantage that its intermediate data structures are
easy to interpret as the linguistic processes of epenthesis, deletion and
metathesis. These things are not possible
with syntactic distance: neither matched sentence collections nor
straightfoward functions for decomposition and combinations.

A secondary reason for dialectometry's focus on phonology is that it
is inherited from dialectology's focus on phonology.
% (TODO:Cite?)
This might be solely to due to the history of dialectology as a field, but it is
likely that more phonological than syntactic differences exist between
dialects, due to historically greater standardization
of syntax via the written form of language. Phonological
dialect features are less likely to be stigmatized and suppressed by a
standard dialect than syntactic ones.
% (TODO:Cite, probably
% Trudgill and Chambers something like '98, maybe where they talk about
% what aspects of dialects are noticed and stigmatized).
Whatever the reason, much less dialectology work on syntax is
available for comparison with new dialectometry results.

\subsection{Problems}

Because of the preceding two reasons, syntax is a relatively
undeveloped area in dialectometry. Currently, the literature lacks a
generally accepted syntax measure. Unfortunately, approaching the
problem by copying phonology is not a good solution; there are real
differences between syntax and phonology that mean phonological
approaches do not apply. For example, there are fewer differences to be
found in syntax, and they occur more sparsely.
% (TODO: Back this up either with reasoning or citation).
However, dialectology has traditionally worked with fairly small
corpora. This suffices for phonology, because
it is easy to extract good features and there are many
consistent differences between corpora. For syntax, though, it is not possible
% (TODO: Weasel a bit)
to identify reliable features in small corpora.

There are two approaches that have been proposed to remedy this. The
first, proposed by \namecite{spruit08} for analyzing the Syntactic
Atlas of the Dutch Dialects \cite{barbiers05}, is to continue using
small dialectology corpora and manually extract features so that only
the most salient features are used. Then a sophisticated method of
combination such as Goebl's Weighted Identity Value (WIV), described
below and by \namecite{goebl06}, can be used to produce a
distance. WIV is more complex mathematically than Levenshtein
distance, and operates on any type of linguistic feature. However, manual feature
extraction is not feasible in knowledge-poor or time-constrained
environments. It is also subject to bias from the
dialectologist. Since the best manually extracted features are those that capture
the difference between two dialects, the best-known features are most
likely to become the best manual features, passing over the rarely
occurring and obscure features that might actually be the best
indicators of a particular dialect.

This approach ignores two important aspects of the problem of syntax
distance. First, syntactic structure is easily decomposed into
features; a decomposition function is easy to create, but it is not
obvious which is the best.
Second, large syntactic corpora are
available for many languages. But with large corpora, extraction of specific
syntactic features from carefully elicited sentences is not
possible. Fortunately, the drawbacks of these two aspects cancel each other:
large corpora should provide enough evidence to properly rank
automatically extracted features.
% (traditional dialectology must collect small
% amounts of data, so the much larger search space of syntactic
% variation does not allow significant differences to emerge)

One method of combination, a simple statistical measure called $R$,
was proposed by \namecite{nerbonne06} based on work by
\namecite{kessler01}. At present, however, $R$ has not been adequately
shown to detect dialect differences. A small body of work suggests
that it does, but as yet there has not been a satisfying correlation
of its results with existing results from the dialectology literature
on syntax.

Nerbonne \& Wiersma's first paper used part-of-speech trigram features
as a proxy for syntactic information and $R$ for syntax distance
together with a test for statistical significance\cite{nerbonne06}.
Their experiment compared two generations of Norwegian L2 speakers of
English.  They found that the two generations were significantly
different, although they had to normalize the trigram counts to
account for differences in sentence length and complexity. However,
showing that two generations of speakers are significantly different
with respect to $R$ does not necessarily imply that the same will be
true for other types of language varieties. Specifically, for this
dissertation, the success of $R$ on generational differences does not
imply success on dialect differences.

\namecite{sanders08b} addressed this problem by measuring $R$ between
the nine Government Office Regions of England, using the International
Corpus of English Great Britain \cite{nelson02}. Speakers were classified by
birthplace. Sanders also attempted to improve syntactic feature
quality by introducing Sampson's leaf-ancestor paths as
features \cite{sampson00}. Sanders found statistically
significant differences between most corpora, using both trigrams and
leaf-ancestor paths as features. However, $R$'s distances were not
significantly correlated with Levenshtein distances. Nor did Sanders
show any qualitative similarities between known syntactic dialect
features and the high-ranked features used by $R$ in producing its
distance. As a result, it is not clear whether the significant $R$ distances
correlate with dialectometric phonological distance or with known
features found by dialectologists.

% NOTE: 2-d stuff is not the primary problem, since we can't compare
% trees to trees anyway. The primary problem is comparing two corpora
% full of differing sentences. A secondary problem arises to make sure
% that the 2-d-extracted features aren't skewed one way or another. I
% guess I need to come up with a general justification for the
% normalizing and smoothing code from Nerbonne & Wiersma

% Additional problems: phonology is 1-dimensional, with one obvious way
% to decompose words into segments and segments into features. Syntax is
% 2-dimensional, so the decomposition must take several more factors
% into account so that the features it produces are
% useful and comparable to each other. And those features are \ldots

% % TODO Henrik Rosenkvist seems to
% % be the main guy interested in syntactic analysis of dialect distance

% Overview : Goal, Variables, Method
%   Contribution
% Literature Review
%   : (including theoretical background)
%   Draw hypotheses from earlier studies
% Method
%   :
%   Experiment section as 'Corpus' section

% Goal: To extend existing measurement methods. To measure them
% better. To measure them on more complete data.

\section{Hypotheses}
% TODO: Rewrite and merge the following question/hypothesis paragraph pairs
% H1 - organization is all wrong still
The state of syntax measures in dialectometry described above leaves
several research questions unresolved. The most important for this
proposal is whether $R$ is a good measure of syntax
distance. Specifically, have the ambiguous results of previous
research been a shortcoming of $R$, differences between phonological
and syntactic corpora, or differences between phonological and
syntactic dialect boundaries?

To investigate this, I propose Hypothesis 1: the features found by
dialectologists will agree with the highly ranked features used by $R$
for classification. I will test Hypothesis 1 by comparing $R$'s
results to the syntactic dialectology literature on Swedish. In
addition, Hypothesis 1B states that the regions of Sweden accepted by
dialectology will be reproduced by the $R$. For example, my
previous research on British English reproduced the well-known North
England-South England dialect regions. However, this research will eliminate the
corpus variability in that research \cite{sanders08b} that resulted in
the confounding factors mentioned above, meaning that more precise
results, such as specific identifying features, should be detectable as well.

%H2 - Dad didn't understand that this is other features to be fed into
%R not replacement of R entire.
A secondary question, relevant once a useful syntax distance measure
is established, is what input features cause $R$ to produce the best
results.  Previous work has shown that leaf-ancestor paths provide a
small advantage over part-of-speech trigrams, presumably by capturing
syntactic structure higher in the parse tree. Additional possible
feature sets include variations on the previously investigated
trigrams and leaf-ancestor paths, along with various kinds of backoff,
for example, to bigrams or coarser node tags. Features from dependency
parses may be useful, too, in capturing non-local dependencies that
can be captured neither by trigrams nor leaf-ancestor paths.

Therefore, I propose Hypothesis 2: better input features
for $R$ will produce more accurate syntax
distances. These features can be discovered by comparing performance
of a number of different feature sets on a fixed corpus. In addition,
combinations of successful features will produce even better
performance.
% This sentence is either redundant or should appear earlier.
The quality of a set of features can be
measured by its sensitivity---the number of significant distances it
finds---and the similarity of the highly ranked features $R$ produces
to those found by dialectologists.

% H3
A third question is whether $R$ agrees with phonological distance
measures like Levenshtein distance. Unlike agreement with traditional
dialectology, there is no {\it a priori} reason to expect agreement
between phonology and syntax in delineating dialect
boundaries. However, agreement with phonological distance would be
further evidence for the suitability of $R$ as a syntactic distance
measure.

Therefore, I propose Hypothesis 3: a phonology corpus and syntax corpus
constructed from the same data will provide better correlation between
phonology and syntax distance measures than a phonology corpus and
syntax corpus drawn from different data sources. I will test this
hypothesis by comparing results to my previous work on British English
phonological and syntactic corpora; there, no significant correlation
was found between the regions extracted from the two corpora drawn
from different data. If significant correlation is found by using the
same set of data for both corpora, it indicates that phonology and
syntax boundaries do coincide but that the agreement is weak enough to
be lost when using corpora collected from different populations.

Finally, if $R$ is found to be a bad measure of syntax distance, this
dissertation will propose and evaluate alternative syntactic distance
measures. Specifically, $R$ is one way to aggregate features that are
created by decomposing sentences. It treats features as atomic, and
does not manipulate them in any syntax-specific ways. As such, $R$ is
not much different than Goebl's WIV. This may not be a problem if the
decomposition methods used to generate features adequately capture
dialect differences in independent, atomic features. If dialect
differences cannot be captured by independent, atomic features, then a
more syntax-specific method of combining features will be needed
instead. Alternatively, a more complex statistical measure may be
useful, taking the basic idea of $R$ and increasing its
sensitivity. For example, Kullbeck-Leibler divergence, like $R$,
provides a dissimilarity that is intuitively similar to distance.

\section{Dissertation Overview}

How I will solve these problems and test these hypotheses.

Blah blah a few sentences on methods: $R$ and feature extraction and
analysis blah blah.

As regards to Swedish: I use the Swediasyn, Swedish section, to test
the hypotheses on Swedish. [[Describes Swediasyn]]
There has been some syntactic dialectology on
Swedish. [[So far I know of a couple of papers by Rosenkvist.]] It is
not clear whether syntactic features follow the same boundaries as
other features. [[There isn't a whole lot of literature on any
boundaries actually.]]

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dissertation.tex"
%%% End: 
