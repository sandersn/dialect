\documentclass[11pt]{article}
%\usepackage{setspace}
\usepackage[all]{xy}
\usepackage{robbib}

\author{Nathan Sanders, Indiana University \\ \tt{ncsander@indiana.edu}}
\title{Syntax Distance in Dialectometry : A Convincing Dissertation
  Proposal}
\begin{document}
\maketitle

This proposal proposes to continue my existing line of research on
dialect distance using syntactic features. My previous research has
focussed on statistical measures as pioneered by Nerbonne \& Wiersma
and extended their features to better capture complex syntactic
structure.

\section{Introduction}
% Overview : Goal, Variables, Method
%   Contribution
% Literature Review
%   : (includng theoretical background)
%   Draw hypotheses from earlier studies
% Method
%   :
%   Experiment section as 'Corpus' section

Entice a goat! Provoke a ruffian! Flatter a Swede! All in one game!

Goal: To extend existing measurement methods. To measure them
better. To measure them on more complete data.

Variables: Corpus from speakers of Swedish (and maybe other Nordisk
dialekter), geographical distance, syntax distance from various
measures.

Contribution: Better syntax distance. A better understanding of syntax
distance.

Literature Review: Good here, but maybe more papers need to be
described . Not sure how much I need; I don't want to add back in all
the phonology stuff.

\begin{enumerate}
\item This dissertation is proposed to continue my existing line of research.
\item And that line is\ldots
\item Introduction, cut down from previous version DONE
\item The very start should be bulked up for accessibility. And
  different somethings.
\item And should flatter the Swedes. TODO
\end{enumerate}

This (dissertation (proposal proposes) dissertation) study on syntax
distance in dialectometry using computational methods as a basis. This
is a continuation of my previous work (CITE qualifying paper) and
earlier work by \namecite{nerbonne06}. And other work that I discover
some time this month. Hopefully this month. Computational methods have
come to dominate dialectometry, but they are limited in focus compared
to previous work; most have explored phonological distance only, while
earlier methods integrated phonological, lexical, and syntactic data.

However, the isogloss bundles that were the primary product of
pre-computational methods do not adequately capture gradient
generalizations; irregular boundaries are not useful for this purpose
and have to be discarded. Recent methods are mathematically
sophisticated, capable of identifying dialect gradients. The purpose
of this work is to integrate phonological and syntactic data in the
way that early dialectology did, but use the computational methods
that have previously only been used to analyze one are of linguistics
at a time.

Previous work has investigated statistical syntax distance in two
corpora: different generations of Norwegian L2 speakers of English
\cite{nerbonne06}, and the ICE corpus, consisting of native English
speakers born in various parts of England \cite{nelson02}. In
addition, Sanders' work used two types of features as input to the
statistical classifier: trigrams and leaf-ancestor paths
\cite{sampson00}. Both Nerbonne \& Wiersma and Sanders used a
permutation test based on Kessler's $R$ \cite{kessler01}. Sanders'
work applied the permutation test to multiple areas by dividing the
ICE up by speaker birthplace. This dissertation will be the first to
use this method while juggling AND cycling. I mean, the first to use
this method on a corpus collected with dialect analyses in mind.

Specifically, the Nordisk dialektkorpus. This is the part where I
flatter the Swedes. TODO: Do this.

\section{Early Methods}

\subsection{S\'eguy}

Measurement of linguistic similarity has always been a part of
linguistics. However, until \namecite{seguy73} dubbed a new set of
approaches `dialectometry', these methods lagged behind the rest of
linguistics in formalization. S\'eguy's quantitative analysis
of Gascogne French, while not aided by computer, was the predecessor
of more powerful statistical methods that essentially required the use
of computer as well as establishing the field's general dependence on
well-crafted dialect surveys that divide the incoming data along
traditional linguistic boundaries: phonology, morphology, syntax, etc.
This makes both collection and analysis easier, although it requires
much more work when trying to produce a complete picture of dialect
variation.

The project to build the Atlas Linguistique et Ethnographique de la
Gascogne, which S\'eguy directed, collected data in a dialect survey
of Gascogne which asked speakers questions informed by different areas
of linguistics. For example, the pronunciation of `dog' (``chien'')
was collected to measure phonological variation. It had two common
variants and many other rare ones: [k\~an], [k\~a], as well as [ka],
[ko], [kano], among others. These variants were, for the most part,
% or hat "chapeau": SapEu, kapEt, kapEu (SapE, SapEl, kapEl
known by linguists ahead of time, but their exact geographical
distribution was not.

The atlases, as eventually published, contained not only annotated
maps, but some analysis as well. This analysis was what S\'eguy named
dialectometry. Dialectometry differs from previous attempts to find
dialect boundaries in the way it combines the information from the
dialect survey. Previously, dialectologists looked for isogloss
boundaries for individual items. A dialect boundary is generated when
enough individual isogloss boundaries coincided. However, there is so
much individual variation that only major dialect boundaries can
be captured this way.

S\'eguy reversed the process. He first combined the survey data to get
a numeric score between each site. Then he posited dialect boundaries
where large distances resulted between sites. The difference is
important, because a single numeric score is dramatically easier to
analyze than hundreds of individual boundaries. The outcome is that
much more subtle dialect boundaries are visible this way; where before
one saw only a jumble of conflicting boundary lines, now one sees
smaller, but consistent numerical differences separating regions. {Dialectometry
  enables classification of gradient dialect boundaries, since now one
can distinguish weak and strong boundaries. Previously, weak
boundaries were too uncertain.}

However, S\'eguy's method of combination was simple both
linguistically and mathematically. When comparing two sites, any
difference in a response would be counted as 1. Only identical
responses counted as a distance of 0. Words were not analyzed
phonologically, nor were responses weighted by their relative amount
of variation. Finally, only geographically adjacent sites were
compared. This is a reasonable restriction, but later studies were
able to lift it because of the availability of greater computational
power. Work following S\'eguy's would improve on both aspects. To wit,
Ger\v{s}i\'c linguistically and Goebl mathematically.

TODO:Change this transition to leave out phonology and hence Gersic.

\subsection{Goebl}

Later, Hans Goebl emerged as a leader in the field of dialectometry,
formalizing the aims and methods of dialectometry. His primary
contribution was development of various methods to combine individual
distances into global distances and from there global clusters. These
methods were more sophisticated mathematically than previous
dialectometry and operated with any features extracted from the data. His
analyses have mostly used the Atlas Linguistique de Fran\c{c}ais.

\namecite{goebl06} provides a summary of his work. Most relevant for
this paper are the measures Relative Identity Value and Weighted
Identity Value. They are general methods that are the basis for nearly
all subsequent fine-grained dialectometrical analyses. They have three
important properties. First, they are independent of the source
data. They can operate over any linguistic data for which they are
given a feature set, such as the one proposed by Ger\v{s}i\'c for
phonology. Second, they can compare data even for items that do not
have identical feature sets. This improves on Ger\v{s}i\'c's $d$,
which cannot compare consonants and vowels. Third, they can compare
data sets that are missing some entries. This improves on S\'eguy's
analysis by providing a principled way to handle missing survey
responses.

Relative Identity Value, when comparing any two items, counts the
number of features which share the same value and then discounts
(lowers) the importance of the result by the number of unshared
features. The result is a single percentage that indicates
relative similarity. Calculating this distance between all pairs
of items in two regions produces a matrix which can be used for
clustering or other purposes. Note that the presentation below splits
Goebl's original equations into more manageable pieces; the high-level
equation for Relative Identity Value is:

\begin{equation}
  \frac{\textrm{identical}_{jk}} {\textrm{identical}_{jk} - \textrm{unidentical}_{jk}}
\label{riv}
\end{equation}
For some items being compared $j$ and $k$. In this case
\textit{identical} is
\begin{equation}
  \textrm{identical}_{jk} = |f \in \textrm{\~N}_{jk} : f_j = f_k|
\end{equation}
where $i$ is each feature shared by $j$ and $k$ (called
$\textrm{\~N}_{jk}$). \textit{unidentical} is defined similarly, except
that it counts all features N, not just the shared features
$\textrm{\~N}_{jk}$.
\begin{equation}
  \textrm{unidentical}_{jk} = |f \in \textrm{N} : f_j \neq f_k|
\end{equation}

Weighted Identity Value is a refinement of Relative Identity
Value. This measure stems defines some differences as more
important than others. In particular, more information arises from
feature values that only happen a few times rather than from those values
that characterize a large number of the items being studied.  This
idea shows up later in the normalization of syntax distance given by
\namecite{nerbonne06}.

The mathematical implementation of this idea is fairly simple. Goebl
is interested in feature values that occur only a few times. If a
feature has some value that is shared by all of the items, then all
items belong to the same group. This feature value provides {\it no}
useful information for distinguishing the items.  The situation
improves if all but one item share the same value for a feature; at
least there are now two groups, although the larger group is still not
very informative.  The most information is available if each item
being studied has a different value for a feature; the items fall
trivially into singleton groups, one per item.

Equation \ref{wiv-ident} works by discounting
the \textit{identical} count from equation \ref{riv} by
the amount of information that feature value conveys. The
amount of information, as discussed above, is based on the number of
items that share a particular value for a feature. If all items share
the same value for some feature, then \textit{identical} will be discounted all the
way to zero--the feature conveys no useful information.
Weighted Identical Value's equation for \textit{identical} is
therefore
\begin{equation}
  \textrm{identical} = \sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_{j}}{(Ni)w} & \textrm{if} f_j = f_k
  \end{array} \right.
\label{wiv-ident}
\end{equation}

\noindent{}The complete definition of Weighted Identity Value is
\begin{equation} \sum_i \frac{\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
\end{array} \right.}
  {\sum_f \left\{
  \begin{array}{ll}
    0 & \textrm{if} f_j \neq f_k \\
    1 - \frac{\textrm{agree}f_j} {(Ni)w} & \textrm{if} f_j = f_k
    \end{array} \right. - |f \in \textrm{N} : f_j \neq f_k|}
  \label{wiv-full}
  \end{equation}

  \noindent{}where $\textrm{agree}f_{j}$ is the number of candidates that agree
  with item $j$ on feature $f$ and $Ni$ is the total number of
  candidates ($w$ is the weight, discussed below). Because of the
  piecewise definition of \textit{identical}, this number is always at
  least $1$ because $f_k$ agrees already with $f_j$. The effect of
  this equation is to take the count of shared features and weight
  them by the size of the sharing group. The features that are shared
  with a large number of other items get a larger fraction of the normal
  count subtracted.

  For example, let $j$ and $k$ be sets of productions for the
  underlying English segment /s/. The allophones of /s/ vary mostly on the feature
  \textit{voice}. Seeing an unvoiced [s] is less ``surprising'' than
  seeing a voiced [z] for /s/, so the discounting process should
  reflect this. For example, assume that an English corpus contains 2000
  underlying /s/ segments. If 500 of them are realized as [z], the
  discounting for \textit{voice} will be as follows:

  \begin{equation}
    \begin{array}{c}
      identical_{/s/\to[z]} = 1 - 500/2000 = 1 - 0.25 = 0.75 \\
      identical_{/s/\to[s]} = 1 - 1500/2000 = 1 - 0.75 = 0.25
    \end{array}
    \label{wiv-voice}
  \end{equation}

  Each time /s/ surfaces as [s], it only receives 1/4 of a point
  toward the agreement score when it matches another [s]. When /s/
  surfaces as [z], it receives three times as much for matching
  another [z]: 3/4 points towards the agreement score. If the
  alternation is even more weighted toward faithfulness, the ratio
  changes even more; if /s/ surfaces as [z] only 1/10 of the time,
  then [z] receives 9 times more value for matching than [s] does.

  The final value, $w$, which is what gives the name ``weighted
  identity value'' to this measure, provides a way to control how much
  is discounted. A high $w$ will subtract more from uninteresting
  groups, so that \textit{voice} might be worth less than
  \textit{place} for /t/ because /t/'s allophones vary more over
  \textit{place}. In equation \ref{wiv-voice}, $w$ is left at 1 to
  facilitate the presentation.

\section{Computational Methods}

It is at this point that the two types of analysis, phonological and
syntactic, diverge. Although Goebl's techniques are general enough to
operate over any set of features that can be extracted, better results
can be obtained by specializing the general measures above to take
advantage of properties of the input.  Specifically, the application
of computational linguistics to dialectometry beginning in the 1990s
introduced methods from other fields. These methods, while generally
giving more accurate results quickly, are tied to the type of data on
which they operate.

% NEW
The dominant phonological distance measure is Levenshtein
distance. This distance is essentially the count of differing
segments, although various refinements have been tried, such as
inclusion of distinctive features or phonetic
correlates. \namecite{heeringa04} gives an excellent analysis of the
applications and variations of Levenshtein distance. While Levenshtein
distance provides much information as a classifier, it is limited
because it must have a word aligned corpus for comparison. A number of
statistical methods have been proposed that remove this requirement
(Sanders, Zastrow, earlier types) but none have been as successful on
existing dialect resources, which are small and are already
word-aligned. New resources are not forthcoming because the proposed
methods still rely on a phonetic transcription process. So this is
basically a dead-end.
% end NEW

\begin{enumerate}
\item Also cut phonological approaches here.
\item Reword intro paragraph. (maybe summarise phonology as another
  paragraph after the current intro)
\item I should really check around to see if there is any new work out
  there. Surely there is. Course John is free to do whatever works and
  Wybo may have graduated or something. So there might not be any more
  work on it.
\item Explain leaf-ancestor paths, trigrams, dependency `paths' (to be
  invented).
\item Previous results on English.
\end{enumerate}

\subsection{Syntactic  Distance}
% 1st sentence is awkward (and 2nd too a little)
Recently, computational dialectometry has expanded to analysis of
syntax as well. The first work in this area was \quotecite{nerbonne06}
analysis of Finnish L2 learners of English, followed by
\quotecite{sanders07} analysis of British dialect areas. Syntax
distance must be approached quite differently than phonological
distance. Syntactic data is extractable from raw text, so it is much
easier to build a syntactic corpus. But this implies an associated
drop in manual linguistic processing of the data. As a result, the
principle difference between present phonological and syntactic
corpora is that phonology data is word-aligned, while syntax data is
not sentence-aligned.

\subsubsection{Nerbonne and Wiersma}
\label{nerbonne06}

Due to the lack of alignment between the
larger corpora available for syntactic analysis, a statistical
comparison of differences is more appropriate than the simple
symbolic approach possible with the word-aligned corpora used in
phonology. Because of the lack of alignment, a syntactic distance
measure will have to use counting as its basis by default.

\namecite{nerbonne06} was an early method proposed for syntactic
distance.  It models syntax by part-of-speech (POS) trigrams and uses
the trigram types in a permutation test of significance. This method was
extended by \namecite{sanders07}, who used \quotecite{sampson00}
leaf-ancestor paths as the basis for building the model instead.

The heart of the measure is simple: the difference in type counts
between the combined types of two corpora. \namecite{kessler01}
originally proposed this measure, the {\sc Recurrence}
metric ($R$):

\begin{equation}
R = \Sigma_i |c_{ai} - c_{bi}|
\label{rmeasure}
\end{equation}

\noindent{}Given two corpora $a$ and $b$, $c_a$ and $c_b$ are the type
counts. $i$ ranges over all types, so $c_{ai}$ and $c_{bi}$ are the
type counts for type $i$.  $R$ is designed to represent the amount of
variation exhibited by the two corpora while allowing the contribution
of individual types to be extracted simply.

To account for differences in corpus size, repeated sampling is
used. In addition, the samples are normalized to account for
differences in sentence length.  Unfortunately, even normalized, the
measure doesn't indicate whether its results are significant; a
permutation test is needed for that.

% Other ideas include training a
% model on one area and comparing the entropy (compression) of other
% areas. At this point it's unclear whether this would provide a
% comparable measure, however.

\subsubsection{Language models}
Part-of-speech (POS) trigrams are quite easy to obtain from a syntactically
annotated corpus. \namecite{nerbonne06} argue that POS trigrams
can accurately represent at least the important parts of syntax,
similar to the way chunk parsing can capture the most important
information about a sentence. POS trigrams can either be generated by
a tagger as Nerbonne and Wiersma did, or taken from the leaves of
the trees of a parsed corpus.

On the other hand, it might be better to directly represent the upper
structure of trees. \quotecite{sampson00} leaf-ancestor paths provide
one way to do this: leaf-ancestor paths produce for each leaf in the
tree the path from that leaf back to the root. Generation is
simple as long as every sibling is unique. For example, the parse tree
\[\xymatrix{
  &&\textrm{S} \ar@{-}[dl] \ar@{-}[dr] &&\\
  &\textrm{NP} \ar@{-}[d] \ar@{-}[dl] &&\textrm{VP} \ar@{-}[d]\\
  \textrm{Det} \ar@{-}[d] & \textrm{N} \ar@{-}[d] && \textrm{V} \ar@{-}[d] \\
\textrm{the}& \textrm{dog} && \textrm{barks}\\}
\]
creates the following leaf-ancestor paths:

\begin{itemize}
\item S-NP-Det-The
\item S-NP-N-dog
\item S-VP-V-barks
\end{itemize}

For identical siblings, brackets must be inserted in the path to
disambiguate the first sibling from the second. The process is not
described here because the details are incidental to the main idea and
in any case identical siblings are somewhat rare.

Sampson originally developed leaf-ancestor paths as an improved
measure of similarity between gold-standard and machine-parsed trees,
to be used in evaluating parsers. The basic idea of a collection of
features that capture distance between trees transfers quite nicely to
this application. \namecite{sanders07} replaced POS trigrams with
leaf-ancestor paths for the ICE corpus and found improved results on
larger corpora. However, smaller corpora are less likely to attain
significance compared to POS trigram features.

% Another idea is supertags rather than leaf-ancestor paths. This is
% quite similar but might work better.

TODO:Come up with and explain a method of extracting paths from
dependency parsing.

\subsubsection{Parsing}

Part-of-speech tagging and parsing will be needed for the Swedish dialect
data in the SweDiaSyn. This corpus transcribed to standard Swedish
orthography, but will need to be POS tagged and parsed, both using
phrase structures and dependencies.

The Tags 'n' Trigrams (T'n'T) tagger will be used for tagging. The POS
annotations from Talbanken05 will be used as the training
corpus. Because of the non-standard pronunciations, speech in the
SweDiaSyn is transcribed both to standard Swedish orthography and to an
orthographic transcription of the original speech. The standard
orthography will be used for POS tagging since the experiment focusses
on syntax, not phonology or the lexicon.

After POS tagging, the sentences will be parsed by a parser trained
on the standard Swedish of Talbanken05. Before training, the trees of
Talbanken will be cleaned. Some are discontinuous and many contain
disfluencies; both will to be removed before training the parser. With
disfluencies removed, discontinuities may be removable by a simple
strategy, such as uncrossing only at the root level. Otherwise, a
strategy similar to Adriane Boyd's splits will be needed.

Collin's parser will be trained on standard
Swedish, again using Talbanken05. This will provide a constituency
parser. Collin's parser is a statistical context-free grammar parser
that \ldots.

Dependency parsing will proceed similarly to constituency parsing;
the dependency structures of Talbanken05 will be cleaned and
normalised, then used to train a parser that will then parse the
sentences of SweDiaSyn.

TODO:Explain the method of parsing dependencies and
constituencies. Since I will be using existing tools, I just need to
choose one for each method, summarise it and point to the author's
papers.

TODO: Find out how much crossing occurs in Swedish corpora, and how
much of it is from interruptions and self-corrections.

\begin{enumerate}
\item Train constituency parser on Talbanken05's constituency trees.
\item Some crossing occurs in Swedish.
\item Constituency trees will require of cleaning to get rid of this.
\item First step: remove disfluencies.
\item Second step: uncross top-level punctuation-like things.
\item Third step (if needed): try splitting things apart like Adriane Boyd.
\item Train the parser.
\item Parse the sentences from Nordisk dialekterna.
\item Cleanup? Probably. Conversion to something anyway.
\item Train dependency parser on Talbanken05's dependency trees.
\item Probably there will be some pre-processing needed here too.
\end{enumerate}

\subsubsection{Permutation test}
\label{permutationtest}

A permutation test detects whether two corpora are significantly
different. It does this on the basis of the R measure described in
section \ref{nerbonne06}. The test first calculates $R$ between
samples of the two corpora. Then the corpora are mixed together and
$R$ is calculated between two samples are drawn from the mixed
corpus. If the two corpora are different, $R$ should be larger between
the samples of the original corpora than $R$ from the mixed
corpus. Any real differences will be randomly redistributed by the
mixing process, lowering $R$ of samples. Repeating this comparison
enough times will show if the difference is significant. Twenty times
is the minimum needed to detect significance for $p < 0.5$
significance, although the test is repeated one thousand times in the
experiments.

For example, assume that $R$ detects real differences between London
and Scotland such that $R(\textrm{London},\textrm{Scotland}) =
100$. The permutation test then mixes the London and Scotland to
create LSMixed and splits it into two pieces. Since the real
differences are now mixed between the two shuffled corpora, we
would expect $R(\textrm{LSMixed}_1, \textrm{LSMixed}_2) < 100$, something
like 90 or 95. This should be true at least 95\% of the time if the
differences are significant.

\subsubsection{Normalization}
Afterward, the distance must be normalized to account for two things:
the length of sentences in the corpus and the amount of variety in the
corpus. If sentence length differs too much between corpora, there
will be consistently lower token counts in one corpus, which would
cause a spuriously large $R$. In addition, if one corpus has less
variety than the other, it will have inflated type counts, because
more tokens will be allocated to fewer types. To avoid
this all tokens are scaled by the average number of types per token
across both corpora: $2n/N$ where $n$ is the type count and $N$ is
the token count. The additional factor $2$ is necessary because we are
recombining the tokens from the two corpora.

% this next subsection might need to be changed or deleted
\subsection{Cluster Analysis}

The results of the two distance methods can be compared using
binary hierarchical clustering. The resulting dendrogram allows the two
methods to be compared visually.

Correlation is also useful to find out how similar the two method's
predictions are. Because of the connected nature of the inter-region
distances, Mantel's test is necessary to ensure that the correlation
is significant. Mantel's test is a permutation test, similar to the
permutation test described for $R$. One distance result set is
permuted repeatedly and at each step correlated with the other
set. The correlation is significant if it is larger than the permuted
correlations more than 95\% of the time.

\subsection{Corpora}

\subsection{Previous Experiments}

TODO: This section should describe the previous experiments in more
detail. Specifically, input data IN DETAIL. (The method has has
already been described.) Also, the writing is not very good.

\namecite{nerbonne06} were the first to use the syntactic
distance measure described above. They analysed two corpora, both of
Norwegian L2 speakers of English. The first corpus was speakers who
learned English after childhood and the second was speakers who learned
English as children. Nerbonne \& Wiersma found a significant different
between the two corpora. The trigrams that contributed most to the
difference were longer (?) in the younger corpus and shorter in the
older corpus. They analysed this as a difference in completeness of
learning; the younger L2 learners of English learned the complex
structures of English more fully.

Wait, longer? That doesn't make sense--they're trigrams; maybe it was more
complementising POSes or something. Anyway, something that was
indicative of longer sentences. In any case, this experiment did not
use leaf-ancestor paths, so trigrams were used. Leaf-ancestor paths
should help by directly representing internal tree complexity.

Subsequent work by \namecite{sanders07} and \namecite{sanders08b}
expanded on the Norwegian experiment in two ways. First, it introduced
leaf-ancestor paths as an alternative feature type. Second, it tested
the distance method on a larger set of corpora: Government Office
Regions of England, as well as Scotland and Wales. This was a total of
11 corpora. Each was smaller than the Norwegian L2 corpora, which
proved to be problematic for some combations of features and
permutation test parameters.

Despite the smaller region-based corpora, significant differences
resulted between quite a few regions. Furthermore, I showed that
syntactic distance does not correlate with phonological distance, as
measured by this permutation test on R and Levenshtein distance. This
could be a result of an erroneous distanc measures, or a mismatch on
the actual boundaries specified by phonological features and syntactic
features. Although the agreement between phonological and syntactic
features of dialects is an interesting question, I will not
investigate it further here, focussing exclusively on syntactic
distance.

\subsection{SweDiaSyn, Nordisk dialecter}

TODO: Same here as in Previous Experiments. Data in detail, as soon as
I know what the corpus is like.

Also, which treebanks to train on. That should be pretty easy to find
since there is probably only one main one of each type for Swedish.

\begin{enumerate}
\item Train on Talbanken05
\item Constituency parsed version was prepared according to the
  uncrossing method above.
\item Dependency parsed version was prepared, hopefully nothing
  special required.
\item Use the Swedish section of SweDiaSyn.
\end{enumerate}

% OLD %
% \section{Dialectometry}

% Dialectometry is the quantitative linguistic study of differences
% between languages. It is a subset of dialectology, which looks for
% boundaries between languages. However, unlike classic dialectology, it
% does not look for strict boundaries (in the form of isogloss bundles)
% between languages based on boundaries specified by multiple
% variables. Instead, dialectometry first defines a distance measure
% that combines information from multiple variables. The result
% specifies gradient boundaries, not absolute boundaries.

% Dialectology is related to sociolinguistics--the two often work with
% the same data. But the questions are different: sociolinguistics asks
% Why (and What) while dialectometry asks How Much (and What).
% In other words, sociolinguistics analyses the conditions that cause
% differences between languages, such as geographic separation, social
% separation, code-switching or social climbing. For dialectometry, it
% does not matter which dimension is used; the only concern is correlating
% the linguistic differences with the areas specified by the
% sociological variable. Historically, geographic separation is the most
% usual application, but others are certainly possible and some have
% been investigated (Sanders, Gooskens, another of John's students)

% \section{Previous Work}

% Results were inconclusive for British dialects when comparing the
% syntactic measure with a well-known phonological measure, Levenshtein
% distance. I will address this in several ways, deepening and
% complicating the study in the process.

% First, a change of corpus. This
% has two improvements: more data, and better data. In the ICE, nobody
% really talked that much, and everybody was in London College at the
% time, so even if they were born out in the country, they were trying
% to sound Proper. The Scandinavian Dialect Corpus
% (Nordisk dialektkorpus) is recorded mostly on location, in traditional
% dialectology style, old people spread somewhat evenly throughout the
% country.

% Second, an expansion in methods. In the previous work on the ICE
% corpus, only trigrams and leaf-ancestor paths are used as a source of
% features for the statistical classifier. Only leaf-ancestor paths are
% sensitive enough to give significant results. With more data, more
% methods should be sensitive enough to produce sensible
% results. Therefore, in addition to constituent-based leaf-ancestor
% paths, a similar measure will be developed for dependency parses.

% This will require some additional preparation relative to experiments
% run on the ICE, which is already parsed by a constituent grammar. The
% unparsed Scandinavian Dialect Corpus will have to be parsed by
% machine. But this provides an opportunity to use multiple parsing
% methods. There exist Swedish treebanks parsed by both constituent parsers and
% dependency parsers.

\bibliographystyle{robbib}
\bibliography{central}
\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

